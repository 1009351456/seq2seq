<!DOCTYPE html>
<!--[if IE 8]><html class="no-js lt-ie9" lang="en" > <![endif]-->
<!--[if gt IE 8]><!--> <html class="no-js" lang="en" > <!--<![endif]-->
<head>
  <meta charset="utf-8">
  <meta http-equiv="X-UA-Compatible" content="IE=edge">
  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  
  
  <link rel="shortcut icon" href="../img/favicon.ico">
  <title>Data - seq2seq</title>
  <link href='https://fonts.googleapis.com/css?family=Lato:400,700|Roboto+Slab:400,700|Inconsolata:400,700' rel='stylesheet' type='text/css'>

  <link rel="stylesheet" href="../css/theme.css" type="text/css" />
  <link rel="stylesheet" href="../css/theme_extra.css" type="text/css" />
  <link rel="stylesheet" href="../css/highlight.css">
  <link href="../extra.css" rel="stylesheet">
  
  <script>
    // Current page data
    var mkdocs_page_name = "Data";
    var mkdocs_page_input_path = "data.md";
    var mkdocs_page_url = "/data/";
  </script>
  
  <script src="../js/jquery-2.1.1.min.js"></script>
  <script src="../js/modernizr-2.8.3.min.js"></script>
  <script type="text/javascript" src="../js/highlight.pack.js"></script> 
  
</head>

<body class="wy-body-for-nav" role="document">

  <div class="wy-grid-for-nav">

    
    <nav data-toggle="wy-nav-shift" class="wy-nav-side stickynav">
      <div class="wy-side-nav-search">
        <a href=".." class="icon icon-home"> seq2seq</a>
        <div role="search">
  <form id ="rtd-search-form" class="wy-form" action="../search.html" method="get">
    <input type="text" name="q" placeholder="Search docs" />
  </form>
</div>
      </div>

      <div class="wy-menu wy-menu-vertical" data-spy="affix" role="navigation" aria-label="main navigation">
	<ul class="current">
          
            <li>
    <li class="toctree-l1 ">
        <a class="" href="..">Overview</a>
        
    </li>
<li>
          
            <li>
    <li class="toctree-l1 ">
        <a class="" href="../getting_started/">Getting Started</a>
        
    </li>
<li>
          
            <li>
    <li class="toctree-l1 current">
        <a class="current" href="./">Data</a>
        
            <ul>
            
                <li class="toctree-l3"><a href="#ready-to-use-datasets">Ready-to-use Datasets</a></li>
                
            
                <li class="toctree-l3"><a href="#creating-your-own-data">Creating your own data</a></li>
                
                    <li><a class="toctree-l4" href="#parallel-text-format">Parallel Text Format</a></li>
                
                    <li><a class="toctree-l4" href="#tokenization">Tokenization</a></li>
                
                    <li><a class="toctree-l4" href="#generating-vocabulary">Generating Vocabulary</a></li>
                
                    <li><a class="toctree-l4" href="#subword-units-bpe">Subword Units (BPE)</a></li>
                
            
            </ul>
        
    </li>
<li>
          
            <li>
    <li class="toctree-l1 ">
        <a class="" href="../models/">Models</a>
        
    </li>
<li>
          
            <li>
    <li class="toctree-l1 ">
        <a class="" href="../training/">Training</a>
        
    </li>
<li>
          
            <li>
    <li class="toctree-l1 ">
        <a class="" href="../inference/">Inference</a>
        
    </li>
<li>
          
            <li>
    <li class="toctree-l1 ">
        <a class="" href="../tools/">Tools</a>
        
    </li>
<li>
          
            <li>
    <li class="toctree-l1 ">
        <a class="" href="../benchmarks/">Results and Pre-Trained Models</a>
        
    </li>
<li>
          
            <li>
    <li class="toctree-l1 ">
        <a class="" href="../contributing/">Contributing</a>
        
    </li>
<li>
          
        </ul>
      </div>
      &nbsp;
    </nav>

    <section data-toggle="wy-nav-shift" class="wy-nav-content-wrap">

      
      <nav class="wy-nav-top" role="navigation" aria-label="top navigation">
        <i data-toggle="wy-nav-top" class="fa fa-bars"></i>
        <a href="..">seq2seq</a>
      </nav>

      
      <div class="wy-nav-content">
        <div class="rst-content">
          <div role="navigation" aria-label="breadcrumbs navigation">
  <ul class="wy-breadcrumbs">
    <li><a href="..">Docs</a> &raquo;</li>
    
      
    
    <li>Data</li>
    <li class="wy-breadcrumbs-aside">
      
    </li>
  </ul>
  <hr/>
</div>
          <div role="main">
            <div class="section">
              
                <h2 id="ready-to-use-datasets">Ready-to-use Datasets</h2>
<p>We prepared a few datasets to help you get started. We also provide data generation scripts that
make explicit what kind of processing has been done on the data.</p>
<table>
<thead>
<tr>
<th>Dataset</th>
<th>Description</th>
<th>Training/Dev/Test Size</th>
<th>Vocabulary</th>
<th>Download</th>
</tr>
</thead>
<tbody>
<tr>
<td>WMT'16 EN-DE</td>
<td>Data for the <a href="http://www.statmt.org/wmt16/translation-task.html">WMT'16 Translation Task</a> English to German. Training data is combined from Europarl v7, Common Crawl, and News Commentary v11. Development data sets include <code>newstest[2010-2015]</code>. <code>newstest2016</code> should serve as test data. All SGM files were converted to plain text.</td>
<td>4.56M/3K/2.6K</td>
<td>50k Words <br/> 50k BPE</td>
<td>Download <br/> <a href="https://github.com/dennybritz/seq2seq/blob/master/bin/data/wmt16_en_de.sh">Generate</a></td>
</tr>
<tr>
<td>Toy Copy</td>
<td>A toy dataset where the target sequence is equal to the source sequence. The model must learn to copy the source sequence.</td>
<td>10k/1k/1k</td>
<td>20</td>
<td>Download <br/> <a href="https://github.com/dennybritz/seq2seq/blob/master/bin/data/toy.sh">Generate</a></td>
</tr>
<tr>
<td>Toy Reverse</td>
<td>A toy dataset where the target sequence is equal to the reversed source sequence.</td>
<td>10k/1k/1k</td>
<td>20</td>
<td>Download <br/> <a href="https://github.com/dennybritz/seq2seq/blob/master/bin/data/toy.sh">Generate</a></td>
</tr>
</tbody>
</table>
<h2 id="creating-your-own-data">Creating your own data</h2>
<p>To use your own data you must bring it into the right format. A typical data preprocessing pipeline looks as follows:</p>
<ol>
<li>Generate data in parallel text format</li>
<li>Tokenize your data</li>
<li>Create a fixed vocabulary for your source and target data</li>
<li>(Optional) Use Subword Units to handle rare or unknown words</li>
</ol>
<h3 id="parallel-text-format">Parallel Text Format</h3>
<p>The input pipeline expects tokenized parallel data in raw text format. You need a <code>sources</code> and <code>targets</code> file that contain  sequences that are aligned line-by-line. Each line corresponds to one input/output example. Tokens must be separated by spaces. For example, in Machine Translation you typically have one file with sentences in the source language and a second one  with the corresponding translations.</p>
<h3 id="tokenization">Tokenization</h3>
<p>For good results it is crucial to <a href="http://nlp.stanford.edu/IR-book/html/htmledition/tokenization-1.html">tokenize your text</a>. Many tools to do tokenization are available, e.g.</p>
<ul>
<li>The Moses <a href="https://github.com/moses-smt/mosesdecoder/blob/master/scripts/tokenizer/tokenizer.perl"><code>tokenizer.perl</code></a> script.</li>
<li>Libraries such a <a href="https://spacy.io/docs/usage/processing-text">spaCy</a>, <a href="http://www.nltk.org/api/nltk.tokenize.html">nltk</a> or <a href="http://nlp.stanford.edu/software/tokenizer.shtml">Stanford Tokenizer</a>.</li>
</ul>
<p>For example, to use the Moses tokenizer:</p>
<pre><code class="bash"># Clone from Github
git clone https://github.com/moses-smt/mosesdecoder.git

# Tokenize English (en) data
mosesdecoder/scripts/tokenizer/tokenizer.perl -l en -threads 8 &lt; english_data &gt; english_data.tok

# Tokenize German (de) data
mosesdecoder/scripts/tokenizer/tokenizer.perl -l de -threads 8 &lt; german_data &gt; german_data.tok
</code></pre>

<h3 id="generating-vocabulary">Generating Vocabulary</h3>
<p>A vocabulary file is a raw text file that contains one word per line. The total number of lines is equal to the size of the vocabulary and each token is mapped to its line number. The special tokens <code>UNK</code>, <code>SEQUENCE_START</code> and <code>SEQUENCE_END</code> that art are generated by the model are not part of the vocabulary file and correspond to <code>vocab_size + 1</code>, <code>vocab_size + 2</code>, and <code>vocab_size + 3</code>, respectively.</p>
<p>We provide a helper script <a href="https://github.com/dennybritz/seq2seq/blob/master/bin/tools/generate_vocab.py"><code>bin/tools/generate_vocab.py</code></a>  that, given a raw text file of tokens separated by spaces, generates a vocabulary file:</p>
<pre><code class="shell">./bin/tools/generate_vocab.py \
  --input_file /data/source.txt \
  --output_file /data/source_vocab \
  --min_frequency 1 \
  --max_vocab_size 50000
</code></pre>

<h3 id="subword-units-bpe">Subword Units (BPE)</h3>
<p>In order to deal with an open vocabulary  rare words can be split into subword units as proposed in <a href="http://arxiv.org/abs/1508.07909">Neural Machine Translation of Rare Words with Subword Units</a>. This improves the model's translation performance particularly on rare words. Subword units are calculated using Byte Pair Encoding (BPE), which iteratively replaces the most frequent pair of symbols with a new symbol. The final symbol vocabulary is equal to the size of an initial vocabulary, all characters appearing in the text, plus the number of merge operations, which is a hyperparameter of the method. To apply BPE as a pre-processing step to your raw (tokenized) text input, follow the below steps:</p>
<pre><code class="bash"># Clone from Github
git clone https://github.com/rsennrich/subword-nmt
cd subword-nmt

# Learn a vocabulary using 10,000 merge operations
./learn_bpe.py -s 10000 &lt; train.tok &gt; codes.bpe
# Apply the vocabulary to the training file
./apply_bpe.py -c codes.bpe &lt; train.tok &gt; train.tok.bpe
</code></pre>

<p>The resulting BPE-processed files can be used as-is in place of the raw text files for training the NMT model. Note that you must now use the BPE vocabulary as your vocabulary file. You can do this by generating a vocabulary based on the BPE-processed files.</p>
<p>In the BPE-processed file the original words are split using a special <code>"@@ "</code> string. To recover the original tokenization you can simply perform a <code>sed "s/@@ //g"</code> operation on the BPE files and the model output. For more details, refer to the paper and <a href="https://github.com/rsennrich/subword-nmt">subword-nmt</a> Github repository.</p>
              
            </div>
          </div>
          <footer>
  
    <div class="rst-footer-buttons" role="navigation" aria-label="footer navigation">
      
        <a href="../models/" class="btn btn-neutral float-right" title="Models">Next <span class="icon icon-circle-arrow-right"></span></a>
      
      
        <a href="../getting_started/" class="btn btn-neutral" title="Getting Started"><span class="icon icon-circle-arrow-left"></span> Previous</a>
      
    </div>
  

  <hr/>

  <div role="contentinfo">
    <!-- Copyright etc -->
    
  </div>

  Built with <a href="http://www.mkdocs.org">MkDocs</a> using a <a href="https://github.com/snide/sphinx_rtd_theme">theme</a> provided by <a href="https://readthedocs.org">Read the Docs</a>.
</footer>
	  
        </div>
      </div>

    </section>
    
  </div>

  <div class="rst-versions" role="note" style="cursor: pointer">
    <span class="rst-current-version" data-toggle="rst-current-version">
      
      
        <span><a href="../getting_started/" style="color: #fcfcfc;">&laquo; Previous</a></span>
      
      
        <span style="margin-left: 15px"><a href="../models/" style="color: #fcfcfc">Next &raquo;</a></span>
      
    </span>
</div>
    <script src="../js/theme.js"></script>

</body>
</html>
