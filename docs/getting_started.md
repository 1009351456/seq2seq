## Download & Setup

To use tf-seq2seq you need a working installation of TensorFlow 1.0 with
Python 2.7 or Python 3.5. Follow the [TensorFlow Getting Started](https://www.tensorflow.org/versions/r1.0/get_started/os_setup) guide for detailed setup instructions. With TensorFlow installed, you can clone this repository:

```bash
git clone https://github.com/dennybritz/seq2seq.git
cd seq2seq
```

To make sure everything works as expect you can run a simple pipeline unit test:

```bash
python -m unittest seq2seq.test.pipeline_test
```

If you see a "success" message, you are all set. Note that you may need to install pyrouge, pyyaml, and matplotlib, in order for these tests to pass. If you run into other setup issues,
please [file a Github issue](https://github.com/dennybritz/seq2seq/issues).

## Using a pre-trained model

Coming soon!

## Training your own model

Let's look at how you can train a model based on your own data. In this section, we will train a model that learns to reverse an input sequence. While this task is not very useful in practice, we can train such a model quickly and use it as as sanity-check to make sure that the end-to-end pipeline is working as intended.

First, let's generate some data:

```bash
DATA_TYPE=reverse ./bin/data/toy.sh
```

To train a new model, we will use the files that are generated by this script, which are described in detail in the table below. To use your own data, you will need to generate corresponding files in the same format.

| Argument Name | Description |
| --- | --- |
| `train_source` | The input sequences. A plain text file where each line corresponds to one input example. Tokens/words are separated by spaces. For natural language data, you may need to use a tokenizer to generate this. See [tools](tools/) for more detail. |
| `train_target` | The target sequences in the same format as `train_source`. Each line corresponds to the "translation" of the input sequence in the same line in `train_source`. Thus, the number of lines in these two files must be equal. |
| `dev_source` | Same as `train_source` but used for validation/development. |
| `dev_target` | Same as `train_target` but used for validation/development. |
| `vocab_source` | The vocabulary for the source sequences in the form of a plain text file containing one word per line. All words that appear in the source text but are not in the vocabulary will be mapped to `UNK` (unknown) tokens. We provide a script to generate a vocabulary file. See [tools](tools/) for more detail. |
| `vocab_target` | Same as `vocab_source` but for the target sequences. |


Given the above input files, you can now train a new model:

```bash
python -m bin.train \
  --train_source $HOME/nmt_data/toy_reverse/train/sources.txt \
  --train_target $HOME/nmt_data/toy_reverse/train/targets.txt \
  --dev_source $HOME/nmt_data/toy_reverse/dev/sources.txt \
  --dev_target $HOME/nmt_data/toy_reverse/dev/targets.txt \
  --vocab_source $HOME/nmt_data/toy_reverse/train/vocab.sources.txt \
  --vocab_target $HOME/nmt_data/toy_reverse/train/vocab.targets.txt \
  --model AttentionSeq2Seq \
  --batch_size 32 \
  --train_steps 2000 \
  --output_dir ${TMPDIR:-/tmp}/nmt_toy_reverse
```

On a CPU, the training may take up to 15 minutes. With the trained model, you can run inference and make predictions as follows:

```bash
python -m bin.infer \
  --source $HOME/nmt_data/toy_reverse/test/sources.txt \
  --model_dir ${TMPDIR:-/tmp}/nmt_toy_reverse \
  > ${TMPDIR:-/tmp}/nmt_toy_reverse/predictions.txt

# Evaluate BLEU score using multi-bleu script from MOSES
./bin/tools/multi-bleu.perl $HOME/nmt_data/toy_reverse/test/targets.txt < ${TMPDIR:-/tmp}/nmt_toy_reverse/predictions.txt
```

To learn more about available models, data and tools, please see the corresponding sections of the documentation.


