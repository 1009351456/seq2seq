<!DOCTYPE html>
<!--[if IE 8]><html class="no-js lt-ie9" lang="en" > <![endif]-->
<!--[if gt IE 8]><!--> <html class="no-js" lang="en" > <!--<![endif]-->
<head>
  <meta charset="utf-8">
  <meta http-equiv="X-UA-Compatible" content="IE=edge">
  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  
  
  <link rel="shortcut icon" href="../img/favicon.ico">
  <title>Reference: Models - seq2seq</title>
  <link href='https://fonts.googleapis.com/css?family=Lato:400,700|Roboto+Slab:400,700|Inconsolata:400,700' rel='stylesheet' type='text/css'>

  <link rel="stylesheet" href="../css/theme.css" type="text/css" />
  <link rel="stylesheet" href="../css/theme_extra.css" type="text/css" />
  <link rel="stylesheet" href="../css/highlight.css">
  <link href="../extra.css" rel="stylesheet">
  
  <script>
    // Current page data
    var mkdocs_page_name = "Reference: Models";
    var mkdocs_page_input_path = "models.md";
    var mkdocs_page_url = "/models/";
  </script>
  
  <script src="../js/jquery-2.1.1.min.js"></script>
  <script src="../js/modernizr-2.8.3.min.js"></script>
  <script type="text/javascript" src="../js/highlight.pack.js"></script> 
  
</head>

<body class="wy-body-for-nav" role="document">

  <div class="wy-grid-for-nav">

    
    <nav data-toggle="wy-nav-shift" class="wy-nav-side stickynav">
      <div class="wy-side-nav-search">
        <a href=".." class="icon icon-home"> seq2seq</a>
        <div role="search">
  <form id ="rtd-search-form" class="wy-form" action="../search.html" method="get">
    <input type="text" name="q" placeholder="Search docs" />
  </form>
</div>
      </div>

      <div class="wy-menu wy-menu-vertical" data-spy="affix" role="navigation" aria-label="main navigation">
	<ul class="current">
          
            <li>
    <li class="toctree-l1 ">
        <a class="" href="..">Overview</a>
        
    </li>
<li>
          
            <li>
    <li class="toctree-l1 ">
        <a class="" href="../getting_started/">Getting Started</a>
        
    </li>
<li>
          
            <li>
    <li class="toctree-l1 ">
        <a class="" href="../concepts/">Concepts</a>
        
    </li>
<li>
          
            <li>
    <li class="toctree-l1 ">
        <a class="" href="../nmt/">Tutorial: Neural Machine Translation</a>
        
    </li>
<li>
          
            <li>
    <li class="toctree-l1 ">
        <a class="" href="../summarization/">Tutorial: Summarization</a>
        
    </li>
<li>
          
            <li>
    <li class="toctree-l1 ">
        <a class="" href="../image_captioning/">Tutorial: Image Captioning</a>
        
    </li>
<li>
          
            <li>
    <li class="toctree-l1 ">
        <a class="" href="../data/">Data</a>
        
    </li>
<li>
          
            <li>
    <li class="toctree-l1 ">
        <a class="" href="../training/">Training</a>
        
    </li>
<li>
          
            <li>
    <li class="toctree-l1 ">
        <a class="" href="../inference/">Inference</a>
        
    </li>
<li>
          
            <li>
    <li class="toctree-l1 ">
        <a class="" href="../tools/">Tools</a>
        
    </li>
<li>
          
            <li>
    <li class="toctree-l1 ">
        <a class="" href="../benchmarks/">Benchmarks</a>
        
    </li>
<li>
          
            <li>
    <li class="toctree-l1 ">
        <a class="" href="../help/">Getting Help</a>
        
    </li>
<li>
          
            <li>
    <li class="toctree-l1 ">
        <a class="" href="../contributing/">Contributing</a>
        
    </li>
<li>
          
            <li>
    <li class="toctree-l1 ">
        <a class="" href="../license/">License</a>
        
    </li>
<li>
          
            <li>
    <li class="toctree-l1 current">
        <a class="current" href="./">Reference: Models</a>
        
            <ul>
            
                <li class="toctree-l3"><a href="#modelbase">ModelBase</a></li>
                
            
                <li class="toctree-l3"><a href="#seq2seqmodel">Seq2SeqModel</a></li>
                
            
                <li class="toctree-l3"><a href="#basicseq2seq">BasicSeq2Seq</a></li>
                
            
                <li class="toctree-l3"><a href="#attentionseq2seq">AttentionSeq2Seq</a></li>
                
            
                <li class="toctree-l3"><a href="#image2seq">Image2Seq</a></li>
                
            
            </ul>
        
    </li>
<li>
          
            <li>
    <li class="toctree-l1 ">
        <a class="" href="../encoders/">Reference: Encoders</a>
        
    </li>
<li>
          
            <li>
    <li class="toctree-l1 ">
        <a class="" href="../decoders/">Reference: Decoders</a>
        
    </li>
<li>
          
        </ul>
      </div>
      &nbsp;
    </nav>

    <section data-toggle="wy-nav-shift" class="wy-nav-content-wrap">

      
      <nav class="wy-nav-top" role="navigation" aria-label="top navigation">
        <i data-toggle="wy-nav-top" class="fa fa-bars"></i>
        <a href="..">seq2seq</a>
      </nav>

      
      <div class="wy-nav-content">
        <div class="rst-content">
          <div role="navigation" aria-label="breadcrumbs navigation">
  <ul class="wy-breadcrumbs">
    <li><a href="..">Docs</a> &raquo;</li>
    
      
    
    <li>Reference: Models</li>
    <li class="wy-breadcrumbs-aside">
      
    </li>
  </ul>
  <hr/>
</div>
          <div role="main">
            <div class="section">
              
                <p>When calling the training script, you can specify a model class using the <code>--model</code> flag and model-specific hyperparameters using the <code>--model_params</code> flag. This page lists all supported models and hyperparameters.</p>
<h2 id="modelbase"><a href="https://github.com/google/seq2seq/blob/master/seq2seq/models/model_base.py"><code>ModelBase</code></a></h2>
<hr />
<p>This is an abstract class that cannot be used as a model during training. Other model classes inherit from this. The following parameters are shared by all models, unless explicitly stated otherwise in the model section.</p>
<table>
<thead>
<tr>
<th>Name</th>
<th>Default</th>
<th>Description</th>
</tr>
</thead>
<tbody>
<tr>
<td><code>optimizer.name</code></td>
<td><code>Adam</code></td>
<td>Type of Optimizer to use, e.g. <code>Adam</code>, <code>SGD</code> or <code>Momentum</code>. The name is fed to TensorFlow's <a href="https://www.tensorflow.org/api_docs/python/contrib.layers/optimization#optimize_loss">optimize_loss</a> function. See TensorFlow documentation for more details and all available options.</td>
</tr>
<tr>
<td><code>optimizer.learning_rate</code></td>
<td><code>1e-4</code></td>
<td>Initial learning rate for the optimizer. This is fed to TensorFlow's <a href="https://www.tensorflow.org/api_docs/python/contrib.layers/optimization#optimize_loss">optimize_loss</a> function.</td>
</tr>
<tr>
<td><code>optimizer.lr_decay_type</code></td>
<td></td>
<td>The name of one of TensorFlow's <a href="https://www.tensorflow.org/api_docs/python/#training--decaying-the-learning-rate">learning rate decay functions</a> defined in <code>tf.train</code>, e.g. <code>exponential_decay</code>. If this is an empty string (default) then no learning rate decay is used.</td>
</tr>
<tr>
<td><code>optimizer.lr_decay_steps</code></td>
<td><code>100</code></td>
<td>How often to apply decay. This is fed as the <code>decay_steps</code> argument to the decay function defined above. See Tensoflow documentation for more details.</td>
</tr>
<tr>
<td><code>optimizer.lr_decay_rate</code></td>
<td><code>0.99</code></td>
<td>The decay rate. This is fed as the <code>decay_rate</code> argument to the decay function defined above. See TensorFlow documentation for more details.</td>
</tr>
<tr>
<td><code>optimizer.lr_start_decay_at</code></td>
<td><code>0</code></td>
<td>Start learning rate decay at this step.</td>
</tr>
<tr>
<td><code>optimizer.lr_stop_decay_at</code></td>
<td><code>1e9</code></td>
<td>Stop learning rate decay at this step.</td>
</tr>
<tr>
<td><code>optimizer.lr_min_learning_rate</code></td>
<td><code>1e-12</code></td>
<td>Never decay below this learning rate.</td>
</tr>
<tr>
<td><code>optimizer.lr_staircase</code></td>
<td><code>False</code></td>
<td>If <code>True</code>, decay the learning rate at discrete intervals. This is fed as the <code>staircase</code> argument to the decay function defined above. See TensorFlow documentation for more details.</td>
</tr>
<tr>
<td><code>optimizer.clip_gradients</code></td>
<td><code>5.0</code></td>
<td>Clip gradients by their global norm.</td>
</tr>
</tbody>
</table>
<h2 id="seq2seqmodel"><a href="https://github.com/google/seq2seq/blob/master/seq2seq/models/seq2seq_model.py"><code>Seq2SeqModel</code></a></h2>
<hr />
<p>This is an abstract class that cannot be used as a model during training. Other model classes inherit from this. The following hyperparameters are shared by all models that inherit from <code>Seq2SeqModel</code>, unless explicitly stated otherwise.</p>
<table>
<thead>
<tr>
<th>Name</th>
<th>Default</th>
<th>Description</th>
</tr>
</thead>
<tbody>
<tr>
<td><code>source.max_seq_len</code></td>
<td><code>50</code></td>
<td>Maximum length of source sequences. An example is sliced to this length before being fed to the encoder.</td>
</tr>
<tr>
<td><code>source.reverse</code></td>
<td><code>True</code></td>
<td>If set to true, reverse the source sequence before feeding it into the encoder.</td>
</tr>
<tr>
<td><code>target.max_seq_len</code></td>
<td><code>50</code></td>
<td>Maximum length of target sequences. An example is sliced to this length before being fed to the decoder.</td>
</tr>
<tr>
<td><code>embedding.dim</code></td>
<td><code>100</code></td>
<td>Dimensionality of the embedding layer.</td>
</tr>
<tr>
<td><code>embedding.share</code></td>
<td><code>False</code></td>
<td>If set to true, share embedding parameters for source and target sequences.</td>
</tr>
<tr>
<td><code>inference.beam_search.beam_width</code></td>
<td><code>0</code></td>
<td>Beam Search beam width used during inference. A value of less or equal than <code>1</code> disables beam search.</td>
</tr>
<tr>
<td><code>inference.max_decode_length</code></td>
<td><code>100</code></td>
<td>During inference mode, decode up to this length or until a <code>SEQUENCE_END</code> token is encountered, whichever happens first.</td>
</tr>
<tr>
<td><code>inference.beam_search.length_penalty_weight</code></td>
<td><code>0.0</code></td>
<td>Length penalty factor applied to beam search hypotheses, as described in <a href="https://arxiv.org/abs/1609.08144">https://arxiv.org/abs/1609.08144</a>.</td>
</tr>
<tr>
<td><code>vocab_source</code></td>
<td><code>""</code></td>
<td>Path to the source vocabulary to use. This is used to map input tokens to integer IDs.</td>
</tr>
<tr>
<td><code>vocab_target</code></td>
<td><code>""</code></td>
<td>Path to the target vocabulary to use. This is used to map input tokens to integer IDs.</td>
</tr>
</tbody>
</table>
<h2 id="basicseq2seq"><a href="https://github.com/google/seq2seq/blob/master/seq2seq/models/basic_seq2seq.py"><code>BasicSeq2Seq</code></a></h2>
<hr />
<p>Includes all parameters from <code>Seq2SeqModel</code>. The <code>BasicSeq2Seq</code> model uses an encoder and decoder with no attention mechanism. The last encoder state is passed through a fully connected layer and used to initialize the decoder (this behavior can be changed using the <code>bridge.*</code> hyperparameter). This is the "vanilla" implementation of the standard seq2seq architecture.</p>
<table>
<thead>
<tr>
<th>Name</th>
<th>Default</th>
<th>Description</th>
</tr>
</thead>
<tbody>
<tr>
<td><code>bridge.class</code></td>
<td><code>seq2seq.models.bridges.InitialStateBridge</code></td>
<td>Type of bridge to use. The bridge defines how state is passed between the encoder and decoder. Refer to the <a href="https://github.com/google/seq2seq/blob/master/seq2seq/models/bridges.py"><code>seq2seq.models.bridges</code></a> module for more details.</td>
</tr>
<tr>
<td><code>bridge.params</code></td>
<td><code>{}</code></td>
<td>Parameters passed to the bridge during construction.</td>
</tr>
<tr>
<td><code>encoder.class</code></td>
<td><code>seq2seq.encoders.UnidirectionalRNNEncoder</code></td>
<td>Type of encoder to use. See the <a href="../encoders/">Encoder Reference</a> for more details and available encoders.</td>
</tr>
<tr>
<td><code>encoder.params</code></td>
<td><code>{}</code></td>
<td>Parameters passed to the encoder during construction. See the <a href="../encoders/">Encoder Reference</a> for more details.</td>
</tr>
<tr>
<td><code>decoder.class</code></td>
<td><code>seq2seq.decoders.BasicDecoder</code></td>
<td>Type of decoder to use. See the <a href="../decoders/">Decoder Reference</a> for more details and available encoders.</td>
</tr>
<tr>
<td><code>decoder.params</code></td>
<td><code>{}</code></td>
<td>Parameters passed to the decoder during construction. See the <a href="../decoders/">Decoder Reference</a> for more details.</td>
</tr>
</tbody>
</table>
<h2 id="attentionseq2seq"><a href="https://github.com/google/seq2seq/blob/master/seq2seq/models/attention_seq2seq.py"><code>AttentionSeq2Seq</code></a></h2>
<hr />
<p>Includes all parameters from <code>Seq2SeqModel</code> and <code>BasicSeq2Seq</code>. This model is similar to <code>BasicSeq2Seq</code>, except that it uses an attention mechanism during decoding. By default, the last encoder state is not fed to the decoder.  The implementation is comparable to the model in <a href="https://arxiv.org/abs/1409.0473">Neural Machine Translation by Jointly Learning to Align and Translate</a>.</p>
<table>
<thead>
<tr>
<th>Name</th>
<th>Default</th>
<th>Description</th>
</tr>
</thead>
<tbody>
<tr>
<td><code>attention.class</code></td>
<td><code>AttentionLayerBahdanau</code></td>
<td>Class name of the attention layer. Can be a fully-qualified name or is assumed to be defined in <code>seq2seq.decoders.attention</code>. Currently available layers are <code>AttentionLayerBahdanau</code> and <code>AttentionLayerDot</code>.</td>
</tr>
<tr>
<td><code>attention.params</code></td>
<td><code>{"num_units": 128}</code></td>
<td>A dictionary of  parameters passed to the attention class constructor.</td>
</tr>
<tr>
<td><code>bridge.class</code></td>
<td><code>seq2seq.models.bridges.ZeroBridge</code></td>
<td>Type of bridge to use. The bridge defines how state is passed between the encoder and decoder. Refer to the <a href="https://github.com/google/seq2seq/blob/master/seq2seq/models/bridges.py"><code>seq2seq.models.bridges</code></a> module for more details.</td>
</tr>
<tr>
<td><code>encoder.class</code></td>
<td><code>seq2seq.encoders.BidirectionalRNNEncoder</code></td>
<td>Type of encoder to use. See the <a href="../encoders/">Encoder Reference</a> for more details and available encoders.</td>
</tr>
<tr>
<td><code>decoder.class</code></td>
<td><code>seq2seq.decoders.AttentionDecoder</code></td>
<td>Type of decoder to use. See the <a href="../decoders/">Decoder Reference</a> for more details and available encoders.</td>
</tr>
</tbody>
</table>
<h2 id="image2seq"><a href="https://github.com/google/seq2seq/blob/master/seq2seq/models/image2seq.py"><code>Image2Seq</code></a></h2>
<hr />
<p><strong>This model is currently experimental.</strong> This model uses the same parameters as <code>AttentionSeq2Seq</code> with the following differences:</p>
<ul>
<li>The default encoder is <code>seq2seq.encoders.InceptionV3Encoder</code></li>
<li>There are not <code>source.max_seq_len</code> and <code>source.reverse</code>, and <code>vocab_source</code> parameters.</li>
</ul>
              
            </div>
          </div>
          <footer>
  
    <div class="rst-footer-buttons" role="navigation" aria-label="footer navigation">
      
        <a href="../encoders/" class="btn btn-neutral float-right" title="Reference: Encoders">Next <span class="icon icon-circle-arrow-right"></span></a>
      
      
        <a href="../license/" class="btn btn-neutral" title="License"><span class="icon icon-circle-arrow-left"></span> Previous</a>
      
    </div>
  

  <hr/>

  <div role="contentinfo">
    <!-- Copyright etc -->
    
  </div>

  Built with <a href="http://www.mkdocs.org">MkDocs</a> using a <a href="https://github.com/snide/sphinx_rtd_theme">theme</a> provided by <a href="https://readthedocs.org">Read the Docs</a>.
</footer>
	  
        </div>
      </div>

    </section>
    
  </div>

  <div class="rst-versions" role="note" style="cursor: pointer">
    <span class="rst-current-version" data-toggle="rst-current-version">
      
      
        <span><a href="../license/" style="color: #fcfcfc;">&laquo; Previous</a></span>
      
      
        <span style="margin-left: 15px"><a href="../encoders/" style="color: #fcfcfc">Next &raquo;</a></span>
      
    </span>
</div>
    <script src="../js/theme.js"></script>

</body>
</html>
