{
    "docs": [
        {
            "location": "/overview/", 
            "text": "We build this framework with the following design goals in mind.\n\n\n\n\n\n\nEase of use.\n One should be able train a  model with single command. The input data are raw text files instead of esoteric file formats. Similarly, using a pre-trained model to make predictions should be straightforward.\n\n\n\n\n\n\nEase to extend\n. Code is structured in such a way that it is easy to build upon. For example, adding a new type of attention mechanism or a new encoder architecture requires only minimal code changes.\n\n\n\n\n\n\nWell-documented\n. In addition to \ngenerated API documentation\n we have written up multiple guides to help users become familiar with the framework.\n\n\n\n\n\n\nGood performance\n. For the sake of code simplicity we have not tried to squeeze out the last bit of performance, but our implementation is fast enough to cover almost all production use cases. It also supports distributed training to trade off computational power and training time.\n\n\n\n\n\n\nStandard Benchmarks\n. We provide pre-trained models and benchmark results for several standard datasets. We hope these can  serve as a baseline for further research.", 
            "title": "Overview"
        }, 
        {
            "location": "/getting_started/", 
            "text": "TODO", 
            "title": "Getting Started"
        }, 
        {
            "location": "/getting_started/#todo", 
            "text": "", 
            "title": "TODO"
        }, 
        {
            "location": "/data/", 
            "text": "Pre-processed datasets\n\n\nSee the data generation notebooks\n for details on how this data was generated.\n\n\n\n\n\n\n\n\nDataset\n\n\nDescription\n\n\nTraining/Dev/Test Examples\n\n\nVocab Size\n\n\nURL\n\n\n\n\n\n\n\n\n\n\nWMT'16 EN-DE\n\n\nData for the \nWMT'16 Translation Task\n English to German. Training data is combined from Europarl v7, Common Crawl, and News v11. Development data is newstest2013. Test data is newstest2015.\n\n\n4.56M/3K/2.6K\n\n\n50k\n\n\nDownload\n\n\n\n\n\n\nToy Copy\n\n\nA toy dataset where the target sequence is equal to the source sequence. Thus, the network must learn to \"copy\" the source sequence.\n\n\n10k/1k/1k\n\n\n20\n\n\nDownload\n\n\n\n\n\n\nToy Reverse\n\n\nA toy dataset where the target sequence is equal to the reversed source sequence.\n\n\n10k/1k/1k\n\n\n20\n\n\nDownload\n\n\n\n\n\n\n\n\nTraining/Dev data in Parallel Text Format\n\n\nThe input pipeline expect parallel tokenized data in raw text format. That is, you need \nsources.txt\n and a \ntargets.txt\n file that contain corresponding sentences, aligned line-by-line. Each line corresponds to one input/output example. These words/tokens in these files must be separated by spaces.\n\n\nGenerating Vocabulary\n\n\nA vocabulary file is a raw text file that contains one token per line. The total number of lines is the size of the vocabulary, and each token is mapped to its line number. The special words \nUNK\n, \nSEQUENCE_START\n and \nSEQUENCE_END\n are not part of the vocabulary file, and correspond to \nvocab_size + 1\n, \nvocab_size + 2\n, and \nvocab_size + 3\n respectively\n\n\nGiven a raw text file of tokens separated by spaces you can generate a vocabulary file using the \ngenerate_vocab.py\n script:\n\n\n./seq2seq/scripts/generate_vocab.py \\\n  --input_file /data/source.txt \\\n  --output_file /data/source_vocab \\\n  --min_frequency 1 \\\n  --max_vocab_size 50000\n\n\n\n\nThe resulting vocabulary file contains one word per line.\n\n\nSubword Unit Preprocessing\n\n\nIn order to deal with an open vocabulary, rare words can be split into\nsubword units as proposed in [1]. This improves the model's translation\nperformance particularly on rare words. The authors propose to use\nByte Pair Encoding (BPE), a simple compression algorithm, for splitting\nwords into subwords. Starting from characters, BPE iteratively replaces the\nmost frequent pair of symbols with a new symbol. The final symbol vocabulary\nis equal to the size of the initial vocabulary plus the number of merge\noperations, which is the only hyperparameter of the method.\nTo apply BPE as a pre-processing step to your raw text input,\nfollow the below steps:\n\n\n\n\nDownload the open-source implementation of the paper\n   from \nhere\n.\n\n\nLearn the BPE encoding: \ncd subword-nmt\n.\n   \n./learn_bpe.py -s {num_operations} -i {train_file} -o {codes_file}\n.\n   \nnum_operations\n is the number of merge operations. Default is \n10000\n.\n\n\nApply the BPE encoding to the training and test files:\n  \n./apply_bpe.py -c {codes_file} -i {input_file} -o {output_file}\n.\n\n\n\n\nThe resulting BPE-processed files can be used as-is in place of the raw text\nfiles for training the NMT model.\n\n\nReferences:\n\n\n[1] Sennrich, R., Haddow, B., \n Birch, A. (2016). Neural Machine Translation of\nRare Words with Subword Units. In Proceedings of the 54th Annual Meeting of\nthe Association for Computational Linguistics (ACL 2016).\nRetrieved from http://arxiv.org/abs/1508.07909\n\n\nTraining/Dev data in TFRecords format (Old)\n\n\nThe input pipeline expects data in \nTFRecord\n format consisting of \ntf.Example\n protocol buffers. Each \nExample\n record contains the following fields:\n\n\n\n\npair_id (string)\n (optional) is a dataset-unique id for this example.\n\n\nsource_len (int64)\n is the length of the source sequence.\n\n\ntarget_len (int64)\n is the length of the target sequence.\n\n\nsource_tokens (string)\n is a list of source tokens.\n\n\ntarget_tokens (string)\n is a list of targets tokens.\n\n\n\n\nGiven a parallel corpus, i.e. source and target files aligned by line such as \nthose from WMT\n, we provide a \nscript\n to generate a corresponding TFRecords file:\n\n\n./seq2seq/scripts/generate_examples.py \\\n  --source_file /path/to/data/train.de.txt \\\n  --target_file /path/to/data/train.en.txt \\\n  --output_file /path/to/data/train.tfrecords", 
            "title": "Data"
        }, 
        {
            "location": "/data/#pre-processed-datasets", 
            "text": "See the data generation notebooks  for details on how this data was generated.     Dataset  Description  Training/Dev/Test Examples  Vocab Size  URL      WMT'16 EN-DE  Data for the  WMT'16 Translation Task  English to German. Training data is combined from Europarl v7, Common Crawl, and News v11. Development data is newstest2013. Test data is newstest2015.  4.56M/3K/2.6K  50k  Download    Toy Copy  A toy dataset where the target sequence is equal to the source sequence. Thus, the network must learn to \"copy\" the source sequence.  10k/1k/1k  20  Download    Toy Reverse  A toy dataset where the target sequence is equal to the reversed source sequence.  10k/1k/1k  20  Download", 
            "title": "Pre-processed datasets"
        }, 
        {
            "location": "/data/#trainingdev-data-in-parallel-text-format", 
            "text": "The input pipeline expect parallel tokenized data in raw text format. That is, you need  sources.txt  and a  targets.txt  file that contain corresponding sentences, aligned line-by-line. Each line corresponds to one input/output example. These words/tokens in these files must be separated by spaces.", 
            "title": "Training/Dev data in Parallel Text Format"
        }, 
        {
            "location": "/data/#generating-vocabulary", 
            "text": "A vocabulary file is a raw text file that contains one token per line. The total number of lines is the size of the vocabulary, and each token is mapped to its line number. The special words  UNK ,  SEQUENCE_START  and  SEQUENCE_END  are not part of the vocabulary file, and correspond to  vocab_size + 1 ,  vocab_size + 2 , and  vocab_size + 3  respectively  Given a raw text file of tokens separated by spaces you can generate a vocabulary file using the  generate_vocab.py  script:  ./seq2seq/scripts/generate_vocab.py \\\n  --input_file /data/source.txt \\\n  --output_file /data/source_vocab \\\n  --min_frequency 1 \\\n  --max_vocab_size 50000  The resulting vocabulary file contains one word per line.", 
            "title": "Generating Vocabulary"
        }, 
        {
            "location": "/data/#subword-unit-preprocessing", 
            "text": "In order to deal with an open vocabulary, rare words can be split into\nsubword units as proposed in [1]. This improves the model's translation\nperformance particularly on rare words. The authors propose to use\nByte Pair Encoding (BPE), a simple compression algorithm, for splitting\nwords into subwords. Starting from characters, BPE iteratively replaces the\nmost frequent pair of symbols with a new symbol. The final symbol vocabulary\nis equal to the size of the initial vocabulary plus the number of merge\noperations, which is the only hyperparameter of the method.\nTo apply BPE as a pre-processing step to your raw text input,\nfollow the below steps:   Download the open-source implementation of the paper\n   from  here .  Learn the BPE encoding:  cd subword-nmt .\n    ./learn_bpe.py -s {num_operations} -i {train_file} -o {codes_file} .\n    num_operations  is the number of merge operations. Default is  10000 .  Apply the BPE encoding to the training and test files:\n   ./apply_bpe.py -c {codes_file} -i {input_file} -o {output_file} .   The resulting BPE-processed files can be used as-is in place of the raw text\nfiles for training the NMT model.  References:  [1] Sennrich, R., Haddow, B.,   Birch, A. (2016). Neural Machine Translation of\nRare Words with Subword Units. In Proceedings of the 54th Annual Meeting of\nthe Association for Computational Linguistics (ACL 2016).\nRetrieved from http://arxiv.org/abs/1508.07909", 
            "title": "Subword Unit Preprocessing"
        }, 
        {
            "location": "/data/#trainingdev-data-in-tfrecords-format-old", 
            "text": "The input pipeline expects data in  TFRecord  format consisting of  tf.Example  protocol buffers. Each  Example  record contains the following fields:   pair_id (string)  (optional) is a dataset-unique id for this example.  source_len (int64)  is the length of the source sequence.  target_len (int64)  is the length of the target sequence.  source_tokens (string)  is a list of source tokens.  target_tokens (string)  is a list of targets tokens.   Given a parallel corpus, i.e. source and target files aligned by line such as  those from WMT , we provide a  script  to generate a corresponding TFRecords file:  ./seq2seq/scripts/generate_examples.py \\\n  --source_file /path/to/data/train.de.txt \\\n  --target_file /path/to/data/train.en.txt \\\n  --output_file /path/to/data/train.tfrecords", 
            "title": "Training/Dev data in TFRecords format (Old)"
        }, 
        {
            "location": "/models/", 
            "text": "TODO\n\n\nModel Library and Hyperparameter documentation", 
            "title": "Models"
        }, 
        {
            "location": "/models/#todo", 
            "text": "Model Library and Hyperparameter documentation", 
            "title": "TODO"
        }, 
        {
            "location": "/training/", 
            "text": "Input Files\n\n\nTo train a model you need to follow files. See \nData\n for more details on how to generate or download each of them.\n\n\n\n\nTraining data: Two parallel (line by line aligned) text files that are tokenized, i.e. have words separated by spaces.\n\n\nDevelopment data: Same format as the training data, but used for validation.\n\n\nSource vocabulary file: A file with one word per line that defines the source vocabulary.\n\n\nTarget vocabulary file: Same format as the source vocabulary, but for the target language.\n\n\n\n\nRunning Training\n\n\nFrom the root directory, run:\n\n\npython -m seq2seq.training.train \\\n--train_source=data/train.sources.txt \\\n--train_target=data/train.targets.txt \\\n--dev_source=data/dev.sources.txt \\\n--dev_target=data/dev.targets.txt \\\n--vocab_source=data/vocab_source \\\n--vocab_target=data/vocab_target \\\n--model AttentionSeq2Seq \\\n--batch_size=16 \\\n--hparams=\nrnn_cell.num_layers=2,\n  rnn_cell.type=GRUCell,\n  rnn_cell.num_units=128,\n  source.max_seq_len=40,\n  target.max_seq_len=40\n \\\n--output_dir=/path/to/model/dir \\\n--buckets=10,15,20,25,30,35\n\n\n\n\nHere, \ntrain_source\n, \ntrain_target\n \ndev_source\n, \ndev_source\n, \nvocab_source\n and \nvocab_target\n are the input files described above. \n\n\nmodel\n is the name of some class defined in \nseq2seq.models\n. Currently, the available models are:\n\n\n\n\nBasicSeq2Seq\n - Uses a unidirectional RNN encoder, passes state from encoder to decoder, and uses no attention mechanism.\n\n\nAttentionSeq2Seq\n - Uses a bidirectional RNN encoder and an attention mechanism.\n\n\n\n\nRefer to the source code comments for more details of these details.\n\n\nhparams\n are model-specific Hyperparameters. Refer to the \ndefault_params\n of model classes for a list of all available hyperparameters.\n\n\nbuckets\n is an optional argument that you can use to speed up training by bucketing training examples into batches of roughly equal length. \n10,15,20,25,30,35\n would result in 8 buckets: Sequences of length \n10\n, \n10..15\n, ..., and \n35\n.\n\n\nMonitoring Training\n\n\nIn addition to looking at the training output, you can use Tensorboard to monitor progress:\n\n\ntensorboard --logdir=/path/to/model/dir", 
            "title": "Training"
        }, 
        {
            "location": "/training/#input-files", 
            "text": "To train a model you need to follow files. See  Data  for more details on how to generate or download each of them.   Training data: Two parallel (line by line aligned) text files that are tokenized, i.e. have words separated by spaces.  Development data: Same format as the training data, but used for validation.  Source vocabulary file: A file with one word per line that defines the source vocabulary.  Target vocabulary file: Same format as the source vocabulary, but for the target language.", 
            "title": "Input Files"
        }, 
        {
            "location": "/training/#running-training", 
            "text": "From the root directory, run:  python -m seq2seq.training.train \\\n--train_source=data/train.sources.txt \\\n--train_target=data/train.targets.txt \\\n--dev_source=data/dev.sources.txt \\\n--dev_target=data/dev.targets.txt \\\n--vocab_source=data/vocab_source \\\n--vocab_target=data/vocab_target \\\n--model AttentionSeq2Seq \\\n--batch_size=16 \\\n--hparams= rnn_cell.num_layers=2,\n  rnn_cell.type=GRUCell,\n  rnn_cell.num_units=128,\n  source.max_seq_len=40,\n  target.max_seq_len=40  \\\n--output_dir=/path/to/model/dir \\\n--buckets=10,15,20,25,30,35  Here,  train_source ,  train_target   dev_source ,  dev_source ,  vocab_source  and  vocab_target  are the input files described above.   model  is the name of some class defined in  seq2seq.models . Currently, the available models are:   BasicSeq2Seq  - Uses a unidirectional RNN encoder, passes state from encoder to decoder, and uses no attention mechanism.  AttentionSeq2Seq  - Uses a bidirectional RNN encoder and an attention mechanism.   Refer to the source code comments for more details of these details.  hparams  are model-specific Hyperparameters. Refer to the  default_params  of model classes for a list of all available hyperparameters.  buckets  is an optional argument that you can use to speed up training by bucketing training examples into batches of roughly equal length.  10,15,20,25,30,35  would result in 8 buckets: Sequences of length  10 ,  10..15 , ..., and  35 .", 
            "title": "Running Training"
        }, 
        {
            "location": "/training/#monitoring-training", 
            "text": "In addition to looking at the training output, you can use Tensorboard to monitor progress:  tensorboard --logdir=/path/to/model/dir", 
            "title": "Monitoring Training"
        }, 
        {
            "location": "/inference/", 
            "text": "To make predictions with a trained model you need the following:\n\n\n\n\nA trained model directory, \nmodel_dir\n: The directory should contain the model checkpoint and a \nhparams.txt\n file with the hyperparameters used during training. This is the \noutput_dir\n you specify in the training script.\n\n\nA \nsource_text\n file in the same format as the \ntraining input data\n, one tokenized input example per line.\n\n\nVocabulary files for both source and targets, \nvocab_source\n and \nvocab_target\n. Same as used during training.\n\n\n\n\nGiven the above, you use \nscripts/infer.py\n as follows:\n\n\n./seq2seq/scripts/infer.py \\\n  --source sources.txt \\\n  --model_dir /tmp/model \\\n  --vocab_source vocab_source.txt \\\n  --vocab_target vocab_target.txt \n out.txt", 
            "title": "Inference"
        }, 
        {
            "location": "/benchmarks/", 
            "text": "", 
            "title": "Benchmarks"
        }, 
        {
            "location": "/profiling/", 
            "text": "There are various ways to profile your model, from printing out the number of parameters to viewing detailed timing and device placement information for all graph operations.\n\n\nMetadataCaptureHook\n\n\nThe easiest way to generate profiling information is to use the \nMetadataCaptureHook\n, which saves a full graph trace and timeline information for a single configurable training step. By default these files are saved a \nmetadata\n subdirectory of your model checkpoint directory.\n\n\nTimelines\n\n\nIf you used the \nMetadataCaptureHook\n you can view the generated \ntimeline.json\n file in your web browser:\n\n\n\n\nGo to \nchrome://tracing\n\n\nLoad the \ntimeline.json\n file that was saved by the \nMetadataCaptureHook\n.\n\n\n\n\nFor complicated graphs timeline files can become quite large and analyzing them using Chrome may be slow and inconvenient. \n\n\nProfiling Script\n\n\nAn easy way to get basic information about your model is to run the \nprofile.py\n script. It uses \nTFProf\n to read metadata saved by the \nMetadataCaptureHook\n and generates several analyses.\n\n\n/seq2seq/scripts/profile.py --model_dir=/path/to/model/dir\n\n\n\n\nThis command will generate 4 files:\n\n\n/path/to/model/dir/params.txt\n contains an analysis of model parameter, including the number of parameters and their shapes and sizes:\n\n\natt_seq2seq/attention_decoder/RNN/rnn_step/logits/weights (384x103, 39.55k/39.55k params, 158.21KB/158.21KB)\n\n\n\n\n/path/to/model/dir/flops.txt\n contains information about floating point operations:\n\n\natt_seq2seq/OptimizeLoss/gradients/att_seq2seq/attention_decoder/RNN/while/rnn_step/attention/inputs_att/MatMul_grad/MatMul (18.87m/18.87m flops, 1.64ms/1.64ms, /job:localhost/replica:0/task:0/cpu:0)\n\n\n\n\n/path/to/model/dir/micro.txt\n contains microsecond timing information for operations that take longer than 1 milliseconds, organized by graph structure:\n\n\natt_seq2seq/attention_decoder/RNN/while/rnn_step/attention/mul_1 (1.89ms/13.72ms, /job:localhost/replica:0/task:0/cpu:0)\n  att_seq2seq/attention_decoder/RNN/while/rnn_step/attention/inputs_att/MatMul (1.21ms/11.83ms, /job:localhost/replica:0/task:0/cpu:0)\n  ....\n\n\n\n\n/path/to/model/dir/device.txt\n contains detailed device placement information for all operations.", 
            "title": "Profiling"
        }, 
        {
            "location": "/profiling/#metadatacapturehook", 
            "text": "The easiest way to generate profiling information is to use the  MetadataCaptureHook , which saves a full graph trace and timeline information for a single configurable training step. By default these files are saved a  metadata  subdirectory of your model checkpoint directory.", 
            "title": "MetadataCaptureHook"
        }, 
        {
            "location": "/profiling/#timelines", 
            "text": "If you used the  MetadataCaptureHook  you can view the generated  timeline.json  file in your web browser:   Go to  chrome://tracing  Load the  timeline.json  file that was saved by the  MetadataCaptureHook .   For complicated graphs timeline files can become quite large and analyzing them using Chrome may be slow and inconvenient.", 
            "title": "Timelines"
        }, 
        {
            "location": "/profiling/#profiling-script", 
            "text": "An easy way to get basic information about your model is to run the  profile.py  script. It uses  TFProf  to read metadata saved by the  MetadataCaptureHook  and generates several analyses.  /seq2seq/scripts/profile.py --model_dir=/path/to/model/dir  This command will generate 4 files:  /path/to/model/dir/params.txt  contains an analysis of model parameter, including the number of parameters and their shapes and sizes:  att_seq2seq/attention_decoder/RNN/rnn_step/logits/weights (384x103, 39.55k/39.55k params, 158.21KB/158.21KB)  /path/to/model/dir/flops.txt  contains information about floating point operations:  att_seq2seq/OptimizeLoss/gradients/att_seq2seq/attention_decoder/RNN/while/rnn_step/attention/inputs_att/MatMul_grad/MatMul (18.87m/18.87m flops, 1.64ms/1.64ms, /job:localhost/replica:0/task:0/cpu:0)  /path/to/model/dir/micro.txt  contains microsecond timing information for operations that take longer than 1 milliseconds, organized by graph structure:  att_seq2seq/attention_decoder/RNN/while/rnn_step/attention/mul_1 (1.89ms/13.72ms, /job:localhost/replica:0/task:0/cpu:0)\n  att_seq2seq/attention_decoder/RNN/while/rnn_step/attention/inputs_att/MatMul (1.21ms/11.83ms, /job:localhost/replica:0/task:0/cpu:0)\n  ....  /path/to/model/dir/device.txt  contains detailed device placement information for all operations.", 
            "title": "Profiling Script"
        }, 
        {
            "location": "/developer_guide/", 
            "text": "Development Setup\n\n\n\n\nInstall Python3. If you're on a Mac the easiest way to do this is probably using \nHomebrew\n: \nbrew install python3\n\n\nClone this repository: \ngit clone https://github.com/dennybritz/seq2seq.git\n. Change into it: \ncd seq2seq\n\n\nCreate a new virtual environment and activate it: \npython3 -m venv ~/path/to/your/venv\n. Then, \nsource ~/path/to/your/venv/bin/activate\n\n\nInstall package dependencies: \npip install -e .\n\n\nInstall testing utilities: \npip install nose pylint tox yapf\n\n\nRun tests and make sure they pass: \nnosetests\n\n\nCode :)\n\n\n\n\nGithub Workflow\n\n\nPushing directly to the master branch is blocked. In order to make changes you must:\n\n\n\n\nMake a new branch for your feature. For example, \ngit checkout -b feature/my-new-feature\n\n\nMake changes and commits\n\n\nRun:\n\n\nnosetests\n to make sure tests are passing\n\n\npylint ./seq2seq\n for linting and catching obvious errors\n\n\nyapf -ir ./seq2seq\n to auto-format code\n\n\nPush your new branch to Github: \ngit push\n\n\nCreate a Pull Request on Github and make sure CircleCI tests are passing\n\n\nHave one person review before merging the change\n\n\n\n\nTo make things easier you can also use the \nGithub Desktop app\n.\n\n\nGeneral Style Guidelines\n\n\n\n\nRun \nYAPF\n to format your code, e.g. \nyapf -ir ./seq2seq\n.\n\n\nRun \npylint\n.\n\n\nCode must be compatible with Python 2/3 using \nfuturize\n. That is, code should be written in Python 3 style and made backwards compatible with Python 2 by adding the appropriate imports.\n\n\nAll public functions and classes must have docstring \nfollowing this style\n.\n\n\nWhen in doubt, follow \nthis Python style guide\n. Running pylint should take care of most issues though.\n\n\n\n\nTensorflow Style\n\n\nGraphModule\n\n\nAll classes that modify the Graph should inherit from \nseq2seq.graph_module.GraphModule\n, which is a wrapper around Tensorflow's \ntf.make_template\n function that allows for easy variable sharing. Basically, it allows you to do something like this:\n\n\nencode_fn = SomeEncoderModule(...)\n\n# New variables are created in this call.\noutput1 = encode_fn(input1)\n\n# No new variables are created here. The variables from the above call are re-used.\n# Note how this is different from normal Tensorflow where you would need to use variable scopes.\noutput2 = encode_fn(input2)\n\n# Because this is a new instance a second set of variables is created\nencode_fn2 = SomeEncoderModule(...)\noutput3 = encode_fn2(input3)\n\n\n\n\nFunctions vs. Classes\n\n\n\n\nOperations that \ncreate new variables\n must be implemented as classes and must inherit from \nGraphModule\n.\n\n\nOperations that \ndo not create new variables\n can be implemented as standard python functions, or as classes that inherit from \nGraphModule\n if they have a lot of logic.", 
            "title": "Developer Guide"
        }, 
        {
            "location": "/developer_guide/#development-setup", 
            "text": "Install Python3. If you're on a Mac the easiest way to do this is probably using  Homebrew :  brew install python3  Clone this repository:  git clone https://github.com/dennybritz/seq2seq.git . Change into it:  cd seq2seq  Create a new virtual environment and activate it:  python3 -m venv ~/path/to/your/venv . Then,  source ~/path/to/your/venv/bin/activate  Install package dependencies:  pip install -e .  Install testing utilities:  pip install nose pylint tox yapf  Run tests and make sure they pass:  nosetests  Code :)", 
            "title": "Development Setup"
        }, 
        {
            "location": "/developer_guide/#github-workflow", 
            "text": "Pushing directly to the master branch is blocked. In order to make changes you must:   Make a new branch for your feature. For example,  git checkout -b feature/my-new-feature  Make changes and commits  Run:  nosetests  to make sure tests are passing  pylint ./seq2seq  for linting and catching obvious errors  yapf -ir ./seq2seq  to auto-format code  Push your new branch to Github:  git push  Create a Pull Request on Github and make sure CircleCI tests are passing  Have one person review before merging the change   To make things easier you can also use the  Github Desktop app .", 
            "title": "Github Workflow"
        }, 
        {
            "location": "/developer_guide/#general-style-guidelines", 
            "text": "Run  YAPF  to format your code, e.g.  yapf -ir ./seq2seq .  Run  pylint .  Code must be compatible with Python 2/3 using  futurize . That is, code should be written in Python 3 style and made backwards compatible with Python 2 by adding the appropriate imports.  All public functions and classes must have docstring  following this style .  When in doubt, follow  this Python style guide . Running pylint should take care of most issues though.", 
            "title": "General Style Guidelines"
        }, 
        {
            "location": "/developer_guide/#tensorflow-style", 
            "text": "", 
            "title": "Tensorflow Style"
        }, 
        {
            "location": "/developer_guide/#graphmodule", 
            "text": "All classes that modify the Graph should inherit from  seq2seq.graph_module.GraphModule , which is a wrapper around Tensorflow's  tf.make_template  function that allows for easy variable sharing. Basically, it allows you to do something like this:  encode_fn = SomeEncoderModule(...)\n\n# New variables are created in this call.\noutput1 = encode_fn(input1)\n\n# No new variables are created here. The variables from the above call are re-used.\n# Note how this is different from normal Tensorflow where you would need to use variable scopes.\noutput2 = encode_fn(input2)\n\n# Because this is a new instance a second set of variables is created\nencode_fn2 = SomeEncoderModule(...)\noutput3 = encode_fn2(input3)", 
            "title": "GraphModule"
        }, 
        {
            "location": "/developer_guide/#functions-vs-classes", 
            "text": "Operations that  create new variables  must be implemented as classes and must inherit from  GraphModule .  Operations that  do not create new variables  can be implemented as standard python functions, or as classes that inherit from  GraphModule  if they have a lot of logic.", 
            "title": "Functions vs. Classes"
        }
    ]
}