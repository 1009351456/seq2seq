{
    "docs": [
        {
            "location": "/", 
            "text": "Introduction\n\n\ntf-seq2seq is an open source framework for sequence learning in Tensorflow.\nThe main use case is Neural Machine Translation (NMT), but tf-seq2seq can be\nused for a variety of other applications such as Conversational Modeling,\nText Summarization or Image Captioning.\n\n\nDesign Goals\n\n\nWe built tf-seq2seq with the following goals in mind:\n\n\n\n\n\n\nUsability\n. You can train a model with a single command. The input data are stored in raw text rather than obscure file formats.\n\n\n\n\n\n\nExtensibility\n. Code is structured so that it is easy to build upon. For example, adding a new attention mechanism or encoder architecture requires only minimal code changes.\n\n\n\n\n\n\nFull Documentation\n. All code is documented using standard Python docstrings, and we have written guides to help you get started with tf-seq2seq.\n\n\n\n\n\n\nGood Performance\n. For the sake of code simplicity, we do not try to squeeze out every last bit of performance, but the implementation is fast enough to cover almost all production and research use cases. tf-seq2seq also supports distributed training to trade off computational power and training time.\n\n\n\n\n\n\nStandard Benchmarks\n. We provide \nconfiguration files and benchmark results\n for standard datasets and models. We hope these can serve as a baseline for future research.\n\n\n\n\n\n\nRelated Frameworks\n\n\nThe following frameworks offer functionality similar to that of tf-seq2seq. We hope to collaborate with the authors of these frameworks so that we can learn from each other.\n\n\n\n\nOpenNMT (Torch)\n\n\nNeural Monkey (Tensorflow)\n\n\nNEMATUS (Theano)", 
            "title": "Overview"
        }, 
        {
            "location": "/#introduction", 
            "text": "tf-seq2seq is an open source framework for sequence learning in Tensorflow.\nThe main use case is Neural Machine Translation (NMT), but tf-seq2seq can be\nused for a variety of other applications such as Conversational Modeling,\nText Summarization or Image Captioning.", 
            "title": "Introduction"
        }, 
        {
            "location": "/#design-goals", 
            "text": "We built tf-seq2seq with the following goals in mind:    Usability . You can train a model with a single command. The input data are stored in raw text rather than obscure file formats.    Extensibility . Code is structured so that it is easy to build upon. For example, adding a new attention mechanism or encoder architecture requires only minimal code changes.    Full Documentation . All code is documented using standard Python docstrings, and we have written guides to help you get started with tf-seq2seq.    Good Performance . For the sake of code simplicity, we do not try to squeeze out every last bit of performance, but the implementation is fast enough to cover almost all production and research use cases. tf-seq2seq also supports distributed training to trade off computational power and training time.    Standard Benchmarks . We provide  configuration files and benchmark results  for standard datasets and models. We hope these can serve as a baseline for future research.", 
            "title": "Design Goals"
        }, 
        {
            "location": "/#related-frameworks", 
            "text": "The following frameworks offer functionality similar to that of tf-seq2seq. We hope to collaborate with the authors of these frameworks so that we can learn from each other.   OpenNMT (Torch)  Neural Monkey (Tensorflow)  NEMATUS (Theano)", 
            "title": "Related Frameworks"
        }, 
        {
            "location": "/getting_started/", 
            "text": "Download \n Setup\n\n\nTo use tf-seq2seq you need a working installation of TensorFlow 1.0 with\nPython 2.7 or Python 3.5. Follow the \nTensorFlow Getting Started\n guide for detailed setup instructions. With TensorFlow installed, you can clone this repository:\n\n\ngit clone https://github.com/dennybritz/seq2seq.git\ncd seq2seq\n\n\n\n\nTo make sure everything works as expect you can run a simple pipeline unit test:\n\n\npython -m unittest seq2seq.test.pipeline_test\n\n\n\n\nIf you see a \"success\" message, you are all set. Note that you may need to install pyrouge, pyyaml, and matplotlib, in order for these tests to pass. If you run into other setup issues,\nplease \nfile a Github issue\n.\n\n\nUsing a pre-trained model\n\n\nComing soon!\n\n\nTraining your own model\n\n\nLet's look at how you can train a model based on your own data. In this section, we will train a model that learns to reverse an input sequence. While this task is not very useful in practice, we can train such a model quickly and use it as as sanity-check to make sure that the end-to-end pipeline is working as intended.\n\n\nFirst, let's generate some data:\n\n\nDATA_TYPE=reverse ./bin/data/toy.sh\n\n\n\n\nTo train a new model, we will use the files that are generated by this script, which are described in detail in the table below. To use your own data, you will need to generate corresponding files in the same format.\n\n\n\n\n\n\n\n\nArgument Name\n\n\nDescription\n\n\n\n\n\n\n\n\n\n\ntrain_source\n\n\nThe input sequences. A plain text file where each line corresponds to one input example. Tokens/words are separated by spaces. For natural language data, you may need to use a tokenizer to generate this. See \ntools\n for more detail.\n\n\n\n\n\n\ntrain_target\n\n\nThe target sequences in the same format as \ntrain_source\n. Each line corresponds to the \"translation\" of the input sequence in the same line in \ntrain_source\n. Thus, the number of lines in these two files must be equal.\n\n\n\n\n\n\ndev_source\n\n\nSame as \ntrain_source\n but used for validation/development.\n\n\n\n\n\n\ndev_target\n\n\nSame as \ntrain_target\n but used for validation/development.\n\n\n\n\n\n\nvocab_source\n\n\nThe vocabulary for the source sequences in the form of a plain text file containing one word per line. All words that appear in the source text but are not in the vocabulary will be mapped to \nUNK\n (unknown) tokens. We provide a script to generate a vocabulary file. See \ntools\n for more detail.\n\n\n\n\n\n\nvocab_target\n\n\nSame as \nvocab_source\n but for the target sequences.\n\n\n\n\n\n\n\n\nGiven the above input files, you can now train a new model:\n\n\npython -m bin.train \\\n  --train_source $HOME/nmt_data/toy_reverse/train/sources.txt \\\n  --train_target $HOME/nmt_data/toy_reverse/train/targets.txt \\\n  --dev_source $HOME/nmt_data/toy_reverse/dev/sources.txt \\\n  --dev_target $HOME/nmt_data/toy_reverse/dev/targets.txt \\\n  --vocab_source $HOME/nmt_data/toy_reverse/train/vocab.sources.txt \\\n  --vocab_target $HOME/nmt_data/toy_reverse/train/vocab.targets.txt \\\n  --model AttentionSeq2Seq \\\n  --batch_size 32 \\\n  --train_steps 2000 \\\n  --output_dir ${TMPDIR:-/tmp}/nmt_toy_reverse\n\n\n\n\nOn a CPU, the training may take up to 15 minutes. With the trained model, you can run inference and make predictions as follows:\n\n\npython -m bin.infer \\\n  --source $HOME/nmt_data/toy_reverse/test/sources.txt \\\n  --model_dir ${TMPDIR:-/tmp}/nmt_toy_reverse \\\n  \n ${TMPDIR:-/tmp}/nmt_toy_reverse/predictions.txt\n\n# Evaluate BLEU score using multi-bleu script from MOSES\n./bin/tools/multi-bleu.perl $HOME/nmt_data/toy_reverse/test/targets.txt \n ${TMPDIR:-/tmp}/nmt_toy_reverse/predictions.txt\n\n\n\n\nTo learn more about available models, data and tools, please see the corresponding sections of the documentation.", 
            "title": "Getting Started"
        }, 
        {
            "location": "/getting_started/#download-setup", 
            "text": "To use tf-seq2seq you need a working installation of TensorFlow 1.0 with\nPython 2.7 or Python 3.5. Follow the  TensorFlow Getting Started  guide for detailed setup instructions. With TensorFlow installed, you can clone this repository:  git clone https://github.com/dennybritz/seq2seq.git\ncd seq2seq  To make sure everything works as expect you can run a simple pipeline unit test:  python -m unittest seq2seq.test.pipeline_test  If you see a \"success\" message, you are all set. Note that you may need to install pyrouge, pyyaml, and matplotlib, in order for these tests to pass. If you run into other setup issues,\nplease  file a Github issue .", 
            "title": "Download &amp; Setup"
        }, 
        {
            "location": "/getting_started/#using-a-pre-trained-model", 
            "text": "Coming soon!", 
            "title": "Using a pre-trained model"
        }, 
        {
            "location": "/getting_started/#training-your-own-model", 
            "text": "Let's look at how you can train a model based on your own data. In this section, we will train a model that learns to reverse an input sequence. While this task is not very useful in practice, we can train such a model quickly and use it as as sanity-check to make sure that the end-to-end pipeline is working as intended.  First, let's generate some data:  DATA_TYPE=reverse ./bin/data/toy.sh  To train a new model, we will use the files that are generated by this script, which are described in detail in the table below. To use your own data, you will need to generate corresponding files in the same format.     Argument Name  Description      train_source  The input sequences. A plain text file where each line corresponds to one input example. Tokens/words are separated by spaces. For natural language data, you may need to use a tokenizer to generate this. See  tools  for more detail.    train_target  The target sequences in the same format as  train_source . Each line corresponds to the \"translation\" of the input sequence in the same line in  train_source . Thus, the number of lines in these two files must be equal.    dev_source  Same as  train_source  but used for validation/development.    dev_target  Same as  train_target  but used for validation/development.    vocab_source  The vocabulary for the source sequences in the form of a plain text file containing one word per line. All words that appear in the source text but are not in the vocabulary will be mapped to  UNK  (unknown) tokens. We provide a script to generate a vocabulary file. See  tools  for more detail.    vocab_target  Same as  vocab_source  but for the target sequences.     Given the above input files, you can now train a new model:  python -m bin.train \\\n  --train_source $HOME/nmt_data/toy_reverse/train/sources.txt \\\n  --train_target $HOME/nmt_data/toy_reverse/train/targets.txt \\\n  --dev_source $HOME/nmt_data/toy_reverse/dev/sources.txt \\\n  --dev_target $HOME/nmt_data/toy_reverse/dev/targets.txt \\\n  --vocab_source $HOME/nmt_data/toy_reverse/train/vocab.sources.txt \\\n  --vocab_target $HOME/nmt_data/toy_reverse/train/vocab.targets.txt \\\n  --model AttentionSeq2Seq \\\n  --batch_size 32 \\\n  --train_steps 2000 \\\n  --output_dir ${TMPDIR:-/tmp}/nmt_toy_reverse  On a CPU, the training may take up to 15 minutes. With the trained model, you can run inference and make predictions as follows:  python -m bin.infer \\\n  --source $HOME/nmt_data/toy_reverse/test/sources.txt \\\n  --model_dir ${TMPDIR:-/tmp}/nmt_toy_reverse \\\n    ${TMPDIR:-/tmp}/nmt_toy_reverse/predictions.txt\n\n# Evaluate BLEU score using multi-bleu script from MOSES\n./bin/tools/multi-bleu.perl $HOME/nmt_data/toy_reverse/test/targets.txt   ${TMPDIR:-/tmp}/nmt_toy_reverse/predictions.txt  To learn more about available models, data and tools, please see the corresponding sections of the documentation.", 
            "title": "Training your own model"
        }, 
        {
            "location": "/data/", 
            "text": "Ready-to-use Datasets\n\n\nWe provide data generation scripts to generate standard datasets.\n\n\n\n\n\n\n\n\nDataset\n\n\nDescription\n\n\nTraining/Dev/Test Size\n\n\nVocabulary\n\n\nDownload\n\n\n\n\n\n\n\n\n\n\nWMT'16 EN-DE\n\n\nData for the \nWMT'16 Translation Task\n English to German. Training data is combined from Europarl v7, Common Crawl, and News Commentary v11. Development data sets include \nnewstest[2010-2015]\n. \nnewstest2016\n should serve as test data. All SGM files were converted to plain text.\n\n\n4.56M/3K/2.6K\n\n\n50k Words \n 8/16/32k BPE\n\n\nGenerate\n\n\n\n\n\n\nToy Copy\n\n\nA toy dataset where the target sequence is equal to the source sequence. The model must learn to copy the source sequence.\n\n\n10k/1k/1k\n\n\n20\n\n\nGenerate\n\n\n\n\n\n\nToy Reverse\n\n\nA toy dataset where the target sequence is equal to the reversed source sequence. The model must learn to reverse the source sequence.\n\n\n10k/1k/1k\n\n\n20\n\n\nGenerate\n\n\n\n\n\n\n\n\nCreating your own data\n\n\nTo use your own data you must bring it into the right format. A typical data preprocessing pipeline looks as follows:\n\n\n\n\nGenerate data in parallel text format\n\n\nTokenize your data\n\n\nCreate fixed vocabularies for your source and target data\n\n\n(Optional) Use Subword Units to handle rare or unknown words\n\n\n\n\nParallel Text Format\n\n\nThe input pipeline expects parallel \nsources\n and \ntargets\n files where each pair of lines corresponds to one input/output example. The files must be in raw text format and tokenized with space delimiters. For example, an (extremely short and silly) Machine Translation dataset could consist of:\n\n\nSources:\n\n\nI love cats .\nWho are you ?\n\n\n\n\nTargets:\n\n\nJ` aime les chats .\nQui es tu ?\n\n\n\n\nTokenization\n\n\nFor good results, it is crucial to \ntokenize your text\n. Many tools to do tokenization are available, e.g.\n\n\n\n\nThe Moses \ntokenizer.perl\n script.\n\n\nLibraries such a \nspaCy\n, \nnltk\n or \nStanford Tokenizer\n.\n\n\n\n\nFor example, to use the Moses tokenizer:\n\n\n# Clone from Github\ngit clone https://github.com/moses-smt/mosesdecoder.git\n\n# Tokenize English (en) data\nmosesdecoder/scripts/tokenizer/tokenizer.perl -l en -threads 8 \n english_data \n english_data.tok\n\n# Tokenize German (de) data\nmosesdecoder/scripts/tokenizer/tokenizer.perl -l de -threads 8 \n german_data \n german_data.tok\n\n\n\n\nGenerating Vocabulary\n\n\nA vocabulary file is a raw text file that contains one word per line. The total number of lines is equal to the size of the vocabulary and each token is mapped to its line number. The special tokens \nUNK\n, \nSEQUENCE_START\n and \nSEQUENCE_END\n are generated by the model and are not included in the vocabulary file. Their corresponding vocabulary ids are \nvocab_size + 1\n, \nvocab_size + 2\n, and \nvocab_size + 3\n, respectively.\n\n\nWe provide a helper script \nbin/tools/generate_vocab.py\n that takes in a raw text file of space-delimited tokens and generates a vocabulary file:\n\n\n./bin/tools/generate_vocab.py \\\n  --input_file /data/source.txt \\\n  --output_file /data/source_vocab \\\n  --min_frequency 1 \\\n  --max_vocab_size 50000\n\n\n\n\nSubword Units (BPE)\n\n\nIn order to deal with an open vocabulary, rare words can be split into subword units as proposed in \nNeural Machine Translation of Rare Words with Subword Units\n. This improves the model's translation performance particularly on rare words. Subword units are calculated using Byte Pair Encoding (BPE), which iteratively replaces the most frequent pair of symbols with a new symbol. The final symbol vocabulary is equal to the size of an initial vocabulary, all characters appearing in the text, plus the number of merge operations, which is a hyperparameter of the method. To apply BPE as a pre-processing step to your raw (tokenized) text input, follow the instructions below:\n\n\n# Clone from Github\ngit clone https://github.com/rsennrich/subword-nmt\ncd subword-nmt\n\n# Learn a vocabulary using 10,000 merge operations\n./learn_bpe.py -s 10000 \n train.tok \n codes.bpe\n# Apply the vocabulary to the training file\n./apply_bpe.py -c codes.bpe \n train.tok \n train.tok.bpe\n\n\n\n\nThe resulting BPE-processed files can be used as-is in place of the raw text files for training the NMT model. Note that you must now use the BPE vocabulary as your vocabulary file. You can do this by generating a vocabulary based on the BPE-processed files.\n\n\nIn the BPE-processed file the original words are split using a special \n\"@@ \"\n string. To recover the original tokenization you can simply perform a \nsed \"s/@@ //g\"\n operation on the BPE files and the model output. For more details, refer to the paper and \nsubword-nmt\n Github repository.\n\n\nCharacter Vocabulary\n\n\nSometimes you want to run training on characters instead of words or subword units. The \nbin/tools/generate_char_vocab.py\n can generate a vocabulary file that contains the unique set of characters found in the text:\n\n\n./bin/tools/generate_char_vocab.py \\\n  \n /data/source.txt \\\n  \n /data/source_vocab.char.txt\n\n\n\n\nTo run training on characters you must pass the \n--delimiter=\"\"\n flag to the training script to avoid splitting words on spaces. See the \nTraining documentation\n for more details.", 
            "title": "Data"
        }, 
        {
            "location": "/data/#ready-to-use-datasets", 
            "text": "We provide data generation scripts to generate standard datasets.     Dataset  Description  Training/Dev/Test Size  Vocabulary  Download      WMT'16 EN-DE  Data for the  WMT'16 Translation Task  English to German. Training data is combined from Europarl v7, Common Crawl, and News Commentary v11. Development data sets include  newstest[2010-2015] .  newstest2016  should serve as test data. All SGM files were converted to plain text.  4.56M/3K/2.6K  50k Words   8/16/32k BPE  Generate    Toy Copy  A toy dataset where the target sequence is equal to the source sequence. The model must learn to copy the source sequence.  10k/1k/1k  20  Generate    Toy Reverse  A toy dataset where the target sequence is equal to the reversed source sequence. The model must learn to reverse the source sequence.  10k/1k/1k  20  Generate", 
            "title": "Ready-to-use Datasets"
        }, 
        {
            "location": "/data/#creating-your-own-data", 
            "text": "To use your own data you must bring it into the right format. A typical data preprocessing pipeline looks as follows:   Generate data in parallel text format  Tokenize your data  Create fixed vocabularies for your source and target data  (Optional) Use Subword Units to handle rare or unknown words", 
            "title": "Creating your own data"
        }, 
        {
            "location": "/data/#parallel-text-format", 
            "text": "The input pipeline expects parallel  sources  and  targets  files where each pair of lines corresponds to one input/output example. The files must be in raw text format and tokenized with space delimiters. For example, an (extremely short and silly) Machine Translation dataset could consist of:  Sources:  I love cats .\nWho are you ?  Targets:  J` aime les chats .\nQui es tu ?", 
            "title": "Parallel Text Format"
        }, 
        {
            "location": "/data/#tokenization", 
            "text": "For good results, it is crucial to  tokenize your text . Many tools to do tokenization are available, e.g.   The Moses  tokenizer.perl  script.  Libraries such a  spaCy ,  nltk  or  Stanford Tokenizer .   For example, to use the Moses tokenizer:  # Clone from Github\ngit clone https://github.com/moses-smt/mosesdecoder.git\n\n# Tokenize English (en) data\nmosesdecoder/scripts/tokenizer/tokenizer.perl -l en -threads 8   english_data   english_data.tok\n\n# Tokenize German (de) data\nmosesdecoder/scripts/tokenizer/tokenizer.perl -l de -threads 8   german_data   german_data.tok", 
            "title": "Tokenization"
        }, 
        {
            "location": "/data/#generating-vocabulary", 
            "text": "A vocabulary file is a raw text file that contains one word per line. The total number of lines is equal to the size of the vocabulary and each token is mapped to its line number. The special tokens  UNK ,  SEQUENCE_START  and  SEQUENCE_END  are generated by the model and are not included in the vocabulary file. Their corresponding vocabulary ids are  vocab_size + 1 ,  vocab_size + 2 , and  vocab_size + 3 , respectively.  We provide a helper script  bin/tools/generate_vocab.py  that takes in a raw text file of space-delimited tokens and generates a vocabulary file:  ./bin/tools/generate_vocab.py \\\n  --input_file /data/source.txt \\\n  --output_file /data/source_vocab \\\n  --min_frequency 1 \\\n  --max_vocab_size 50000", 
            "title": "Generating Vocabulary"
        }, 
        {
            "location": "/data/#subword-units-bpe", 
            "text": "In order to deal with an open vocabulary, rare words can be split into subword units as proposed in  Neural Machine Translation of Rare Words with Subword Units . This improves the model's translation performance particularly on rare words. Subword units are calculated using Byte Pair Encoding (BPE), which iteratively replaces the most frequent pair of symbols with a new symbol. The final symbol vocabulary is equal to the size of an initial vocabulary, all characters appearing in the text, plus the number of merge operations, which is a hyperparameter of the method. To apply BPE as a pre-processing step to your raw (tokenized) text input, follow the instructions below:  # Clone from Github\ngit clone https://github.com/rsennrich/subword-nmt\ncd subword-nmt\n\n# Learn a vocabulary using 10,000 merge operations\n./learn_bpe.py -s 10000   train.tok   codes.bpe\n# Apply the vocabulary to the training file\n./apply_bpe.py -c codes.bpe   train.tok   train.tok.bpe  The resulting BPE-processed files can be used as-is in place of the raw text files for training the NMT model. Note that you must now use the BPE vocabulary as your vocabulary file. You can do this by generating a vocabulary based on the BPE-processed files.  In the BPE-processed file the original words are split using a special  \"@@ \"  string. To recover the original tokenization you can simply perform a  sed \"s/@@ //g\"  operation on the BPE files and the model output. For more details, refer to the paper and  subword-nmt  Github repository.", 
            "title": "Subword Units (BPE)"
        }, 
        {
            "location": "/data/#character-vocabulary", 
            "text": "Sometimes you want to run training on characters instead of words or subword units. The  bin/tools/generate_char_vocab.py  can generate a vocabulary file that contains the unique set of characters found in the text:  ./bin/tools/generate_char_vocab.py \\\n    /data/source.txt \\\n    /data/source_vocab.char.txt  To run training on characters you must pass the  --delimiter=\"\"  flag to the training script to avoid splitting words on spaces. See the  Training documentation  for more details.", 
            "title": "Character Vocabulary"
        }, 
        {
            "location": "/training/", 
            "text": "Input Files\n\n\nIn order to train a model, you need the following files. Refer to \nData\n for more details on each of these.\n\n\n\n\nTraining data: Two parallel (aligned line by line) tokenized text files with tokens separated by spaces.\n\n\nDevelopment data: Same format as the training data, but used for validation.\n\n\nSource vocabulary: Defines the source vocabulary. A raw text file that contains one word per line.\n\n\nTarget vocabulary: Defines the target vocabulary. A raw text file that contains one word per line.\n\n\n\n\nRunning Training\n\n\nTo train a new model, run the training script below (also see \nGetting Started\n):\n\n\npython -m bin.train \\\n  --train_source $HOME/nmt_data/toy_reverse/train/sources.txt \\\n  --train_target $HOME/nmt_data/toy_reverse/train/targets.txt \\\n  --dev_source $HOME/nmt_data/toy_reverse/dev/sources.txt \\\n  --dev_target $HOME/nmt_data/toy_reverse/dev/targets.txt \\\n  --vocab_source $HOME/nmt_data/toy_reverse/train/vocab.sources.txt \\\n  --vocab_target $HOME/nmt_data/toy_reverse/train/vocab.targets.txt \\\n  --model AttentionSeq2Seq \\\n  --batch_size 32 \\\n  --train_steps 1500 \\\n  --hparams '\n      embedding.dim: 512\n      optimizer.name: Adam' \\\n  --output_dir ${TMPDIR}/nmt_toy_reverse\n\n\n\n\nPassing Hyperparameters and Configuration Files\n\n\nModel hyperparameters\n can be passed via the \nhparams\n flags. This flag is a string in \nYAML\n or JSON format.\n\n\nAn alternative to passing arguments to the training script is to define a configuration file in YAML format and pass it via the \nconfig_path\n flags. For example, the train command above would be expressed as follows in a configuration file:\n\n\ntrain_source: /home/nmt_data/toy_reverse/train/sources.txt\ntrain_target: /home/nmt_data/toy_reverse/train/targets.txt\ndev_source: /home/nmt_data/toy_reverse/dev/sources.txt\ndev_target: /home/nmt_data/toy_reverse/dev/targets.txt\nvocab_source: /home/nmt_data/toy_reverse/train/vocab.sources.txt\nvocab_target: /home/nmt_data/toy_reverse/train/vocab.targets.txt\nmodel: AttentionSeq2Seq\nbatch_size: 32\ntrain_steps: 1500\nhparams:\n  embedding.dim: 512\n  optimizer.name: Adam\noutput_dir: /tmp/nmt_toy_reverse\n\n\n\n\nNote that environment variables in configuration files are not yet supported. Flags defined as both command line arguments and configuration file values are overwritten by configuration file values. Command line flags not present in the configuration file will be merged. Parameters that are JSON objects on the command like (e.g. \ncell_params\n) can be defined as key-value pairs directly in the YAML file.\n\n\nDistributed Training\n\n\nDistributed Training is supported out of the box using \ntf.learn\n. Cluster Configurations can be specified using the \nTF_CONFIG\n environment variable, which is parsed by the \nRunConfig\n. Refer to the \nDistributed Tensorflow\n Guide for more information.\n\n\nMonitoring Training\n\n\nIn addition to looking at the output of the training script, Tensorflow write summaries and training logs into the specified \noutput_dir\n. Use \nTensorboard\n to visualize training progress.\n\n\ntensorboard --logdir=/path/to/model/dir\n\n\n\n\nTraining script arguments\n\n\nThe \ntrain.py\n script has many more options. Bold arguments are required.\n\n\n\n\n\n\n\n\nArgument\n\n\nDefault\n\n\nDescription\n\n\n\n\n\n\n\n\n\n\ntrain_source\n\n\n---\n\n\nPath to the training data source sentences. A raw text file with tokens separated by spaces.\n\n\n\n\n\n\ntrain_target\n\n\n---\n\n\nPath to the training data target sentences. A raw text file with tokens separated by spaces.\n\n\n\n\n\n\ndev_source\n\n\n---\n\n\nPath to the development data source sentences. Same format as training data.\n\n\n\n\n\n\ndev_target\n\n\n---\n\n\nPath to the development data target sentences. Same format as training data.\n\n\n\n\n\n\nvocab_source\n\n\n---\n\n\nPath to the source vocabulary. A raw text file with one word per line.\n\n\n\n\n\n\nvocab_target\n\n\n---\n\n\nPath to the target vocabulary. A raw text file with one word per line.\n\n\n\n\n\n\nsource_delimiter\n\n\n\" \"\n\n\nSplit source files into tokens on this delimiter. Defaults to \n\" \"\n (space).\n\n\n\n\n\n\ntarget_delimiter\n\n\n\" \"\n\n\nSplit target files into tokens on this delimiter. Defaults to \n\" \"\n (space).\n\n\n\n\n\n\nconfig_path\n\n\n---\n\n\nPath to a YAML configuration file defining FLAG values and hyperparameters. See below.\n\n\n\n\n\n\nmodel\n\n\nAttentionSeq2Seq\n\n\nThe model class to use. Refer to the documentation for all available models.\n\n\n\n\n\n\nbuckets\n\n\nNone\n\n\nBuckets input sequences according to these length. A comma-separated list of sequence length buckets, e.g. \n\"10,20,30\"\n would result in 4 buckets: \n10, 10-20, 20-30, \n30\n. \nNone\n disables bucketing.\n\n\n\n\n\n\nbatch_size\n\n\n16\n\n\nBatch size used for training and evaluation.\n\n\n\n\n\n\nhparams\n\n\nNone\n\n\nA comma-separated list of hyeperparameter values that overwrite the model defaults, e.g. \n\"optimizer.name=Adam,optimizer.learning_rate=0.1\"\n. Refer to the Models section and the TensorFlow documentation for a detailed list of available hyperparameters.\n\n\n\n\n\n\noutput_dir\n\n\nNone\n\n\nThe directory to write model checkpoints and summaries to. If None, a local temporary directory is created.\n\n\n\n\n\n\ntrain_steps\n\n\nNone\n\n\nMaximum number of training steps to run. If None, train forever.\n\n\n\n\n\n\ntrain_epochs\n\n\nNone\n\n\nMaximum number of training epochs over the data. If None, train forever.\n\n\n\n\n\n\neval_every_n_steps\n\n\n1000\n\n\nRun evaluation on validation data every N steps.\n\n\n\n\n\n\nsample_every_n_steps\n\n\n500\n\n\nSample and print sequence predictions every N steps during training.\n\n\n\n\n\n\nmetrics\n\n\nlog_perplexity,bleu\n\n\nComma-separated list of metrics to evaluate. Each metric must be defined in the \nMETRIC_SPECS_DICT\n in \nmetric_specs.py\n.\n\n\n\n\n\n\ntf_random_seed\n\n\nNone\n\n\nRandom seed for TensorFlow initializers. Setting this value allows consistency between reruns.\n\n\n\n\n\n\nsave_checkpoints_secs\n\n\n600\n\n\nSave checkpoints every N seconds. Can not be specified with \nsave_checkpoints_steps\n.\n\n\n\n\n\n\nsave_checkpoints_steps\n\n\nNone\n\n\nSave checkpoints every N steps. Can not be specified with \nsave_checkpoints_secs\n.\n\n\n\n\n\n\nkeep_checkpoint_max\n\n\n5\n\n\nMaximum number of recent checkpoint files to keep. As new files are created, older files are deleted. If None or 0, all checkpoint files are kept.\n\n\n\n\n\n\nkeep_checkpoint_every_n_hours\n\n\n4\n\n\nIn addition to keeping the most recent checkpoint files, keep one checkpoint file for every N hours of training.", 
            "title": "Training"
        }, 
        {
            "location": "/training/#input-files", 
            "text": "In order to train a model, you need the following files. Refer to  Data  for more details on each of these.   Training data: Two parallel (aligned line by line) tokenized text files with tokens separated by spaces.  Development data: Same format as the training data, but used for validation.  Source vocabulary: Defines the source vocabulary. A raw text file that contains one word per line.  Target vocabulary: Defines the target vocabulary. A raw text file that contains one word per line.", 
            "title": "Input Files"
        }, 
        {
            "location": "/training/#running-training", 
            "text": "To train a new model, run the training script below (also see  Getting Started ):  python -m bin.train \\\n  --train_source $HOME/nmt_data/toy_reverse/train/sources.txt \\\n  --train_target $HOME/nmt_data/toy_reverse/train/targets.txt \\\n  --dev_source $HOME/nmt_data/toy_reverse/dev/sources.txt \\\n  --dev_target $HOME/nmt_data/toy_reverse/dev/targets.txt \\\n  --vocab_source $HOME/nmt_data/toy_reverse/train/vocab.sources.txt \\\n  --vocab_target $HOME/nmt_data/toy_reverse/train/vocab.targets.txt \\\n  --model AttentionSeq2Seq \\\n  --batch_size 32 \\\n  --train_steps 1500 \\\n  --hparams '\n      embedding.dim: 512\n      optimizer.name: Adam' \\\n  --output_dir ${TMPDIR}/nmt_toy_reverse", 
            "title": "Running Training"
        }, 
        {
            "location": "/training/#passing-hyperparameters-and-configuration-files", 
            "text": "Model hyperparameters  can be passed via the  hparams  flags. This flag is a string in  YAML  or JSON format.  An alternative to passing arguments to the training script is to define a configuration file in YAML format and pass it via the  config_path  flags. For example, the train command above would be expressed as follows in a configuration file:  train_source: /home/nmt_data/toy_reverse/train/sources.txt\ntrain_target: /home/nmt_data/toy_reverse/train/targets.txt\ndev_source: /home/nmt_data/toy_reverse/dev/sources.txt\ndev_target: /home/nmt_data/toy_reverse/dev/targets.txt\nvocab_source: /home/nmt_data/toy_reverse/train/vocab.sources.txt\nvocab_target: /home/nmt_data/toy_reverse/train/vocab.targets.txt\nmodel: AttentionSeq2Seq\nbatch_size: 32\ntrain_steps: 1500\nhparams:\n  embedding.dim: 512\n  optimizer.name: Adam\noutput_dir: /tmp/nmt_toy_reverse  Note that environment variables in configuration files are not yet supported. Flags defined as both command line arguments and configuration file values are overwritten by configuration file values. Command line flags not present in the configuration file will be merged. Parameters that are JSON objects on the command like (e.g.  cell_params ) can be defined as key-value pairs directly in the YAML file.", 
            "title": "Passing Hyperparameters and Configuration Files"
        }, 
        {
            "location": "/training/#distributed-training", 
            "text": "Distributed Training is supported out of the box using  tf.learn . Cluster Configurations can be specified using the  TF_CONFIG  environment variable, which is parsed by the  RunConfig . Refer to the  Distributed Tensorflow  Guide for more information.", 
            "title": "Distributed Training"
        }, 
        {
            "location": "/training/#monitoring-training", 
            "text": "In addition to looking at the output of the training script, Tensorflow write summaries and training logs into the specified  output_dir . Use  Tensorboard  to visualize training progress.  tensorboard --logdir=/path/to/model/dir", 
            "title": "Monitoring Training"
        }, 
        {
            "location": "/training/#training-script-arguments", 
            "text": "The  train.py  script has many more options. Bold arguments are required.     Argument  Default  Description      train_source  ---  Path to the training data source sentences. A raw text file with tokens separated by spaces.    train_target  ---  Path to the training data target sentences. A raw text file with tokens separated by spaces.    dev_source  ---  Path to the development data source sentences. Same format as training data.    dev_target  ---  Path to the development data target sentences. Same format as training data.    vocab_source  ---  Path to the source vocabulary. A raw text file with one word per line.    vocab_target  ---  Path to the target vocabulary. A raw text file with one word per line.    source_delimiter  \" \"  Split source files into tokens on this delimiter. Defaults to  \" \"  (space).    target_delimiter  \" \"  Split target files into tokens on this delimiter. Defaults to  \" \"  (space).    config_path  ---  Path to a YAML configuration file defining FLAG values and hyperparameters. See below.    model  AttentionSeq2Seq  The model class to use. Refer to the documentation for all available models.    buckets  None  Buckets input sequences according to these length. A comma-separated list of sequence length buckets, e.g.  \"10,20,30\"  would result in 4 buckets:  10, 10-20, 20-30,  30 .  None  disables bucketing.    batch_size  16  Batch size used for training and evaluation.    hparams  None  A comma-separated list of hyeperparameter values that overwrite the model defaults, e.g.  \"optimizer.name=Adam,optimizer.learning_rate=0.1\" . Refer to the Models section and the TensorFlow documentation for a detailed list of available hyperparameters.    output_dir  None  The directory to write model checkpoints and summaries to. If None, a local temporary directory is created.    train_steps  None  Maximum number of training steps to run. If None, train forever.    train_epochs  None  Maximum number of training epochs over the data. If None, train forever.    eval_every_n_steps  1000  Run evaluation on validation data every N steps.    sample_every_n_steps  500  Sample and print sequence predictions every N steps during training.    metrics  log_perplexity,bleu  Comma-separated list of metrics to evaluate. Each metric must be defined in the  METRIC_SPECS_DICT  in  metric_specs.py .    tf_random_seed  None  Random seed for TensorFlow initializers. Setting this value allows consistency between reruns.    save_checkpoints_secs  600  Save checkpoints every N seconds. Can not be specified with  save_checkpoints_steps .    save_checkpoints_steps  None  Save checkpoints every N steps. Can not be specified with  save_checkpoints_secs .    keep_checkpoint_max  5  Maximum number of recent checkpoint files to keep. As new files are created, older files are deleted. If None or 0, all checkpoint files are kept.    keep_checkpoint_every_n_hours  4  In addition to keeping the most recent checkpoint files, keep one checkpoint file for every N hours of training.", 
            "title": "Training script arguments"
        }, 
        {
            "location": "/inference/", 
            "text": "Performing Inference\n\n\nAfter you have trained a model, you can use the \nbin/infer.py\n script to make predictions. For example, from the \nGetting Started Guide\n:\n\n\npython -m bin.infer \\\n  --source $HOME/nmt_data/toy_reverse/test/sources.txt \\\n  --model_dir ${TMPDIR:-/tmp}/nmt_toy_reverse \\\n  \n ${TMPDIR:-/tmp}/nmt_toy_reverse/predictions.txt\n\n\n\n\nThe inference script reads the model hyperparameters from the \ntrain_options.json\n file in the model directory, so\nyou do not need to pass them explicitly. By default, the latest model checkpoint found in \nmodel_dir\n is used, but you can also pass a specific checkpoint (e.g. \n${TMPDIR:-/tmp}/nmt_toy_reverse/model.ckpt-1562\n) via\nthe \ncheckpoint_path\n flag.\n\n\nBeam Search\n\n\nBeam Search is currently experimental.\n To perform beam search you can can set the \ninference.beam_search.beam_width\n model parameter to a number greater than 1 (see below). When using beam search, your batch size will be set to 1 and the \nbeam_width\n will be used as an implicit batch size. Beam search can become very expensive with large beam widths.\n\n\nOverwriting hyperparameters\n\n\nTo overwrite specific hyperparameters of a model, pass an hparams JSON object as the \nhparams\n flag to the inference script:\n\n\npython -m bin.infer \\\n  --source $HOME/nmt_data/toy_reverse/test/sources.txt \\\n  --model_dir ${TMPDIR:-/tmp}/nmt_toy_reverse \\\n  --hparams '\n      inference.beam_search.length_penalty_weight: 0.6\n      inference.beam_search.beam_width: 5' \\\n  \n ${TMPDIR:-/tmp}/nmt_toy_reverse/predictions.txt\n\n\n\n\nUNK token replacement using a Copy Mechanism\n\n\nRare words (such as place and people names) are often absent from the target vocabulary and result in \nUNK\n tokens in the output predictions. An easy strategy to improve predictions is to replace each \nUNK\n token with the word in the source sequence it is best aligned with. Alignments are typically calculated using an attention mechanism which produces \n[source_length]\n alignment scores for each target token.\n\n\nIf you trained a model that generates such attention scores (e.g. \nAttentionSeq2Seq\n), you can use them to perform UNK token replacement by passing the \nunk_replace\n flag to the inference script.\n\n\npython -m bin.infer \\\n  --source $HOME/nmt_data/toy_reverse/test/sources.txt \\\n  --model_dir ${TMPDIR:-/tmp}/nmt_toy_reverse \\\n  --unk_replace \\\n  \n ${TMPDIR:-/tmp}/nmt_toy_reverse/predictions.txt\n\n\n\n\nUNK token replacement using a mapping\n\n\nA slightly more sophisticated approach to UNK token replacement is to use a mapping instead of copying words from the source. For example, \"Munich\" is always translated as \"M\u00fcnchen\" in German, so that simply copying \"Munich\" from the source you would never result in the right translation even if the words are perfectly aligned using attention scores.\n\n\nOne strategy is to use \nfast_align\n to generate a mapping based on the conditional probabilities of target given source.\n\n\n# Download and build fast_align\ngit clone https://github.com/clab/fast_align.git\nmkdir fast_align/build \n cd fast_align/build\ncmake ../ \n make\n\n# Convert your data into a format that fast_align understands:\n# \nsource\n ||| \ntarget\n\npaste \\\n  $HOME/nmt_data/toy_reverse/train/sources.txt \\\n  $HOME/nmt_data/toy_reverse/train/targets.txt \\\n  | sed \ns/$(printf '\\t')/ ||| /g\n \n $HOME/nmt_data/toy_reverse/train/source_targets.fastalign\n\n# Learn alignments\n./fast_align \\\n  -i $HOME/nmt_data/toy_reverse/train/source_targets.fastalign \\\n  -v -p $HOME/nmt_data/toy_reverse/train/source_targets.cond \\\n  \n $HOME/nmt_data/toy_reverse/train/source_targets.align\n\n# Find the most probable traslation for each word and write them to a file\nsort -k1,1 -k3,3gr $HOME/nmt_data/toy_reverse/train/source_targets.cond \\\n  | sort -k1,1 -u \\\n  \n $HOME/nmt_data/toy_reverse/train/source_targets.cond.dict\n\n\n\n\n\nThe output file specified by the \n-p\n argument will contain conditional probabilities for \np(target | source)\n in the form of \nsource\n\\t\ntarget\n\\t\nprob\n. These can be used to do smarter UNK token replacement by passing the \nunk_mapping\n flag.\n\n\npython -m bin.infer \\\n  --source $HOME/nmt_data/toy_reverse/test/sources.txt \\\n  --model_dir ${TMPDIR:-/tmp}/nmt_toy_reverse \\\n  --unk_replace \\\n  --unk_mapping $HOME/nmt_data/toy_reverse/train/source_targets.cond.dict \\\n  \n ${TMPDIR:-/tmp}/nmt_toy_reverse/predictions.txt", 
            "title": "Inference"
        }, 
        {
            "location": "/inference/#performing-inference", 
            "text": "After you have trained a model, you can use the  bin/infer.py  script to make predictions. For example, from the  Getting Started Guide :  python -m bin.infer \\\n  --source $HOME/nmt_data/toy_reverse/test/sources.txt \\\n  --model_dir ${TMPDIR:-/tmp}/nmt_toy_reverse \\\n    ${TMPDIR:-/tmp}/nmt_toy_reverse/predictions.txt  The inference script reads the model hyperparameters from the  train_options.json  file in the model directory, so\nyou do not need to pass them explicitly. By default, the latest model checkpoint found in  model_dir  is used, but you can also pass a specific checkpoint (e.g.  ${TMPDIR:-/tmp}/nmt_toy_reverse/model.ckpt-1562 ) via\nthe  checkpoint_path  flag.", 
            "title": "Performing Inference"
        }, 
        {
            "location": "/inference/#beam-search", 
            "text": "Beam Search is currently experimental.  To perform beam search you can can set the  inference.beam_search.beam_width  model parameter to a number greater than 1 (see below). When using beam search, your batch size will be set to 1 and the  beam_width  will be used as an implicit batch size. Beam search can become very expensive with large beam widths.", 
            "title": "Beam Search"
        }, 
        {
            "location": "/inference/#overwriting-hyperparameters", 
            "text": "To overwrite specific hyperparameters of a model, pass an hparams JSON object as the  hparams  flag to the inference script:  python -m bin.infer \\\n  --source $HOME/nmt_data/toy_reverse/test/sources.txt \\\n  --model_dir ${TMPDIR:-/tmp}/nmt_toy_reverse \\\n  --hparams '\n      inference.beam_search.length_penalty_weight: 0.6\n      inference.beam_search.beam_width: 5' \\\n    ${TMPDIR:-/tmp}/nmt_toy_reverse/predictions.txt", 
            "title": "Overwriting hyperparameters"
        }, 
        {
            "location": "/inference/#unk-token-replacement-using-a-copy-mechanism", 
            "text": "Rare words (such as place and people names) are often absent from the target vocabulary and result in  UNK  tokens in the output predictions. An easy strategy to improve predictions is to replace each  UNK  token with the word in the source sequence it is best aligned with. Alignments are typically calculated using an attention mechanism which produces  [source_length]  alignment scores for each target token.  If you trained a model that generates such attention scores (e.g.  AttentionSeq2Seq ), you can use them to perform UNK token replacement by passing the  unk_replace  flag to the inference script.  python -m bin.infer \\\n  --source $HOME/nmt_data/toy_reverse/test/sources.txt \\\n  --model_dir ${TMPDIR:-/tmp}/nmt_toy_reverse \\\n  --unk_replace \\\n    ${TMPDIR:-/tmp}/nmt_toy_reverse/predictions.txt", 
            "title": "UNK token replacement using a Copy Mechanism"
        }, 
        {
            "location": "/inference/#unk-token-replacement-using-a-mapping", 
            "text": "A slightly more sophisticated approach to UNK token replacement is to use a mapping instead of copying words from the source. For example, \"Munich\" is always translated as \"M\u00fcnchen\" in German, so that simply copying \"Munich\" from the source you would never result in the right translation even if the words are perfectly aligned using attention scores.  One strategy is to use  fast_align  to generate a mapping based on the conditional probabilities of target given source.  # Download and build fast_align\ngit clone https://github.com/clab/fast_align.git\nmkdir fast_align/build   cd fast_align/build\ncmake ../   make\n\n# Convert your data into a format that fast_align understands:\n#  source  |||  target \npaste \\\n  $HOME/nmt_data/toy_reverse/train/sources.txt \\\n  $HOME/nmt_data/toy_reverse/train/targets.txt \\\n  | sed  s/$(printf '\\t')/ ||| /g    $HOME/nmt_data/toy_reverse/train/source_targets.fastalign\n\n# Learn alignments\n./fast_align \\\n  -i $HOME/nmt_data/toy_reverse/train/source_targets.fastalign \\\n  -v -p $HOME/nmt_data/toy_reverse/train/source_targets.cond \\\n    $HOME/nmt_data/toy_reverse/train/source_targets.align\n\n# Find the most probable traslation for each word and write them to a file\nsort -k1,1 -k3,3gr $HOME/nmt_data/toy_reverse/train/source_targets.cond \\\n  | sort -k1,1 -u \\\n    $HOME/nmt_data/toy_reverse/train/source_targets.cond.dict  The output file specified by the  -p  argument will contain conditional probabilities for  p(target | source)  in the form of  source \\t target \\t prob . These can be used to do smarter UNK token replacement by passing the  unk_mapping  flag.  python -m bin.infer \\\n  --source $HOME/nmt_data/toy_reverse/test/sources.txt \\\n  --model_dir ${TMPDIR:-/tmp}/nmt_toy_reverse \\\n  --unk_replace \\\n  --unk_mapping $HOME/nmt_data/toy_reverse/train/source_targets.cond.dict \\\n    ${TMPDIR:-/tmp}/nmt_toy_reverse/predictions.txt", 
            "title": "UNK token replacement using a mapping"
        }, 
        {
            "location": "/models/", 
            "text": "When calling the training script, you can specify a model class using the \n--model\n flags and model-specific hyperparameters using the \n--hparams\n flag. This page lists all supported models and hyperparameters.\n\n\nCommon Hyperparameters\n\n\nThe following hyperparameters are shared by all models, unless explicitly stated otherwise in the model section.\n\n\n\n\n\n\n\n\nName\n\n\nDefault\n\n\nDescription\n\n\n\n\n\n\n\n\n\n\nsource.max_seq_len\n\n\n50\n\n\nMaximum length of source sequences. An example is sliced to this length before being fed to the encoder.\n\n\n\n\n\n\nsource.reverse\n\n\nTrue\n\n\nIf set to true, reverse the source sequence before feeding it into the encoder.\n\n\n\n\n\n\ntarget.max_seq_len\n\n\n50\n\n\nMaximum length of target sequences. An example is sliced to this length before being fed to the decoder.\n\n\n\n\n\n\nembedding.dim\n\n\n100\n\n\nDimensionality of the embedding layer.\n\n\n\n\n\n\nembedding.share\n\n\nFalse\n\n\nIf set to true, share embedding parameters for source and target sequences.\n\n\n\n\n\n\ninference.max_decode_length\n\n\n100\n\n\nDuring inference mode, decode up to this length or until a \nSEQUENCE_END\n token is encountered, whichever happens first.\n\n\n\n\n\n\ninference.beam_search.beam_width\n\n\n0\n\n\nBeam Search beam width used during inference. A value of \n0\n or \n1\n disables beam search.\n\n\n\n\n\n\ninference.beam_search.length_penalty_weight\n\n\n0.0\n\n\nLength penalty factor applied to beam search hypotheses, as described in \nhttps://arxiv.org/abs/1609.08144\n.\n\n\n\n\n\n\noptimizer.name\n\n\nAdam\n\n\nType of Optimizer to use, e.g. \nAdam\n, \nSGD\n or \nMomentum\n. The name is fed to TensorFlow's \noptimize_loss\n function. See TensorFlow documentation for more details and all available options.\n\n\n\n\n\n\noptimizer.learning_rate\n\n\n1e-4\n\n\nInitial learning rate for the optimizer. This is fed to TensorFlow's \noptimize_loss\n function.\n\n\n\n\n\n\noptimizer.lr_decay_type\n\n\n\n\nThe name of one of TensorFlow's \nlearning rate decay functions\n defined in \ntf.train\n, e.g. \nexponential_decay\n. If this is an empty string (default) then no learning rate decay is used.\n\n\n\n\n\n\noptimizer.lr_decay_steps\n\n\n100\n\n\nHow often to apply decay. This is fed as the \ndecay_steps\n argument to the decay function defined above. See Tensoflow documentation for more details.\n\n\n\n\n\n\noptimizer.lr_decay_rate\n\n\n0.99\n\n\nThe decay rate. This is fed as the \ndecay_rate\n argument to the decay function defined above. See TensorFlow documentation for more details.\n\n\n\n\n\n\noptimizer.lr_start_decay_at\n\n\n0\n\n\nStart learning rate decay at this step.\n\n\n\n\n\n\noptimizer.lr_stop_decay_at\n\n\n1e9\n\n\nStop learning rate decay at this step.\n\n\n\n\n\n\noptimizer.lr_min_learning_rate\n\n\n1e-12\n\n\nNever decay below this learning rate.\n\n\n\n\n\n\noptimizer.lr_staircase\n\n\nFalse\n\n\nIf \nTrue\n, decay the learning rate at discrete intervals. This is fed as the \nstaircase\n argument to the decay function defined above. See TensorFlow documentation for more details.\n\n\n\n\n\n\noptimizer.clip_gradients\n\n\n5.0\n\n\nClip gradients by their global norm.\n\n\n\n\n\n\n\n\nBasicSeq2Seq\n\n\nThe \nBasicSeq2Seq\n model uses an encoder and decoder with no attention mechanism. The last encoder state is passed through a fully connected layer and used to initialize the decoder (this behavior can be changed using the \nbridge_spec\n hyperparameter). This is the \"vanilla\" implementation of the seq2seq architecture. The model supports the following additional hyperparameters.\n\n\n\n\n\n\n\n\nName\n\n\nDefault\n\n\nDescription\n\n\n\n\n\n\n\n\n\n\nbridge_spec\n\n\n{ \"class\": \"InitialStateBridge\"}\n\n\nA dictionary that defines how state is passed between encoder and decoder. The \nclass\n property corresponds to a name of bridge class defined in \nseq2seq.models.bridges\n. All additional properties in the dictinary are passed to the bridge class constructor, e.g. \n{\"class\": \"InitialStateBridge\", \"activation_fn\": \"tanh\"}\n.\n\n\n\n\n\n\nencoder.class\n\n\nseq2seq.encoders.UnidirectionalRNNEncoder\n\n\nType of encoder to use. See the \nEncoder Reference\n for more details and available encoders.\n\n\n\n\n\n\nencoder.params\n\n\n{}\n\n\nParameters passed to the encoder during construction. See the \nEncoder Reference\n for more details.\n\n\n\n\n\n\ndecoder.class\n\n\nseq2seq.decoders.BasicDecoder\n\n\nType of decoder to use. See the \nDecoder Reference\n for more details and available encoders.\n\n\n\n\n\n\ndecoder.params\n\n\n{}\n\n\nParameters passed to the decoder during construction. See the \nDecoder Reference\n for more details.\n\n\n\n\n\n\n\n\nAttentionSeq2seq\n\n\nAttentionSeq2seq\n is a sequence to sequence model that uses a unidirectional or bidirectional encoder and a decoder with an attention mechanism. By default, the last encoder state is not fed to the decoder. This implementation is comparable to the model in \nNeural Machine Translation by Jointly Learning to Align and Translate\n. This model supports the same parameters as the \nBasicSeq2Seq\n model, plus the following:\n\n\n\n\n\n\n\n\nName\n\n\nDefault\n\n\nDescription\n\n\n\n\n\n\n\n\n\n\nattention.dim\n\n\n128\n\n\nNumber of units in the attention layer.\n\n\n\n\n\n\nattention.score_type\n\n\ndot\n\n\nThe formula used to calculate attention scores. Available values are \nbahdanau\n and \ndot\n. \nbahdanau\n is described in \nNeural Machine Translation by Jointly Learning to Align and Translate\n. \ndot\n is described in \nEffective Approaches to Attention-based Neural Machine Translation\n.\n\n\n\n\n\n\nencoder.class\n\n\nseq2seq.encoders.BidirectionalRNNEncoder\n\n\nType of encoder to use. See the \nEncoder Reference\n for more details and available encoders.\n\n\n\n\n\n\ndecoder.class\n\n\nseq2seq.decoders.AttentionDecoder\n\n\nType of decoder to use. See the \nDecoder Reference\n for more details and available encoders.", 
            "title": "Model Reference"
        }, 
        {
            "location": "/models/#common-hyperparameters", 
            "text": "The following hyperparameters are shared by all models, unless explicitly stated otherwise in the model section.     Name  Default  Description      source.max_seq_len  50  Maximum length of source sequences. An example is sliced to this length before being fed to the encoder.    source.reverse  True  If set to true, reverse the source sequence before feeding it into the encoder.    target.max_seq_len  50  Maximum length of target sequences. An example is sliced to this length before being fed to the decoder.    embedding.dim  100  Dimensionality of the embedding layer.    embedding.share  False  If set to true, share embedding parameters for source and target sequences.    inference.max_decode_length  100  During inference mode, decode up to this length or until a  SEQUENCE_END  token is encountered, whichever happens first.    inference.beam_search.beam_width  0  Beam Search beam width used during inference. A value of  0  or  1  disables beam search.    inference.beam_search.length_penalty_weight  0.0  Length penalty factor applied to beam search hypotheses, as described in  https://arxiv.org/abs/1609.08144 .    optimizer.name  Adam  Type of Optimizer to use, e.g.  Adam ,  SGD  or  Momentum . The name is fed to TensorFlow's  optimize_loss  function. See TensorFlow documentation for more details and all available options.    optimizer.learning_rate  1e-4  Initial learning rate for the optimizer. This is fed to TensorFlow's  optimize_loss  function.    optimizer.lr_decay_type   The name of one of TensorFlow's  learning rate decay functions  defined in  tf.train , e.g.  exponential_decay . If this is an empty string (default) then no learning rate decay is used.    optimizer.lr_decay_steps  100  How often to apply decay. This is fed as the  decay_steps  argument to the decay function defined above. See Tensoflow documentation for more details.    optimizer.lr_decay_rate  0.99  The decay rate. This is fed as the  decay_rate  argument to the decay function defined above. See TensorFlow documentation for more details.    optimizer.lr_start_decay_at  0  Start learning rate decay at this step.    optimizer.lr_stop_decay_at  1e9  Stop learning rate decay at this step.    optimizer.lr_min_learning_rate  1e-12  Never decay below this learning rate.    optimizer.lr_staircase  False  If  True , decay the learning rate at discrete intervals. This is fed as the  staircase  argument to the decay function defined above. See TensorFlow documentation for more details.    optimizer.clip_gradients  5.0  Clip gradients by their global norm.", 
            "title": "Common Hyperparameters"
        }, 
        {
            "location": "/models/#basicseq2seq", 
            "text": "The  BasicSeq2Seq  model uses an encoder and decoder with no attention mechanism. The last encoder state is passed through a fully connected layer and used to initialize the decoder (this behavior can be changed using the  bridge_spec  hyperparameter). This is the \"vanilla\" implementation of the seq2seq architecture. The model supports the following additional hyperparameters.     Name  Default  Description      bridge_spec  { \"class\": \"InitialStateBridge\"}  A dictionary that defines how state is passed between encoder and decoder. The  class  property corresponds to a name of bridge class defined in  seq2seq.models.bridges . All additional properties in the dictinary are passed to the bridge class constructor, e.g.  {\"class\": \"InitialStateBridge\", \"activation_fn\": \"tanh\"} .    encoder.class  seq2seq.encoders.UnidirectionalRNNEncoder  Type of encoder to use. See the  Encoder Reference  for more details and available encoders.    encoder.params  {}  Parameters passed to the encoder during construction. See the  Encoder Reference  for more details.    decoder.class  seq2seq.decoders.BasicDecoder  Type of decoder to use. See the  Decoder Reference  for more details and available encoders.    decoder.params  {}  Parameters passed to the decoder during construction. See the  Decoder Reference  for more details.", 
            "title": "BasicSeq2Seq"
        }, 
        {
            "location": "/models/#attentionseq2seq", 
            "text": "AttentionSeq2seq  is a sequence to sequence model that uses a unidirectional or bidirectional encoder and a decoder with an attention mechanism. By default, the last encoder state is not fed to the decoder. This implementation is comparable to the model in  Neural Machine Translation by Jointly Learning to Align and Translate . This model supports the same parameters as the  BasicSeq2Seq  model, plus the following:     Name  Default  Description      attention.dim  128  Number of units in the attention layer.    attention.score_type  dot  The formula used to calculate attention scores. Available values are  bahdanau  and  dot .  bahdanau  is described in  Neural Machine Translation by Jointly Learning to Align and Translate .  dot  is described in  Effective Approaches to Attention-based Neural Machine Translation .    encoder.class  seq2seq.encoders.BidirectionalRNNEncoder  Type of encoder to use. See the  Encoder Reference  for more details and available encoders.    decoder.class  seq2seq.decoders.AttentionDecoder  Type of decoder to use. See the  Decoder Reference  for more details and available encoders.", 
            "title": "AttentionSeq2seq"
        }, 
        {
            "location": "/encoders/", 
            "text": "Encoder Reference\n\n\nAll encoders inherit from the abstract \nEncoder\n defined in \nseq2seq.encoders.encoder\n and receive \nparams\n, \nmode\n arguments at instantiation time. Available hyperparameters vary by encoder class.\n\n\nUnidirectionalRNNEncoder\n\n\n\n\n\n\n\n\nName\n\n\nDefault\n\n\nDescription\n\n\n\n\n\n\n\n\n\n\nrnn_cell.cell_class\n\n\nBasicLSTMCell\n\n\nThe class of the rnn cell. Cell classes can be fully defined (e.g. \ntensorflow.contrib.rnn.BasicRNNCell\n) or must be in \ntf.contrib.rnn\n or \nseq2seq.contrib.rnn_cell\n.\n\n\n\n\n\n\nrnn_cell.cell_params\n\n\n{\"num_units\": 128}\n\n\nA dictionary of parameters to pass to the cell class constructor.\n\n\n\n\n\n\nrnn_cell.dropout_input_keep_prob\n\n\n1.0\n\n\nApply dropout to the (non-recurrent) inputs of each RNN layer using this keep probability. A value of \n1.0\n disables dropout.\n\n\n\n\n\n\nrnn_cell.dropout_output_keep_prob\n\n\n1.0\n\n\nApply dropout to the (non-recurrent) outputs of each RNN layer using this keep probability. A value of \n1.0\n disables dropout.\n\n\n\n\n\n\nrnn_cell.num_layers\n\n\n1\n\n\nNumber of RNN layers.\n\n\n\n\n\n\nrnn_cell.residual_connections\n\n\nFalse\n\n\nIf true, add residual connections between all RNN layers in the encoder.\n\n\n\n\n\n\n\n\nBidirectionalRNNEncoder\n\n\nSame as the \nUnidirectionalRNNEncoder\n. The same cell is used for forward and backward RNNs.\n\n\nStackBidirectionalRNNEncoder\n\n\nSame as the \nUnidirectionalRNNEncoder\n. The same cell is used for forward and backward RNNs.", 
            "title": "Encoder Reference"
        }, 
        {
            "location": "/encoders/#encoder-reference", 
            "text": "All encoders inherit from the abstract  Encoder  defined in  seq2seq.encoders.encoder  and receive  params ,  mode  arguments at instantiation time. Available hyperparameters vary by encoder class.", 
            "title": "Encoder Reference"
        }, 
        {
            "location": "/encoders/#unidirectionalrnnencoder", 
            "text": "Name  Default  Description      rnn_cell.cell_class  BasicLSTMCell  The class of the rnn cell. Cell classes can be fully defined (e.g.  tensorflow.contrib.rnn.BasicRNNCell ) or must be in  tf.contrib.rnn  or  seq2seq.contrib.rnn_cell .    rnn_cell.cell_params  {\"num_units\": 128}  A dictionary of parameters to pass to the cell class constructor.    rnn_cell.dropout_input_keep_prob  1.0  Apply dropout to the (non-recurrent) inputs of each RNN layer using this keep probability. A value of  1.0  disables dropout.    rnn_cell.dropout_output_keep_prob  1.0  Apply dropout to the (non-recurrent) outputs of each RNN layer using this keep probability. A value of  1.0  disables dropout.    rnn_cell.num_layers  1  Number of RNN layers.    rnn_cell.residual_connections  False  If true, add residual connections between all RNN layers in the encoder.", 
            "title": "UnidirectionalRNNEncoder"
        }, 
        {
            "location": "/encoders/#bidirectionalrnnencoder", 
            "text": "Same as the  UnidirectionalRNNEncoder . The same cell is used for forward and backward RNNs.", 
            "title": "BidirectionalRNNEncoder"
        }, 
        {
            "location": "/encoders/#stackbidirectionalrnnencoder", 
            "text": "Same as the  UnidirectionalRNNEncoder . The same cell is used for forward and backward RNNs.", 
            "title": "StackBidirectionalRNNEncoder"
        }, 
        {
            "location": "/decoders/", 
            "text": "Decoder Reference\n\n\nThe following tables list available decoder classes and their hyperparameters.\n\n\nBasicDecoder\n\n\n\n\n\n\n\n\nName\n\n\nDefault\n\n\nDescription\n\n\n\n\n\n\n\n\n\n\nrnn_cell.cell_class\n\n\nBasicLSTMCell\n\n\nThe class of the rnn cell. Cell classes can be fully defined (e.g. \ntensorflow.contrib.rnn.BasicRNNCell\n) or must be in \ntf.contrib.rnn\n or \nseq2seq.contrib.rnn_cell\n.\n\n\n\n\n\n\nrnn_cell.cell_params\n\n\n{\"num_units\": 128}\n\n\nA dictionary of parameters to pass to the cell class constructor.\n\n\n\n\n\n\nrnn_cell.dropout_input_keep_prob\n\n\n1.0\n\n\nApply dropout to the (non-recurrent) inputs of each RNN layer using this keep probability. A value of \n1.0\n disables dropout.\n\n\n\n\n\n\nrnn_cell.dropout_output_keep_prob\n\n\n1.0\n\n\nApply dropout to the (non-recurrent) outputs of each RNN layer using this keep probability. A value of \n1.0\n disables dropout.\n\n\n\n\n\n\nrnn_cell.num_layers\n\n\n1\n\n\nNumber of RNN layers.\n\n\n\n\n\n\nrnn_cell.residual_connections\n\n\nFalse\n\n\nIf true, add residual connections between all RNN layers in the encoder.\n\n\n\n\n\n\n\n\nAttentionDecoder\n\n\nSame as \nBasicDecoder\n.", 
            "title": "Decoder Reference"
        }, 
        {
            "location": "/decoders/#decoder-reference", 
            "text": "The following tables list available decoder classes and their hyperparameters.", 
            "title": "Decoder Reference"
        }, 
        {
            "location": "/decoders/#basicdecoder", 
            "text": "Name  Default  Description      rnn_cell.cell_class  BasicLSTMCell  The class of the rnn cell. Cell classes can be fully defined (e.g.  tensorflow.contrib.rnn.BasicRNNCell ) or must be in  tf.contrib.rnn  or  seq2seq.contrib.rnn_cell .    rnn_cell.cell_params  {\"num_units\": 128}  A dictionary of parameters to pass to the cell class constructor.    rnn_cell.dropout_input_keep_prob  1.0  Apply dropout to the (non-recurrent) inputs of each RNN layer using this keep probability. A value of  1.0  disables dropout.    rnn_cell.dropout_output_keep_prob  1.0  Apply dropout to the (non-recurrent) outputs of each RNN layer using this keep probability. A value of  1.0  disables dropout.    rnn_cell.num_layers  1  Number of RNN layers.    rnn_cell.residual_connections  False  If true, add residual connections between all RNN layers in the encoder.", 
            "title": "BasicDecoder"
        }, 
        {
            "location": "/decoders/#attentiondecoder", 
            "text": "Same as  BasicDecoder .", 
            "title": "AttentionDecoder"
        }, 
        {
            "location": "/tools/", 
            "text": "Visualizing Attention\n\n\nIf you train an \nAttentionSeq2Seq\n model, you can dump the raw attention scores and generate alignment visualizations during inference:\n\n\npython -m bin.infer \\\n  --source $HOME/nmt_data/toy_reverse/test/sources.txt \\\n  --model_dir ${TMPDIR:-/tmp}/nmt_toy_reverse \\\n  --dump_attention_dir ${TMPDIR:-/tmp}/attention_plots \n /dev/null\n\n\n\n\nBy default, this script generates an \nattention_score.npy\n array file and one attention plot per example. The array file can be \nloaded used numpy\n and will contain a list of arrays with shape \n[target_length, source_length]\n. If you only want the raw attention score data without the plots you can pass the \n--dump_atention_no_plot\n flag. For more details and additional options, see the \ninfer.py\n script.\n\n\nVisualizing Beam Search\n\n\nTo dump beam search debugging information you can pass the \n--dump_beams\n flag to the \ninfer.py script\n. This will\nwrite a numpy npz with the raw beam search data.\n\n\npython -m bin.infer \\\n  --source $HOME/nmt_data/toy_reverse/test/sources.txt \\\n  --model_dir ${TMPDIR:-/tmp}/nmt_toy_reverse \\\n  --hparams '\n      inference.beam_search.score_fn: length_normalized_score\n      inference.beam_search.beam_width: 5' \\\n  --dump_beams ${TMPDIR:-/tmp}/beams.npz \n /dev/null\n\n\n\n\nYou can inspect the beam search data by loading the array using numpy, or generate beam search visualizations using the\n\ngenerate_beam_viz.py\n script. This required the \nnetworkx\n module to be installed.\n\n\npython -m bin.tools.generate_beam_viz  \\\n  -o ${TMPDIR:-/tmp}/beam_visualizations \\\n  -d ${TMPDIR:-/tmp}/beams.npz \\\n  -v $HOME/nmt_data/toy_reverse//train/vocab.targets.txt\n\n\n\n\n\n\nModel Performance Profiling\n\n\nDuring training, the \nMetadataCaptureHook\n saves a full trace and timeline information for a single training step (step 10 by default) into a \nmetadata\n subdirectory of your model directory. You can view the generated \ntimeline.json\n file in Chrome:\n\n\n\n\nGo to \nchrome://tracing\n\n\nLoad the \ntimeline.json\n file that was saved in \n/path/to/model/dir/metadata\n\n\n\n\nFor large complicated graphs, the timeline files can become quite large and analyzing them using Chrome may be slow, which is why we also provide a \nprofile.py\n script that generates useful profiling information:\n\n\npython -m bin.tools.profile --model_dir=${TMPDIR:-/tmp}/nmt_toy_reverse\n\n\n\n\nThis command will generate the following four files:\n\n\n\n\n/path/to/model/dir/params.txt\n contains an analysis of model parameters, including the number of parameters and their shapes and sizes\n\n\n/path/to/model/dir/flops.txt\n contains information about long-running floating point operations per second (FLOPS)\n\n\n/path/to/model/dir/micro.txt\n contains microsecond timing information for operations that take longer than 1 millisecond, organized by graph structure\n\n\n/path/to/model/dir/device.txt\n contains detailed device placement information for all operations", 
            "title": "Tools"
        }, 
        {
            "location": "/tools/#visualizing-attention", 
            "text": "If you train an  AttentionSeq2Seq  model, you can dump the raw attention scores and generate alignment visualizations during inference:  python -m bin.infer \\\n  --source $HOME/nmt_data/toy_reverse/test/sources.txt \\\n  --model_dir ${TMPDIR:-/tmp}/nmt_toy_reverse \\\n  --dump_attention_dir ${TMPDIR:-/tmp}/attention_plots   /dev/null  By default, this script generates an  attention_score.npy  array file and one attention plot per example. The array file can be  loaded used numpy  and will contain a list of arrays with shape  [target_length, source_length] . If you only want the raw attention score data without the plots you can pass the  --dump_atention_no_plot  flag. For more details and additional options, see the  infer.py  script.", 
            "title": "Visualizing Attention"
        }, 
        {
            "location": "/tools/#visualizing-beam-search", 
            "text": "To dump beam search debugging information you can pass the  --dump_beams  flag to the  infer.py script . This will\nwrite a numpy npz with the raw beam search data.  python -m bin.infer \\\n  --source $HOME/nmt_data/toy_reverse/test/sources.txt \\\n  --model_dir ${TMPDIR:-/tmp}/nmt_toy_reverse \\\n  --hparams '\n      inference.beam_search.score_fn: length_normalized_score\n      inference.beam_search.beam_width: 5' \\\n  --dump_beams ${TMPDIR:-/tmp}/beams.npz   /dev/null  You can inspect the beam search data by loading the array using numpy, or generate beam search visualizations using the generate_beam_viz.py  script. This required the  networkx  module to be installed.  python -m bin.tools.generate_beam_viz  \\\n  -o ${TMPDIR:-/tmp}/beam_visualizations \\\n  -d ${TMPDIR:-/tmp}/beams.npz \\\n  -v $HOME/nmt_data/toy_reverse//train/vocab.targets.txt", 
            "title": "Visualizing Beam Search"
        }, 
        {
            "location": "/tools/#model-performance-profiling", 
            "text": "During training, the  MetadataCaptureHook  saves a full trace and timeline information for a single training step (step 10 by default) into a  metadata  subdirectory of your model directory. You can view the generated  timeline.json  file in Chrome:   Go to  chrome://tracing  Load the  timeline.json  file that was saved in  /path/to/model/dir/metadata   For large complicated graphs, the timeline files can become quite large and analyzing them using Chrome may be slow, which is why we also provide a  profile.py  script that generates useful profiling information:  python -m bin.tools.profile --model_dir=${TMPDIR:-/tmp}/nmt_toy_reverse  This command will generate the following four files:   /path/to/model/dir/params.txt  contains an analysis of model parameters, including the number of parameters and their shapes and sizes  /path/to/model/dir/flops.txt  contains information about long-running floating point operations per second (FLOPS)  /path/to/model/dir/micro.txt  contains microsecond timing information for operations that take longer than 1 millisecond, organized by graph structure  /path/to/model/dir/device.txt  contains detailed device placement information for all operations", 
            "title": "Model Performance Profiling"
        }, 
        {
            "location": "/benchmarks/", 
            "text": "Machine Translation: WMT'16 English-German\n\n\nSingle models only, no ensembles. Results are listed in chronological order.\n\n\n\n\n\n\n\n\nModel Name \n Reference\n\n\nSettings / Notes\n\n\nTraining Time\n\n\nTest Set BLEU\n\n\n\n\n\n\n\n\n\n\ntf-seq2seq\n\n\nConfiguration\n\n\n~4 days on 8 NVidia K80 GPUs\n\n\nnewstest2014: \n22.03\n \n newstest2015: \n24.75\n\n\n\n\n\n\nGehring, et al. (2016-11)\n \n Deep Convolutional 15/5\n\n\n\n\n---\n\n\nnewstest2014: - \n newstest2015: \n24.3\n\n\n\n\n\n\nWu et al. (2016-09)\n \n GNMT\n\n\n8 encoder/decoder layers, 1024 LSTM units, 32k shared wordpieces (similar to BPE); residual between layers connections; lots of other tricks; newstest2012 and newstest2013 as validation sets. Paper mentions 5M training examples, how is this possible if WMT only has ~4.5M?\n\n\n---\n\n\nnewstest2014:\n24.61\n \nnewstest2015: -\n\n\n\n\n\n\nZhou et al. (2016-06)\n \n Deep-Att\n\n\n\n\n---\n\n\nnewstest2014: \n20.6\n \n newstest2015: -\n\n\n\n\n\n\nChung, et al. (2016-03)\n \n BPE-Char\n\n\nCharacter-level decoder with BPE encoder.\n Based on Bahdanau attention model; Bidirectional encoder with 512 GRU units; 2-layer GRU decoder with 1024 units; Adam; batch size 128; gradient clipping at norm 1; Moses Tokenizer; limit sequences to 50 symbols in source and 100 symbols and 500 characters in target.\n\n\n---\n\n\nnewstest2014: \n21.5\n \n newstest2015: \n23.9\n\n\n\n\n\n\nSennrich et al. (2015-8)\n \n BPE\n\n\nAuthors propose BPE for subword unit nsegmentation as a pre/post-processing step to handle open vocabulary\n;  Base model is based on \nBahndanau's paper\n. Bidirectional encoder; GRU; 1000 hidden units; 1000 attention units; 620-dimensional word embeddings; single-layer; beam search width 12; Adadelta with batch size 80; Using \nGroundhog\n;\n\n\n\n\nnewstest2014: - \nnewstest2015: \n20.5\n\n\n\n\n\n\nLuong et al. (2015-08)\n\n\nNovel local/global attention mechanism;\n 50k vocabulary; 4 layers in encoder and decoder; unidirectional encoder; gradient clipping at norm 5;  1028 LSTM units, 1028-dimensional embeddings; (somewhat complicated) SGD decay schedule; dropout 0.2; UNK replace;\n\n\n---\n\n\nnewstest2014: \n20.9\n \n newstest2015: -\n\n\n\n\n\n\nJean et al. (2014-12)\n \n RNNsearch-LV\n\n\nAuthors propose a new sampling-based approach to incorporate a larger vocabulary\n; Base model is based on \nBahndanau's paper\n. Bidirectional encoder; GRU; 1000 hidden units; 1000 attention units; 620-dimensional word embeddings; single-layer; beam search width 12;\n\n\n---\n\n\nnewstest2014: \n19.4\n \n newstest2015: -\n\n\n\n\n\n\n\n\nMachine Translation: WMT'16 German-English\n\n\nComing soon.\n\n\nText Summarization: Gigaword\n\n\nComing soon.\n\n\nConversational Modeling\n\n\nComing soon.", 
            "title": "Benchmarks"
        }, 
        {
            "location": "/benchmarks/#machine-translation-wmt16-english-german", 
            "text": "Single models only, no ensembles. Results are listed in chronological order.     Model Name   Reference  Settings / Notes  Training Time  Test Set BLEU      tf-seq2seq  Configuration  ~4 days on 8 NVidia K80 GPUs  newstest2014:  22.03    newstest2015:  24.75    Gehring, et al. (2016-11)    Deep Convolutional 15/5   ---  newstest2014: -   newstest2015:  24.3    Wu et al. (2016-09)    GNMT  8 encoder/decoder layers, 1024 LSTM units, 32k shared wordpieces (similar to BPE); residual between layers connections; lots of other tricks; newstest2012 and newstest2013 as validation sets. Paper mentions 5M training examples, how is this possible if WMT only has ~4.5M?  ---  newstest2014: 24.61   newstest2015: -    Zhou et al. (2016-06)    Deep-Att   ---  newstest2014:  20.6    newstest2015: -    Chung, et al. (2016-03)    BPE-Char  Character-level decoder with BPE encoder.  Based on Bahdanau attention model; Bidirectional encoder with 512 GRU units; 2-layer GRU decoder with 1024 units; Adam; batch size 128; gradient clipping at norm 1; Moses Tokenizer; limit sequences to 50 symbols in source and 100 symbols and 500 characters in target.  ---  newstest2014:  21.5    newstest2015:  23.9    Sennrich et al. (2015-8)    BPE  Authors propose BPE for subword unit nsegmentation as a pre/post-processing step to handle open vocabulary ;  Base model is based on  Bahndanau's paper . Bidirectional encoder; GRU; 1000 hidden units; 1000 attention units; 620-dimensional word embeddings; single-layer; beam search width 12; Adadelta with batch size 80; Using  Groundhog ;   newstest2014: -  newstest2015:  20.5    Luong et al. (2015-08)  Novel local/global attention mechanism;  50k vocabulary; 4 layers in encoder and decoder; unidirectional encoder; gradient clipping at norm 5;  1028 LSTM units, 1028-dimensional embeddings; (somewhat complicated) SGD decay schedule; dropout 0.2; UNK replace;  ---  newstest2014:  20.9    newstest2015: -    Jean et al. (2014-12)    RNNsearch-LV  Authors propose a new sampling-based approach to incorporate a larger vocabulary ; Base model is based on  Bahndanau's paper . Bidirectional encoder; GRU; 1000 hidden units; 1000 attention units; 620-dimensional word embeddings; single-layer; beam search width 12;  ---  newstest2014:  19.4    newstest2015: -", 
            "title": "Machine Translation: WMT'16 English-German"
        }, 
        {
            "location": "/benchmarks/#machine-translation-wmt16-german-english", 
            "text": "Coming soon.", 
            "title": "Machine Translation: WMT'16 German-English"
        }, 
        {
            "location": "/benchmarks/#text-summarization-gigaword", 
            "text": "Coming soon.", 
            "title": "Text Summarization: Gigaword"
        }, 
        {
            "location": "/benchmarks/#conversational-modeling", 
            "text": "Coming soon.", 
            "title": "Conversational Modeling"
        }, 
        {
            "location": "/contributing/", 
            "text": "Development Setup\n\n\n1. Install Python3. If you're on a Mac the easiest way to do this is probably using \nHomebrew\n. Then,\n\n\n# Clone this repository.\ngit clone https://github.com/dennybritz/seq2seq.git\ncd seq2seq\n\n# Create a new virtual environment and activate it.\npython3 -m venv ~/tf-venv\nsource ~/tf-venv/bin/activate\n\n# Install package dependencies and utilities.\npip install -e .\npip install nose pylint tox yapf mkdocs\n\n# Make sure tests are passing.\nnosetests\n\n# Code :)\n\n\n\n\nGithub Workflow\n\n\nPushing directly to the master branch is disabled and you must create feature branches and submit them via pull request. To make things easier you can also use the \nGithub Desktop app\n. A typical workflow looks as follows:\n\n\n# Make sure you are in the seq2seq root directory.\n# Start from the master branch.\ngit checkout master\n\n# Pull latest changes from Github.\ngit pull\n\n# Create a new feature branch.\ngit checkout -b feature/my-new-feature\n\n# Make changes and commits\necho \nblabla\n \n test\ngit commit -am \nTest commit\n\n\n# Push the branch upstream.\ngit push -u origin/my-new-feature\n\n# Submit a pull request on Github.\n\n\n\n\nAfter you submit a Pull Request, one person must review the change and\nCircleCI integration tests must be passing before you can merge into the\nmaster branch.\n\n\nPython Style\n\n\nWe use \npylint\n and \nyapf\n for automated code formatting. Before submitting a pull request, make sure you run them:\n\n\n# Run this and fix all errors.\npylint ./seq2seq\n\n# Optional, run this to auto-format all code.\nyapf -ir ./seq2seq\n\n\n\n\nNote that CircleCI integration test will fail if pylint reports any critical\nerrors, preventing you from merging your changes.\n\n\nTensorflow Style\n\n\nGraphModule\n\n\nAll classes that modify the Graph should inherit from \nseq2seq.graph_module.GraphModule\n, which is a wrapper around TensorFlow's \ntf.make_template\n function that enables easy variable sharing. Basically, it allows you to do something like this:\n\n\nencode_fn = SomeEncoderModule(...)\n\n# New variables are created in this call.\noutput1 = encode_fn(input1)\n\n# No new variables are created here. The variables from the above call are re-used.\n# Note how this is different from normal TensorFlow where you would need to use variable scopes.\noutput2 = encode_fn(input2)\n\n# Because this is a new instance a second set of variables is created.\nencode_fn2 = SomeEncoderModule(...)\noutput3 = encode_fn2(input3)\n\n\n\n\nFunctions vs. Classes\n\n\n\n\nOperations that \ncreate new variables\n must be implemented as classes and must inherit from \nGraphModule\n.\n\n\nOperations that \ndo not create new variables\n can be implemented as standard python functions, or as classes that inherit from \nGraphModule\n if they have a lot of logic.", 
            "title": "Contributing"
        }, 
        {
            "location": "/contributing/#development-setup", 
            "text": "1. Install Python3. If you're on a Mac the easiest way to do this is probably using  Homebrew . Then,  # Clone this repository.\ngit clone https://github.com/dennybritz/seq2seq.git\ncd seq2seq\n\n# Create a new virtual environment and activate it.\npython3 -m venv ~/tf-venv\nsource ~/tf-venv/bin/activate\n\n# Install package dependencies and utilities.\npip install -e .\npip install nose pylint tox yapf mkdocs\n\n# Make sure tests are passing.\nnosetests\n\n# Code :)", 
            "title": "Development Setup"
        }, 
        {
            "location": "/contributing/#github-workflow", 
            "text": "Pushing directly to the master branch is disabled and you must create feature branches and submit them via pull request. To make things easier you can also use the  Github Desktop app . A typical workflow looks as follows:  # Make sure you are in the seq2seq root directory.\n# Start from the master branch.\ngit checkout master\n\n# Pull latest changes from Github.\ngit pull\n\n# Create a new feature branch.\ngit checkout -b feature/my-new-feature\n\n# Make changes and commits\necho  blabla    test\ngit commit -am  Test commit \n\n# Push the branch upstream.\ngit push -u origin/my-new-feature\n\n# Submit a pull request on Github.  After you submit a Pull Request, one person must review the change and\nCircleCI integration tests must be passing before you can merge into the\nmaster branch.", 
            "title": "Github Workflow"
        }, 
        {
            "location": "/contributing/#python-style", 
            "text": "We use  pylint  and  yapf  for automated code formatting. Before submitting a pull request, make sure you run them:  # Run this and fix all errors.\npylint ./seq2seq\n\n# Optional, run this to auto-format all code.\nyapf -ir ./seq2seq  Note that CircleCI integration test will fail if pylint reports any critical\nerrors, preventing you from merging your changes.", 
            "title": "Python Style"
        }, 
        {
            "location": "/contributing/#tensorflow-style", 
            "text": "", 
            "title": "Tensorflow Style"
        }, 
        {
            "location": "/contributing/#graphmodule", 
            "text": "All classes that modify the Graph should inherit from  seq2seq.graph_module.GraphModule , which is a wrapper around TensorFlow's  tf.make_template  function that enables easy variable sharing. Basically, it allows you to do something like this:  encode_fn = SomeEncoderModule(...)\n\n# New variables are created in this call.\noutput1 = encode_fn(input1)\n\n# No new variables are created here. The variables from the above call are re-used.\n# Note how this is different from normal TensorFlow where you would need to use variable scopes.\noutput2 = encode_fn(input2)\n\n# Because this is a new instance a second set of variables is created.\nencode_fn2 = SomeEncoderModule(...)\noutput3 = encode_fn2(input3)", 
            "title": "GraphModule"
        }, 
        {
            "location": "/contributing/#functions-vs-classes", 
            "text": "Operations that  create new variables  must be implemented as classes and must inherit from  GraphModule .  Operations that  do not create new variables  can be implemented as standard python functions, or as classes that inherit from  GraphModule  if they have a lot of logic.", 
            "title": "Functions vs. Classes"
        }
    ]
}