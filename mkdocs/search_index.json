{
    "docs": [
        {
            "location": "/", 
            "text": "Introduction\n\n\n[seq2seq] is an open source framework for sequence learning in Tensorflow.\nThe main use case is Neural Machine Translation (NMT), but [seq2seq] can be\nused for a variety of other applications such as Conversational Modeling,\nText Summarization or Image Captioning.\n\n\nDesign Goals\n\n\nWe built [seq2seq] with the following goals in mind.\n\n\n\n\n\n\nEase of use.\n You can train a model with single command. The input data are raw text files and not esoteric file formats. Using pre-trained models to make predictions is straightforward.\n\n\n\n\n\n\nEasy to extend\n. Code is structured so that it is easy to build upon. For example, adding a new type of attention mechanism or encoder architecture requires only minimal code changes.\n\n\n\n\n\n\nWell-documented\n. In addition to the \nAPI documentation\n we have written up guides to help you get started with [seq2seq].\n\n\n\n\n\n\nGood performance\n. For the sake of code simplicity we do not try to squeeze out every last bit of performance, but the implementation is fast enough to cover almost all production and research use cases. [seq2seq] also supports distributed training to trade off computational power and training time.\n\n\n\n\n\n\nStandard Benchmarks\n. We provide \npre-trained models and benchmark results\n for several standard datasets. We hope these can serve as a baseline for future research.\n\n\n\n\n\n\nRelated Frameworks\n\n\nThe following frameworks offer functionality that similar to those of [seq2seq]. We hope to collaborate with the authors of these framework so that we can learn from each other.\n\n\n\n\nOpenNMT (Torch)\n\n\nNeural Monkey (Tensorflow)\n\n\nNEMATUS (Theano)", 
            "title": "Overview"
        }, 
        {
            "location": "/#introduction", 
            "text": "[seq2seq] is an open source framework for sequence learning in Tensorflow.\nThe main use case is Neural Machine Translation (NMT), but [seq2seq] can be\nused for a variety of other applications such as Conversational Modeling,\nText Summarization or Image Captioning.", 
            "title": "Introduction"
        }, 
        {
            "location": "/#design-goals", 
            "text": "We built [seq2seq] with the following goals in mind.    Ease of use.  You can train a model with single command. The input data are raw text files and not esoteric file formats. Using pre-trained models to make predictions is straightforward.    Easy to extend . Code is structured so that it is easy to build upon. For example, adding a new type of attention mechanism or encoder architecture requires only minimal code changes.    Well-documented . In addition to the  API documentation  we have written up guides to help you get started with [seq2seq].    Good performance . For the sake of code simplicity we do not try to squeeze out every last bit of performance, but the implementation is fast enough to cover almost all production and research use cases. [seq2seq] also supports distributed training to trade off computational power and training time.    Standard Benchmarks . We provide  pre-trained models and benchmark results  for several standard datasets. We hope these can serve as a baseline for future research.", 
            "title": "Design Goals"
        }, 
        {
            "location": "/#related-frameworks", 
            "text": "The following frameworks offer functionality that similar to those of [seq2seq]. We hope to collaborate with the authors of these framework so that we can learn from each other.   OpenNMT (Torch)  Neural Monkey (Tensorflow)  NEMATUS (Theano)", 
            "title": "Related Frameworks"
        }, 
        {
            "location": "/getting_started/", 
            "text": "Download \n Setup\n\n\nTo use [seq2seq] you need a working installation of Tensorflow with\nPython 2.7 or Python 3.5. Follow the \nTensorflow Getting Started\n\nfor detailed setup instructions. With Tensorflow installed, you can clone this repository:\n\n\ngit clone https://github.com/dennybritz/seq2seq.git\ncd seq2seq\n\n\n\n\nTo make sure everything works as expect you can run a simple training test:\n\n\n./test/train_infer_test.sh\n\n\n\n\nIf you see a \"success\" message you are all set. If you run into setup issues\nplease \nfile a Github issue\n.\n\n\nUsing a pre-trained model\n\n\nTraining sequence models can take a long time. For example, training a model to\ntranslate from English to German takes several days on a modern GPU (you can speed this up by using distributed training). If all you want is \nuse\n a model that has already been trained check out the \nmodels\n page. Let's use a model that has been trained to translate from English to German.\n\n\n# Download the trained model\nMODEL_DIR=${TMPDIR}/en_de_pretrained\nwget TODO\n\n# Create an input file with English sentences to feed to the model\n# Note: The input must be tokenized. See the data section for more info.\necho \nWhat is your name ?\n \n $TMPDIR/test.en\n\n# Call the model inference script\n./bin/infer.py \\\n  --source $TMPDIR/test.en \\\n  --vocab_source $MODEL_DIR/vocab.en \\\n  --vocab_target $MODEL_DIR/vocab.de \\\n  --model AttentionSeq2Seq \\\n  --model_dir $MODEL_DIR\n\n# You should see German output: TODO\n\n\n\n\nTraining your own model\n\n\nLet's now look at how you can train a model based on your own data. In this section we will train a model that learns to reverse the input sequence. While this task is not very useful in practice the training time for it is rather short and it serves as a good way to demonstrate and sanity-check that everything is working as intended.\n\n\nFist, let's generate some data:\n\n\nDATA_TYPE=revsere tools/data/toy.sh\n\n\n\n\nTo train a new model we will use the following files that were generated by the above script. To use your own data you will need the same corresponding files.\n\n\n\n\n\n\n\n\nArgument Name\n\n\nDescription\n\n\n\n\n\n\n\n\n\n\ntrain_source\n\n\nThe input sequences. A plain text file where each line corresponds to one input example, and tokens/words are separated by spaces. For natural language data you may need to use a tokenizer to generate this. See \ntools\n for more details.\n\n\n\n\n\n\ntrain_target\n\n\nThe targets sequences in the same format as \ntrain_target\n. Each line corresponds to the \"translation\" of the input sequence in the same line in \ntrain_target\n. Thus, the number of lines in these two files must be equal.\n\n\n\n\n\n\ndev_source\n\n\nSame as \ntrain_source\n but used as a validation/development set.\n\n\n\n\n\n\ndev_target\n\n\nSame as \ntrain_target\n but used as a validation/development set.\n\n\n\n\n\n\nvocab_source\n\n\nThe vocabulary for the source sequences. A plain text file that contains one word per line. All words that appear in the source text but are not in the vocabulary will be mapped to \nUNK\n (unknown) tokens. We provide a script to generate a vocabulary file. See \ntools\n for more details.\n\n\n\n\n\n\nvocab_target\n\n\nSame as \nvocab_source\n but for the target sequences.\n\n\n\n\n\n\n\n\nWith the above input files you can now train a new model:\n\n\n./bin/train.py \\\n  --train_source $HOME/nmt_data/toy_reverse/train/sources.txt \\\n  --train_target $HOME/nmt_data/toy_reverse/train/targets.txt \\\n  --dev_source $HOME/nmt_data/toy_reverse/dev/sources.txt \\\n  --dev_target $HOME/nmt_data/toy_reverse/dev/targets.txt \\\n  --vocab_source $HOME/nmt_data/toy_reverse/train/vocab.sources.txt \\\n  --vocab_target $HOME/nmt_data/toy_reverse/train/vocab.targets.txt \\\n  --model AttentionSeq2Seq \\\n  --batch_size 32 \\\n  --train_epochs 5 \\\n  --output_dir ${TMPDIR}/nmt_toy_reverse\n\n\n\n\nOn a CPU the training may take up to 15 minutes. With the trained model you can now perform inference:\n\n\n./bin/infer.py \\\n  --source $HOME/nmt_data/toy_reverse/test/sources.txt \\\n  --vocab_source $HOME/nmt_data/toy_reverse/train/vocab.sources.txt \\\n  --vocab_target $HOME/nmt_data/toy_reverse/train/vocab.targets.txt \\\n  --model AttentionSeq2Seq \\\n  --model_dir ${TMPDIR}/nmt_toy_reverse \\\n  \n ${TMPDIR}/nmt_toy_reverse/predictions.txt\n\n# Evaluate BLEU score using multi-bleu script from MOSES\n./bin/multi-bleu.perl $HOME/nmt_data/toy_reverse/test/targets.txt \n ${TMPDIR}/nmt_toy_reverse/predictions.txt\n\n\n\n\nYou can learn more about available models, data and tools by browsing the documentation.", 
            "title": "Getting Started"
        }, 
        {
            "location": "/getting_started/#download-setup", 
            "text": "To use [seq2seq] you need a working installation of Tensorflow with\nPython 2.7 or Python 3.5. Follow the  Tensorflow Getting Started \nfor detailed setup instructions. With Tensorflow installed, you can clone this repository:  git clone https://github.com/dennybritz/seq2seq.git\ncd seq2seq  To make sure everything works as expect you can run a simple training test:  ./test/train_infer_test.sh  If you see a \"success\" message you are all set. If you run into setup issues\nplease  file a Github issue .", 
            "title": "Download &amp; Setup"
        }, 
        {
            "location": "/getting_started/#using-a-pre-trained-model", 
            "text": "Training sequence models can take a long time. For example, training a model to\ntranslate from English to German takes several days on a modern GPU (you can speed this up by using distributed training). If all you want is  use  a model that has already been trained check out the  models  page. Let's use a model that has been trained to translate from English to German.  # Download the trained model\nMODEL_DIR=${TMPDIR}/en_de_pretrained\nwget TODO\n\n# Create an input file with English sentences to feed to the model\n# Note: The input must be tokenized. See the data section for more info.\necho  What is your name ?    $TMPDIR/test.en\n\n# Call the model inference script\n./bin/infer.py \\\n  --source $TMPDIR/test.en \\\n  --vocab_source $MODEL_DIR/vocab.en \\\n  --vocab_target $MODEL_DIR/vocab.de \\\n  --model AttentionSeq2Seq \\\n  --model_dir $MODEL_DIR\n\n# You should see German output: TODO", 
            "title": "Using a pre-trained model"
        }, 
        {
            "location": "/getting_started/#training-your-own-model", 
            "text": "Let's now look at how you can train a model based on your own data. In this section we will train a model that learns to reverse the input sequence. While this task is not very useful in practice the training time for it is rather short and it serves as a good way to demonstrate and sanity-check that everything is working as intended.  Fist, let's generate some data:  DATA_TYPE=revsere tools/data/toy.sh  To train a new model we will use the following files that were generated by the above script. To use your own data you will need the same corresponding files.     Argument Name  Description      train_source  The input sequences. A plain text file where each line corresponds to one input example, and tokens/words are separated by spaces. For natural language data you may need to use a tokenizer to generate this. See  tools  for more details.    train_target  The targets sequences in the same format as  train_target . Each line corresponds to the \"translation\" of the input sequence in the same line in  train_target . Thus, the number of lines in these two files must be equal.    dev_source  Same as  train_source  but used as a validation/development set.    dev_target  Same as  train_target  but used as a validation/development set.    vocab_source  The vocabulary for the source sequences. A plain text file that contains one word per line. All words that appear in the source text but are not in the vocabulary will be mapped to  UNK  (unknown) tokens. We provide a script to generate a vocabulary file. See  tools  for more details.    vocab_target  Same as  vocab_source  but for the target sequences.     With the above input files you can now train a new model:  ./bin/train.py \\\n  --train_source $HOME/nmt_data/toy_reverse/train/sources.txt \\\n  --train_target $HOME/nmt_data/toy_reverse/train/targets.txt \\\n  --dev_source $HOME/nmt_data/toy_reverse/dev/sources.txt \\\n  --dev_target $HOME/nmt_data/toy_reverse/dev/targets.txt \\\n  --vocab_source $HOME/nmt_data/toy_reverse/train/vocab.sources.txt \\\n  --vocab_target $HOME/nmt_data/toy_reverse/train/vocab.targets.txt \\\n  --model AttentionSeq2Seq \\\n  --batch_size 32 \\\n  --train_epochs 5 \\\n  --output_dir ${TMPDIR}/nmt_toy_reverse  On a CPU the training may take up to 15 minutes. With the trained model you can now perform inference:  ./bin/infer.py \\\n  --source $HOME/nmt_data/toy_reverse/test/sources.txt \\\n  --vocab_source $HOME/nmt_data/toy_reverse/train/vocab.sources.txt \\\n  --vocab_target $HOME/nmt_data/toy_reverse/train/vocab.targets.txt \\\n  --model AttentionSeq2Seq \\\n  --model_dir ${TMPDIR}/nmt_toy_reverse \\\n    ${TMPDIR}/nmt_toy_reverse/predictions.txt\n\n# Evaluate BLEU score using multi-bleu script from MOSES\n./bin/multi-bleu.perl $HOME/nmt_data/toy_reverse/test/targets.txt   ${TMPDIR}/nmt_toy_reverse/predictions.txt  You can learn more about available models, data and tools by browsing the documentation.", 
            "title": "Training your own model"
        }, 
        {
            "location": "/data/", 
            "text": "Pre-processed datasets\n\n\nSee the data generation notebooks\n for details on how this data was generated.\n\n\n\n\n\n\n\n\nDataset\n\n\nDescription\n\n\nTraining/Dev/Test Examples\n\n\nVocab Size\n\n\nURL\n\n\n\n\n\n\n\n\n\n\nWMT'16 EN-DE\n\n\nData for the \nWMT'16 Translation Task\n English to German. Training data is combined from Europarl v7, Common Crawl, and News v11. Development data is newstest2013. Test data is newstest2015.\n\n\n4.56M/3K/2.6K\n\n\n50k\n\n\nDownload\n\n\n\n\n\n\nToy Copy\n\n\nA toy dataset where the target sequence is equal to the source sequence. Thus, the network must learn to \"copy\" the source sequence.\n\n\n10k/1k/1k\n\n\n20\n\n\nDownload\n\n\n\n\n\n\nToy Reverse\n\n\nA toy dataset where the target sequence is equal to the reversed source sequence.\n\n\n10k/1k/1k\n\n\n20\n\n\nDownload\n\n\n\n\n\n\n\n\nTraining/Dev data in Parallel Text Format\n\n\nThe input pipeline expect parallel tokenized data in raw text format. That is, you need \nsources.txt\n and a \ntargets.txt\n file that contain corresponding sentences, aligned line-by-line. Each line corresponds to one input/output example. These words/tokens in these files must be separated by spaces.\n\n\nGenerating Vocabulary\n\n\nA vocabulary file is a raw text file that contains one token per line. The total number of lines is the size of the vocabulary, and each token is mapped to its line number. The special words \nUNK\n, \nSEQUENCE_START\n and \nSEQUENCE_END\n are not part of the vocabulary file, and correspond to \nvocab_size + 1\n, \nvocab_size + 2\n, and \nvocab_size + 3\n respectively\n\n\nGiven a raw text file of tokens separated by spaces you can generate a vocabulary file using the \ngenerate_vocab.py\n script:\n\n\n./seq2seq/scripts/generate_vocab.py \\\n  --input_file /data/source.txt \\\n  --output_file /data/source_vocab \\\n  --min_frequency 1 \\\n  --max_vocab_size 50000\n\n\n\n\nThe resulting vocabulary file contains one word per line.\n\n\nSubword Unit Preprocessing\n\n\nIn order to deal with an open vocabulary, rare words can be split into\nsubword units as proposed in [1]. This improves the model's translation\nperformance particularly on rare words. The authors propose to use\nByte Pair Encoding (BPE), a simple compression algorithm, for splitting\nwords into subwords. Starting from characters, BPE iteratively replaces the\nmost frequent pair of symbols with a new symbol. The final symbol vocabulary\nis equal to the size of the initial vocabulary plus the number of merge\noperations, which is the only hyperparameter of the method.\nTo apply BPE as a pre-processing step to your raw text input,\nfollow the below steps:\n\n\n\n\nDownload the open-source implementation of the paper\n   from \nhere\n.\n\n\nLearn the BPE encoding: \ncd subword-nmt\n.\n   \n./learn_bpe.py -s {num_operations} -i {train_file} -o {codes_file}\n.\n   \nnum_operations\n is the number of merge operations. Default is \n10000\n.\n\n\nApply the BPE encoding to the training and test files:\n  \n./apply_bpe.py -c {codes_file} -i {input_file} -o {output_file}\n.\n\n\n\n\nThe resulting BPE-processed files can be used as-is in place of the raw text\nfiles for training the NMT model.\n\n\nReferences:\n\n\n[1] Sennrich, R., Haddow, B., \n Birch, A. (2016). Neural Machine Translation of\nRare Words with Subword Units. In Proceedings of the 54th Annual Meeting of\nthe Association for Computational Linguistics (ACL 2016).\nRetrieved from http://arxiv.org/abs/1508.07909\n\n\nTraining/Dev data in TFRecords format (Old)\n\n\nThe input pipeline expects data in \nTFRecord\n format consisting of \ntf.Example\n protocol buffers. Each \nExample\n record contains the following fields:\n\n\n\n\npair_id (string)\n (optional) is a dataset-unique id for this example.\n\n\nsource_len (int64)\n is the length of the source sequence.\n\n\ntarget_len (int64)\n is the length of the target sequence.\n\n\nsource_tokens (string)\n is a list of source tokens.\n\n\ntarget_tokens (string)\n is a list of targets tokens.\n\n\n\n\nGiven a parallel corpus, i.e. source and target files aligned by line such as \nthose from WMT\n, we provide a \nscript\n to generate a corresponding TFRecords file:\n\n\n./seq2seq/scripts/generate_examples.py \\\n  --source_file /path/to/data/train.de.txt \\\n  --target_file /path/to/data/train.en.txt \\\n  --output_file /path/to/data/train.tfrecords", 
            "title": "Data"
        }, 
        {
            "location": "/data/#pre-processed-datasets", 
            "text": "See the data generation notebooks  for details on how this data was generated.     Dataset  Description  Training/Dev/Test Examples  Vocab Size  URL      WMT'16 EN-DE  Data for the  WMT'16 Translation Task  English to German. Training data is combined from Europarl v7, Common Crawl, and News v11. Development data is newstest2013. Test data is newstest2015.  4.56M/3K/2.6K  50k  Download    Toy Copy  A toy dataset where the target sequence is equal to the source sequence. Thus, the network must learn to \"copy\" the source sequence.  10k/1k/1k  20  Download    Toy Reverse  A toy dataset where the target sequence is equal to the reversed source sequence.  10k/1k/1k  20  Download", 
            "title": "Pre-processed datasets"
        }, 
        {
            "location": "/data/#trainingdev-data-in-parallel-text-format", 
            "text": "The input pipeline expect parallel tokenized data in raw text format. That is, you need  sources.txt  and a  targets.txt  file that contain corresponding sentences, aligned line-by-line. Each line corresponds to one input/output example. These words/tokens in these files must be separated by spaces.", 
            "title": "Training/Dev data in Parallel Text Format"
        }, 
        {
            "location": "/data/#generating-vocabulary", 
            "text": "A vocabulary file is a raw text file that contains one token per line. The total number of lines is the size of the vocabulary, and each token is mapped to its line number. The special words  UNK ,  SEQUENCE_START  and  SEQUENCE_END  are not part of the vocabulary file, and correspond to  vocab_size + 1 ,  vocab_size + 2 , and  vocab_size + 3  respectively  Given a raw text file of tokens separated by spaces you can generate a vocabulary file using the  generate_vocab.py  script:  ./seq2seq/scripts/generate_vocab.py \\\n  --input_file /data/source.txt \\\n  --output_file /data/source_vocab \\\n  --min_frequency 1 \\\n  --max_vocab_size 50000  The resulting vocabulary file contains one word per line.", 
            "title": "Generating Vocabulary"
        }, 
        {
            "location": "/data/#subword-unit-preprocessing", 
            "text": "In order to deal with an open vocabulary, rare words can be split into\nsubword units as proposed in [1]. This improves the model's translation\nperformance particularly on rare words. The authors propose to use\nByte Pair Encoding (BPE), a simple compression algorithm, for splitting\nwords into subwords. Starting from characters, BPE iteratively replaces the\nmost frequent pair of symbols with a new symbol. The final symbol vocabulary\nis equal to the size of the initial vocabulary plus the number of merge\noperations, which is the only hyperparameter of the method.\nTo apply BPE as a pre-processing step to your raw text input,\nfollow the below steps:   Download the open-source implementation of the paper\n   from  here .  Learn the BPE encoding:  cd subword-nmt .\n    ./learn_bpe.py -s {num_operations} -i {train_file} -o {codes_file} .\n    num_operations  is the number of merge operations. Default is  10000 .  Apply the BPE encoding to the training and test files:\n   ./apply_bpe.py -c {codes_file} -i {input_file} -o {output_file} .   The resulting BPE-processed files can be used as-is in place of the raw text\nfiles for training the NMT model.  References:  [1] Sennrich, R., Haddow, B.,   Birch, A. (2016). Neural Machine Translation of\nRare Words with Subword Units. In Proceedings of the 54th Annual Meeting of\nthe Association for Computational Linguistics (ACL 2016).\nRetrieved from http://arxiv.org/abs/1508.07909", 
            "title": "Subword Unit Preprocessing"
        }, 
        {
            "location": "/data/#trainingdev-data-in-tfrecords-format-old", 
            "text": "The input pipeline expects data in  TFRecord  format consisting of  tf.Example  protocol buffers. Each  Example  record contains the following fields:   pair_id (string)  (optional) is a dataset-unique id for this example.  source_len (int64)  is the length of the source sequence.  target_len (int64)  is the length of the target sequence.  source_tokens (string)  is a list of source tokens.  target_tokens (string)  is a list of targets tokens.   Given a parallel corpus, i.e. source and target files aligned by line such as  those from WMT , we provide a  script  to generate a corresponding TFRecords file:  ./seq2seq/scripts/generate_examples.py \\\n  --source_file /path/to/data/train.de.txt \\\n  --target_file /path/to/data/train.en.txt \\\n  --output_file /path/to/data/train.tfrecords", 
            "title": "Training/Dev data in TFRecords format (Old)"
        }, 
        {
            "location": "/models/", 
            "text": "When calling the training script you can specify a model class using the \n--model\n flags and model-specific hyperparameters using the \n--hparams\n flag. This page list all support models and hyperparameters.\n\n\nCommon Hyperparameters\n\n\nThe following hyperparameters are sahred by all models, unless explicitly stated in the model section.\n\n\n\n\n\n\n\n\nName\n\n\nDefault\n\n\nDescription\n\n\n\n\n\n\n\n\n\n\nsource.max_seq_len\n\n\n40\n\n\nMaximum length of source sequences. An example is sliced to this length before being fed to the encoder.\n\n\n\n\n\n\nsource.reverse\n\n\nFalse\n\n\nIf set to true, reverse the source sequence before feeding it into the encoder.\n\n\n\n\n\n\ntarget.max_seq_len\n\n\n40\n\n\nMaximum length of target sequences. An example is sliced to this length before being fed to the decoder.\n\n\n\n\n\n\nembedding.dim\n\n\n100\n\n\nDimensionality of the embedding layer.\n\n\n\n\n\n\noptimizer.name\n\n\nAdam\n\n\nType of Optimizer to use, e.g. \nAdam\n, \nSGD\n or \nMomentum\n. The name is fed to Tensorflow's \noptimize_loss\n function. See Tensorflow documentation for more details and all available options.\n\n\n\n\n\n\noptimizer.learning_rate\n\n\n1e-4\n\n\nInitial learning rate for the optimizer. This is fed to Tensorflow's \noptimize_loss\n function.\n\n\n\n\n\n\noptimizer.lr_decay_type\n\n\n\n\nThe name of one of Tensorflow's \nlearning rate decay function\n defined in \ntf.train\n, e.g. \nexponential_decay\n. If this is an empty string (default) then no learning rate decay is used.\n\n\n\n\n\n\noptimizer.lr_decay_steps\n\n\n100\n\n\nHow often to apply decay. This is fed as the \ndecay_steps\n argument to the decay function defined above. See Tensoflow documentation for more details.\n\n\n\n\n\n\noptimizer.lr_decay_rate\n\n\n0.99\n\n\nThe decay rate. This is fed as the \ndecay_rate\n argument to the decay function defined above. See Tensorfow documentation for more details.\n\n\n\n\n\n\noptimizer.lr_start_decay_at\n\n\n0\n\n\nStart learning rate decay at this step.\n\n\n\n\n\n\noptimizer.lr_stop_decay_at\n\n\n1e9\n\n\nStop learning rate decay at this step.\n\n\n\n\n\n\noptimizer.lr_min_learning_rate\n\n\n1e-12\n\n\nNever decay below this learning rate.\n\n\n\n\n\n\noptimizer.lr_staircase\n\n\nFalse\n\n\nIf \nTrue\n decay the learning rate at discrete intervals. This is fed as the \nstaircase\n argument to the decay function defined above. See Tensorfow documentation for more details.\n\n\n\n\n\n\noptimizer.clip_gradients\n\n\n5.0\n\n\nClip gradients by their global norm.\n\n\n\n\n\n\n\n\nBasicSeq2Seq\n\n\nA sequence to sequence model that uses a unidirectional encoder and decoder without attention mechanism. The last encoder state is used to initialize the decoder. This is the \"vanilla\" implementation of the seq2seq architecture.\n\n\nThis model suports the following additional hyperparameters.\n\n\n\n\n\n\n\n\nName\n\n\nDefault\n\n\nDescription\n\n\n\n\n\n\n\n\n\n\nrnn_cell.type\n\n\nBasicLSTMCell\n\n\nThe class name of a RNN Cell defined in \ntf.contrib.rnn\n. The most common values are \nBasicLSTMCell\n, \nLSTMCell\n or \nGRUCell\n. Applies to both encoder and decoder.\n\n\n\n\n\n\nrnn_cell.num_units\n\n\n128\n\n\nThe number of units to use for the RNN Cell. Applies to both encoder and decoder.\n\n\n\n\n\n\nrnn_cell.dropout_input_keep_prob\n\n\n1.0\n\n\nApply dropout to the (non-recurrent) inputs of each RNN layer using this keep probability. A value of \n1.0\n disables dropout. Applies to both encoder and decoder.\n\n\n\n\n\n\nrnn_cell.dropout_output_keep_prob\n\n\n1.0\n\n\nApply dropout to the (non-recurrent) outputs of each RNN layer using this keep probability. A value of \n1.0\n disables dropout. Applies to both encoder and decoder.\n\n\n\n\n\n\nrnn_cell.num_layers\n\n\n1\n\n\nNumber of RNN layers. Applies to both encoder and decoder.\n\n\n\n\n\n\n\n\nAttentionSeq2seq\n\n\nA sequence to sequence model that uses a unidirectional or bidirectional encoder and a decoder with attention mechanism. The last encoder state is not fed to the decoder. This implementation is comparable to the model in \nNeural Machine Translation by Jointly Learning to Align and Translate\n.\n\n\n\n\n\n\n\n\nName\n\n\nDefault\n\n\nDescription\n\n\n\n\n\n\n\n\n\n\nattention.dim\n\n\n128\n\n\nNumber of units in the attention layer.\n\n\n\n\n\n\nattention.score_type\n\n\nbahdanau\n\n\nThe formula used to calculate attention scores. Available values are \nbahdanau\n and \ndot\n. \nbahdanau\n is described in \nNeural Machine Translation by Jointly Learning to Align and Translate\n. \ndot\n is described in \nEffective Approaches to Attention-based Neural Machine Translation\n.\n\n\n\n\n\n\nencoder.type\n\n\nBidirectionalRNNEncoder\n\n\nType of encoder to use. This is the class name of an encoder defined in \nseq2seq.encoder\n. Currently the supported value are \nBidirectionalRNNEncoder\n and \nUnidirectionalRNNEncoder\n.\n\n\n\n\n\n\nrnn_cell.type\n\n\nBasicLSTMCell\n\n\nThe class name of a RNN Cell defined in \ntf.contrib.rnn\n. The most common values are \nBasicLSTMCell\n, \nLSTMCell\n or \nGRUCell\n. Applies to both encoder and decoder.\n\n\n\n\n\n\nrnn_cell.num_units\n\n\n128\n\n\nThe number of units to use for the RNN Cell. Applies to both encoder and decoder.\n\n\n\n\n\n\nrnn_cell.dropout_input_keep_prob\n\n\n1.0\n\n\nApply dropout to the (non-recurrent) inputs of each RNN layer using this keep probability. A value of \n1.0\n disables dropout. Applies to both encoder and decoder.\n\n\n\n\n\n\nrnn_cell.dropout_output_keep_prob\n\n\n1.0\n\n\nApply dropout to the (non-recurrent) outputs of each RNN layer using this keep probability. A value of \n1.0\n disables dropout. Applies to both encoder and decoder.\n\n\n\n\n\n\nrnn_cell.num_layers\n\n\n1\n\n\nNumber of RNN layers. Applies to both encoder and decoder.", 
            "title": "Models"
        }, 
        {
            "location": "/models/#common-hyperparameters", 
            "text": "The following hyperparameters are sahred by all models, unless explicitly stated in the model section.     Name  Default  Description      source.max_seq_len  40  Maximum length of source sequences. An example is sliced to this length before being fed to the encoder.    source.reverse  False  If set to true, reverse the source sequence before feeding it into the encoder.    target.max_seq_len  40  Maximum length of target sequences. An example is sliced to this length before being fed to the decoder.    embedding.dim  100  Dimensionality of the embedding layer.    optimizer.name  Adam  Type of Optimizer to use, e.g.  Adam ,  SGD  or  Momentum . The name is fed to Tensorflow's  optimize_loss  function. See Tensorflow documentation for more details and all available options.    optimizer.learning_rate  1e-4  Initial learning rate for the optimizer. This is fed to Tensorflow's  optimize_loss  function.    optimizer.lr_decay_type   The name of one of Tensorflow's  learning rate decay function  defined in  tf.train , e.g.  exponential_decay . If this is an empty string (default) then no learning rate decay is used.    optimizer.lr_decay_steps  100  How often to apply decay. This is fed as the  decay_steps  argument to the decay function defined above. See Tensoflow documentation for more details.    optimizer.lr_decay_rate  0.99  The decay rate. This is fed as the  decay_rate  argument to the decay function defined above. See Tensorfow documentation for more details.    optimizer.lr_start_decay_at  0  Start learning rate decay at this step.    optimizer.lr_stop_decay_at  1e9  Stop learning rate decay at this step.    optimizer.lr_min_learning_rate  1e-12  Never decay below this learning rate.    optimizer.lr_staircase  False  If  True  decay the learning rate at discrete intervals. This is fed as the  staircase  argument to the decay function defined above. See Tensorfow documentation for more details.    optimizer.clip_gradients  5.0  Clip gradients by their global norm.", 
            "title": "Common Hyperparameters"
        }, 
        {
            "location": "/models/#basicseq2seq", 
            "text": "A sequence to sequence model that uses a unidirectional encoder and decoder without attention mechanism. The last encoder state is used to initialize the decoder. This is the \"vanilla\" implementation of the seq2seq architecture.  This model suports the following additional hyperparameters.     Name  Default  Description      rnn_cell.type  BasicLSTMCell  The class name of a RNN Cell defined in  tf.contrib.rnn . The most common values are  BasicLSTMCell ,  LSTMCell  or  GRUCell . Applies to both encoder and decoder.    rnn_cell.num_units  128  The number of units to use for the RNN Cell. Applies to both encoder and decoder.    rnn_cell.dropout_input_keep_prob  1.0  Apply dropout to the (non-recurrent) inputs of each RNN layer using this keep probability. A value of  1.0  disables dropout. Applies to both encoder and decoder.    rnn_cell.dropout_output_keep_prob  1.0  Apply dropout to the (non-recurrent) outputs of each RNN layer using this keep probability. A value of  1.0  disables dropout. Applies to both encoder and decoder.    rnn_cell.num_layers  1  Number of RNN layers. Applies to both encoder and decoder.", 
            "title": "BasicSeq2Seq"
        }, 
        {
            "location": "/models/#attentionseq2seq", 
            "text": "A sequence to sequence model that uses a unidirectional or bidirectional encoder and a decoder with attention mechanism. The last encoder state is not fed to the decoder. This implementation is comparable to the model in  Neural Machine Translation by Jointly Learning to Align and Translate .     Name  Default  Description      attention.dim  128  Number of units in the attention layer.    attention.score_type  bahdanau  The formula used to calculate attention scores. Available values are  bahdanau  and  dot .  bahdanau  is described in  Neural Machine Translation by Jointly Learning to Align and Translate .  dot  is described in  Effective Approaches to Attention-based Neural Machine Translation .    encoder.type  BidirectionalRNNEncoder  Type of encoder to use. This is the class name of an encoder defined in  seq2seq.encoder . Currently the supported value are  BidirectionalRNNEncoder  and  UnidirectionalRNNEncoder .    rnn_cell.type  BasicLSTMCell  The class name of a RNN Cell defined in  tf.contrib.rnn . The most common values are  BasicLSTMCell ,  LSTMCell  or  GRUCell . Applies to both encoder and decoder.    rnn_cell.num_units  128  The number of units to use for the RNN Cell. Applies to both encoder and decoder.    rnn_cell.dropout_input_keep_prob  1.0  Apply dropout to the (non-recurrent) inputs of each RNN layer using this keep probability. A value of  1.0  disables dropout. Applies to both encoder and decoder.    rnn_cell.dropout_output_keep_prob  1.0  Apply dropout to the (non-recurrent) outputs of each RNN layer using this keep probability. A value of  1.0  disables dropout. Applies to both encoder and decoder.    rnn_cell.num_layers  1  Number of RNN layers. Applies to both encoder and decoder.", 
            "title": "AttentionSeq2seq"
        }, 
        {
            "location": "/training/", 
            "text": "Input Files\n\n\nTo train a model you need to follow files. See \nData\n for more details on how to generate or download each of them.\n\n\n\n\nTraining data: Two parallel (line by line aligned) text files that are tokenized, i.e. have words separated by spaces.\n\n\nDevelopment data: Same format as the training data, but used for validation.\n\n\nSource vocabulary file: A file with one word per line that defines the source vocabulary.\n\n\nTarget vocabulary file: Same format as the source vocabulary, but for the target language.\n\n\n\n\nRunning Training\n\n\nFrom the root directory, run:\n\n\npython -m seq2seq.training.train \\\n--train_source=data/train.sources.txt \\\n--train_target=data/train.targets.txt \\\n--dev_source=data/dev.sources.txt \\\n--dev_target=data/dev.targets.txt \\\n--vocab_source=data/vocab_source \\\n--vocab_target=data/vocab_target \\\n--model AttentionSeq2Seq \\\n--batch_size=16 \\\n--hparams=\nrnn_cell.num_layers=2,\n  rnn_cell.type=GRUCell,\n  rnn_cell.num_units=128,\n  source.max_seq_len=40,\n  target.max_seq_len=40\n \\\n--output_dir=/path/to/model/dir \\\n--buckets=10,15,20,25,30,35\n\n\n\n\nHere, \ntrain_source\n, \ntrain_target\n \ndev_source\n, \ndev_source\n, \nvocab_source\n and \nvocab_target\n are the input files described above. \n\n\nmodel\n is the name of some class defined in \nseq2seq.models\n. Currently, the available models are:\n\n\n\n\nBasicSeq2Seq\n - Uses a unidirectional RNN encoder, passes state from encoder to decoder, and uses no attention mechanism.\n\n\nAttentionSeq2Seq\n - Uses a bidirectional RNN encoder and an attention mechanism.\n\n\n\n\nRefer to the source code comments for more details of these details.\n\n\nhparams\n are model-specific Hyperparameters. Refer to the \ndefault_params\n of model classes for a list of all available hyperparameters.\n\n\nbuckets\n is an optional argument that you can use to speed up training by bucketing training examples into batches of roughly equal length. \n10,15,20,25,30,35\n would result in 8 buckets: Sequences of length \n10\n, \n10..15\n, ..., and \n35\n.\n\n\nMonitoring Training\n\n\nIn addition to looking at the training output, you can use Tensorboard to monitor progress:\n\n\ntensorboard --logdir=/path/to/model/dir", 
            "title": "Training"
        }, 
        {
            "location": "/training/#input-files", 
            "text": "To train a model you need to follow files. See  Data  for more details on how to generate or download each of them.   Training data: Two parallel (line by line aligned) text files that are tokenized, i.e. have words separated by spaces.  Development data: Same format as the training data, but used for validation.  Source vocabulary file: A file with one word per line that defines the source vocabulary.  Target vocabulary file: Same format as the source vocabulary, but for the target language.", 
            "title": "Input Files"
        }, 
        {
            "location": "/training/#running-training", 
            "text": "From the root directory, run:  python -m seq2seq.training.train \\\n--train_source=data/train.sources.txt \\\n--train_target=data/train.targets.txt \\\n--dev_source=data/dev.sources.txt \\\n--dev_target=data/dev.targets.txt \\\n--vocab_source=data/vocab_source \\\n--vocab_target=data/vocab_target \\\n--model AttentionSeq2Seq \\\n--batch_size=16 \\\n--hparams= rnn_cell.num_layers=2,\n  rnn_cell.type=GRUCell,\n  rnn_cell.num_units=128,\n  source.max_seq_len=40,\n  target.max_seq_len=40  \\\n--output_dir=/path/to/model/dir \\\n--buckets=10,15,20,25,30,35  Here,  train_source ,  train_target   dev_source ,  dev_source ,  vocab_source  and  vocab_target  are the input files described above.   model  is the name of some class defined in  seq2seq.models . Currently, the available models are:   BasicSeq2Seq  - Uses a unidirectional RNN encoder, passes state from encoder to decoder, and uses no attention mechanism.  AttentionSeq2Seq  - Uses a bidirectional RNN encoder and an attention mechanism.   Refer to the source code comments for more details of these details.  hparams  are model-specific Hyperparameters. Refer to the  default_params  of model classes for a list of all available hyperparameters.  buckets  is an optional argument that you can use to speed up training by bucketing training examples into batches of roughly equal length.  10,15,20,25,30,35  would result in 8 buckets: Sequences of length  10 ,  10..15 , ..., and  35 .", 
            "title": "Running Training"
        }, 
        {
            "location": "/training/#monitoring-training", 
            "text": "In addition to looking at the training output, you can use Tensorboard to monitor progress:  tensorboard --logdir=/path/to/model/dir", 
            "title": "Monitoring Training"
        }, 
        {
            "location": "/inference/", 
            "text": "To make predictions with a trained model you need the following:\n\n\n\n\nA trained model directory, \nmodel_dir\n: The directory should contain the model checkpoint and a \nhparams.txt\n file with the hyperparameters used during training. This is the \noutput_dir\n you specify in the training script.\n\n\nA \nsource_text\n file in the same format as the \ntraining input data\n, one tokenized input example per line.\n\n\nVocabulary files for both source and targets, \nvocab_source\n and \nvocab_target\n. Same as used during training.\n\n\n\n\nGiven the above, you use \nscripts/infer.py\n as follows:\n\n\n./seq2seq/scripts/infer.py \\\n  --source sources.txt \\\n  --model_dir /tmp/model \\\n  --vocab_source vocab_source.txt \\\n  --vocab_target vocab_target.txt \n out.txt", 
            "title": "Inference"
        }, 
        {
            "location": "/tools/", 
            "text": "Tokenization\n\n\nTODO\n\n\nVocabulary Generation\n\n\nTODO\n\n\nDebugging Attention\n\n\nIf you trained an \nAttentionSeq2Seq\n model you can use the \nbin/print_attention.py\n script to dump the raw attention scores and generate alignment visualizations. The usage is similar to the inference script and uses the same input data. For example:\n\n\n./bin/print_attention.py \\\n  --source $HOME/nmt_data/toy_reverse/test/sources.txt \\\n  --vocab_source $HOME/nmt_data/toy_reverse/train/vocab.sources.txt \\\n  --vocab_target $HOME/nmt_data/toy_reverse/train/vocab.targets.txt \\\n  --model AttentionSeq2Seq \\\n  --model_dir ${TMPDIR}/nmt_toy_reverse \\\n  --output_dir ${TMPDIR}/attention\n\n\n\n\nBy default, the script generates an \nattention_score.npy\n array file and an attention plot per example. The array file can be \nloaded used numpy\n. It will be of shape \n[num_examples, target_length, source_length]\n. If you want only the raw attention score data without the plots you can pass the \n--no_plot\n flag. For more details and additional options see the \nprint_attention.py\n file.\n\n\nUsing Subword Units (BPE)\n\n\nTODO\n\n\nUsing an Alignment Dictionary\n\n\nTODO\n\n\nVisualizing Beam Search\n\n\nTODO", 
            "title": "Tools"
        }, 
        {
            "location": "/tools/#tokenization", 
            "text": "TODO", 
            "title": "Tokenization"
        }, 
        {
            "location": "/tools/#vocabulary-generation", 
            "text": "TODO", 
            "title": "Vocabulary Generation"
        }, 
        {
            "location": "/tools/#debugging-attention", 
            "text": "If you trained an  AttentionSeq2Seq  model you can use the  bin/print_attention.py  script to dump the raw attention scores and generate alignment visualizations. The usage is similar to the inference script and uses the same input data. For example:  ./bin/print_attention.py \\\n  --source $HOME/nmt_data/toy_reverse/test/sources.txt \\\n  --vocab_source $HOME/nmt_data/toy_reverse/train/vocab.sources.txt \\\n  --vocab_target $HOME/nmt_data/toy_reverse/train/vocab.targets.txt \\\n  --model AttentionSeq2Seq \\\n  --model_dir ${TMPDIR}/nmt_toy_reverse \\\n  --output_dir ${TMPDIR}/attention  By default, the script generates an  attention_score.npy  array file and an attention plot per example. The array file can be  loaded used numpy . It will be of shape  [num_examples, target_length, source_length] . If you want only the raw attention score data without the plots you can pass the  --no_plot  flag. For more details and additional options see the  print_attention.py  file.", 
            "title": "Debugging Attention"
        }, 
        {
            "location": "/tools/#using-subword-units-bpe", 
            "text": "TODO", 
            "title": "Using Subword Units (BPE)"
        }, 
        {
            "location": "/tools/#using-an-alignment-dictionary", 
            "text": "TODO", 
            "title": "Using an Alignment Dictionary"
        }, 
        {
            "location": "/tools/#visualizing-beam-search", 
            "text": "TODO", 
            "title": "Visualizing Beam Search"
        }, 
        {
            "location": "/benchmarks/", 
            "text": "Machine Translation\n\n\n\n\n\n\n\n\nCorpus\n\n\nSetttings\n\n\nTraining Time\n\n\nEvaluation\n\n\nDownloads\n\n\n\n\n\n\n\n\n\n\nWMT'16 English-German\u00a0(4.5M Sentence Pairs)\n\n\nHyperparameters\n\n\nN/A\n\n\n0.00 PPL \n 0.00 BLEU\n\n\nModel\n \n \nData\n\n\n\n\n\n\nWMT'16 English-French (?? Sentence Pairs)\n\n\n---\n\n\n---\n\n\n---\n\n\n---\n\n\n\n\n\n\nIWSLT 2015 English-Vietnamese (200k Sentence Pairs)\n\n\n---\n\n\n---\n\n\n---\n\n\n---", 
            "title": "Results and Pre-Trained Models"
        }, 
        {
            "location": "/benchmarks/#machine-translation", 
            "text": "Corpus  Setttings  Training Time  Evaluation  Downloads      WMT'16 English-German\u00a0(4.5M Sentence Pairs)  Hyperparameters  N/A  0.00 PPL   0.00 BLEU  Model     Data    WMT'16 English-French (?? Sentence Pairs)  ---  ---  ---  ---    IWSLT 2015 English-Vietnamese (200k Sentence Pairs)  ---  ---  ---  ---", 
            "title": "Machine Translation"
        }, 
        {
            "location": "/profiling/", 
            "text": "There are various ways to profile your model, from printing out the number of parameters to viewing detailed timing and device placement information for all graph operations.\n\n\nMetadataCaptureHook\n\n\nThe easiest way to generate profiling information is to use the \nMetadataCaptureHook\n, which saves a full graph trace and timeline information for a single configurable training step. By default these files are saved a \nmetadata\n subdirectory of your model checkpoint directory.\n\n\nTimelines\n\n\nIf you used the \nMetadataCaptureHook\n you can view the generated \ntimeline.json\n file in your web browser:\n\n\n\n\nGo to \nchrome://tracing\n\n\nLoad the \ntimeline.json\n file that was saved by the \nMetadataCaptureHook\n.\n\n\n\n\nFor complicated graphs timeline files can become quite large and analyzing them using Chrome may be slow and inconvenient. \n\n\nProfiling Script\n\n\nAn easy way to get basic information about your model is to run the \nprofile.py\n script. It uses \nTFProf\n to read metadata saved by the \nMetadataCaptureHook\n and generates several analyses.\n\n\n/seq2seq/scripts/profile.py --model_dir=/path/to/model/dir\n\n\n\n\nThis command will generate 4 files:\n\n\n/path/to/model/dir/params.txt\n contains an analysis of model parameter, including the number of parameters and their shapes and sizes:\n\n\natt_seq2seq/attention_decoder/RNN/rnn_step/logits/weights (384x103, 39.55k/39.55k params, 158.21KB/158.21KB)\n\n\n\n\n/path/to/model/dir/flops.txt\n contains information about floating point operations:\n\n\natt_seq2seq/OptimizeLoss/gradients/att_seq2seq/attention_decoder/RNN/while/rnn_step/attention/inputs_att/MatMul_grad/MatMul (18.87m/18.87m flops, 1.64ms/1.64ms, /job:localhost/replica:0/task:0/cpu:0)\n\n\n\n\n/path/to/model/dir/micro.txt\n contains microsecond timing information for operations that take longer than 1 milliseconds, organized by graph structure:\n\n\natt_seq2seq/attention_decoder/RNN/while/rnn_step/attention/mul_1 (1.89ms/13.72ms, /job:localhost/replica:0/task:0/cpu:0)\n  att_seq2seq/attention_decoder/RNN/while/rnn_step/attention/inputs_att/MatMul (1.21ms/11.83ms, /job:localhost/replica:0/task:0/cpu:0)\n  ....\n\n\n\n\n/path/to/model/dir/device.txt\n contains detailed device placement information for all operations.", 
            "title": "Profiling"
        }, 
        {
            "location": "/profiling/#metadatacapturehook", 
            "text": "The easiest way to generate profiling information is to use the  MetadataCaptureHook , which saves a full graph trace and timeline information for a single configurable training step. By default these files are saved a  metadata  subdirectory of your model checkpoint directory.", 
            "title": "MetadataCaptureHook"
        }, 
        {
            "location": "/profiling/#timelines", 
            "text": "If you used the  MetadataCaptureHook  you can view the generated  timeline.json  file in your web browser:   Go to  chrome://tracing  Load the  timeline.json  file that was saved by the  MetadataCaptureHook .   For complicated graphs timeline files can become quite large and analyzing them using Chrome may be slow and inconvenient.", 
            "title": "Timelines"
        }, 
        {
            "location": "/profiling/#profiling-script", 
            "text": "An easy way to get basic information about your model is to run the  profile.py  script. It uses  TFProf  to read metadata saved by the  MetadataCaptureHook  and generates several analyses.  /seq2seq/scripts/profile.py --model_dir=/path/to/model/dir  This command will generate 4 files:  /path/to/model/dir/params.txt  contains an analysis of model parameter, including the number of parameters and their shapes and sizes:  att_seq2seq/attention_decoder/RNN/rnn_step/logits/weights (384x103, 39.55k/39.55k params, 158.21KB/158.21KB)  /path/to/model/dir/flops.txt  contains information about floating point operations:  att_seq2seq/OptimizeLoss/gradients/att_seq2seq/attention_decoder/RNN/while/rnn_step/attention/inputs_att/MatMul_grad/MatMul (18.87m/18.87m flops, 1.64ms/1.64ms, /job:localhost/replica:0/task:0/cpu:0)  /path/to/model/dir/micro.txt  contains microsecond timing information for operations that take longer than 1 milliseconds, organized by graph structure:  att_seq2seq/attention_decoder/RNN/while/rnn_step/attention/mul_1 (1.89ms/13.72ms, /job:localhost/replica:0/task:0/cpu:0)\n  att_seq2seq/attention_decoder/RNN/while/rnn_step/attention/inputs_att/MatMul (1.21ms/11.83ms, /job:localhost/replica:0/task:0/cpu:0)\n  ....  /path/to/model/dir/device.txt  contains detailed device placement information for all operations.", 
            "title": "Profiling Script"
        }, 
        {
            "location": "/developer_guide/", 
            "text": "Development Setup\n\n\n1. Install Python3. If you're on a Mac the easiest way to do this is probably using \nHomebrew\n.\n\n\nbrew install python3\n\n\n\n\n2. Clone this repository.\n\n\ngit clone https://github.com/dennybritz/seq2seq.git\ncd seq2seq\n\n\n\n\n3. Create a new \nvirtual environment\n and activate it:\n\n\npython3 -m venv ~/tf-venv\nsource ~/tf-venv/bin/activate\n\n\n\n\n4. Install package dependencies and utilities. \n\n\npip install -e .\npip install nose pylint tox yapf mkdocs\n\n\n\n\n5. Make sure tests are passing.\n\n\nnosetests\n\n\n\n\n6. Code :)\n\n\nGithub Workflow\n\n\nPushing directly to the master branch is disabled and you must create feature branches and submit them via pull request. To make things easier you can also use the \nGithub Desktop app\n. A typical workflow looks as follows:\n\n\n# Make sure you are in the seq2seq root directory\n# Start from the master branch\ngit checkout master\n\n# Pull latest changes from github\ngit pull\n\n# Create a new feature branch\ngit checkout -b feature/my-new-feature\n\n# Make changes and commits\necho \nblabla\n \n test\ngit commit -am \nTest commit\n\n\n# Push the branch upstream\ngit push -u origin/my-new-feature\n\n# Submit a pull request on Github\n\n\n\n\nAfter you submit a Pull Request one person must review the change and\nCircleCI integration tests must be passing before you can merge into the\nmaster branch.\n\n\nPython Style\n\n\nWe use \npylint\n and \nyapf\n\nfor automated code formatting. Before submitting a pull request, make sure you\nrun them:\n\n\npylint ./seq2seq\nyapf -ir ./seq2seq\n\n\n\n\nNote that CirlceCI integrations test will fail if pylint reports any critical\nerrors, preventing you from merging your changes.\n\n\nTensorflow Style\n\n\nGraphModule\n\n\nAll classes that modify the Graph should inherit from \nseq2seq.graph_module.GraphModule\n, which is a wrapper around Tensorflow's \ntf.make_template\n function that allows for easy variable sharing. Basically, it allows you to do something like this:\n\n\nencode_fn = SomeEncoderModule(...)\n\n# New variables are created in this call.\noutput1 = encode_fn(input1)\n\n# No new variables are created here. The variables from the above call are re-used.\n# Note how this is different from normal Tensorflow where you would need to use variable scopes.\noutput2 = encode_fn(input2)\n\n# Because this is a new instance a second set of variables is created\nencode_fn2 = SomeEncoderModule(...)\noutput3 = encode_fn2(input3)\n\n\n\n\nFunctions vs. Classes\n\n\n\n\nOperations that \ncreate new variables\n must be implemented as classes and must inherit from \nGraphModule\n.\n\n\nOperations that \ndo not create new variables\n can be implemented as standard python functions, or as classes that inherit from \nGraphModule\n if they have a lot of logic.", 
            "title": "Developer Guide"
        }, 
        {
            "location": "/developer_guide/#development-setup", 
            "text": "1. Install Python3. If you're on a Mac the easiest way to do this is probably using  Homebrew .  brew install python3  2. Clone this repository.  git clone https://github.com/dennybritz/seq2seq.git\ncd seq2seq  3. Create a new  virtual environment  and activate it:  python3 -m venv ~/tf-venv\nsource ~/tf-venv/bin/activate  4. Install package dependencies and utilities.   pip install -e .\npip install nose pylint tox yapf mkdocs  5. Make sure tests are passing.  nosetests  6. Code :)", 
            "title": "Development Setup"
        }, 
        {
            "location": "/developer_guide/#github-workflow", 
            "text": "Pushing directly to the master branch is disabled and you must create feature branches and submit them via pull request. To make things easier you can also use the  Github Desktop app . A typical workflow looks as follows:  # Make sure you are in the seq2seq root directory\n# Start from the master branch\ngit checkout master\n\n# Pull latest changes from github\ngit pull\n\n# Create a new feature branch\ngit checkout -b feature/my-new-feature\n\n# Make changes and commits\necho  blabla    test\ngit commit -am  Test commit \n\n# Push the branch upstream\ngit push -u origin/my-new-feature\n\n# Submit a pull request on Github  After you submit a Pull Request one person must review the change and\nCircleCI integration tests must be passing before you can merge into the\nmaster branch.", 
            "title": "Github Workflow"
        }, 
        {
            "location": "/developer_guide/#python-style", 
            "text": "We use  pylint  and  yapf \nfor automated code formatting. Before submitting a pull request, make sure you\nrun them:  pylint ./seq2seq\nyapf -ir ./seq2seq  Note that CirlceCI integrations test will fail if pylint reports any critical\nerrors, preventing you from merging your changes.", 
            "title": "Python Style"
        }, 
        {
            "location": "/developer_guide/#tensorflow-style", 
            "text": "", 
            "title": "Tensorflow Style"
        }, 
        {
            "location": "/developer_guide/#graphmodule", 
            "text": "All classes that modify the Graph should inherit from  seq2seq.graph_module.GraphModule , which is a wrapper around Tensorflow's  tf.make_template  function that allows for easy variable sharing. Basically, it allows you to do something like this:  encode_fn = SomeEncoderModule(...)\n\n# New variables are created in this call.\noutput1 = encode_fn(input1)\n\n# No new variables are created here. The variables from the above call are re-used.\n# Note how this is different from normal Tensorflow where you would need to use variable scopes.\noutput2 = encode_fn(input2)\n\n# Because this is a new instance a second set of variables is created\nencode_fn2 = SomeEncoderModule(...)\noutput3 = encode_fn2(input3)", 
            "title": "GraphModule"
        }, 
        {
            "location": "/developer_guide/#functions-vs-classes", 
            "text": "Operations that  create new variables  must be implemented as classes and must inherit from  GraphModule .  Operations that  do not create new variables  can be implemented as standard python functions, or as classes that inherit from  GraphModule  if they have a lot of logic.", 
            "title": "Functions vs. Classes"
        }
    ]
}