{
    "docs": [
        {
            "location": "/",
            "text": "Introduction\n\n\ntf-seq2seq is a general-purpose encoder-decoder framework for Tensorflow that can be used for Machine Translation, Text Summarization, Conversational Modeling, Image Captioning, and more.\n\n\n\n\nDesign Goals\n\n\nWe built tf-seq2seq with the following goals in mind:\n\n\n\n\n\n\nGeneral Purpose\n: We initially built this framework for Machine Translation, but have since used it for a variety of other tasks, including Summarization, Conversational Modeling, and Image Captioning. As long as your problem can be phrased as encoding input data in one format and decoding it into another format, you should be able to use or extend this framework.\n\n\n\n\n\n\nUsability\n: You can train a model with a single command. Several types of input data are supported, including standard raw text.\n\n\n\n\n\n\nReproducibility\n: Training pipelines and models are configured using YAML files. This allows other to run your exact same model configurations.\n\n\n\n\n\n\nExtensibility\n: Code is structured in a modular way and that easy to build upon. For example, adding a new type of attention mechanism or encoder architecture requires only minimal code changes.\n\n\n\n\n\n\nDocumentation\n: All code is documented using standard Python docstrings, and we have written guides to help you get started with common tasks.\n\n\n\n\n\n\nGood Performance\n: For the sake of code simplicity, we did not try to squeeze out every last bit of performance, but the implementation is fast enough to cover almost all production and research use cases. tf-seq2seq also supports distributed training to trade off computational power and training time.\n\n\n\n\n\n\nFAQ\n\n\n1. How does this framework compare to the \nGoogle Neural Machine Translation\n system? Is this the official open-source implementation?\n\n\nNo, this is not an official open-source implementation of the GNMT system. This framework was built from the bottom up to cover a wider range of tasks, Neural Machine Translation being one of them. We have not replicated the exact GNMT architecture in this framework, but we welcome \ncontributions\n in that direction.\n\n\nRelated Frameworks\n\n\nThe following frameworks offer functionality similar to that of tf-seq2seq. We hope to collaborate with the authors of these frameworks so that we can learn from each other.\n\n\n\n\nOpenNMT (Torch)\n\n\nNeural Monkey (Tensorflow)\n\n\nNEMATUS (Theano)",
            "title": "Overview"
        },
        {
            "location": "/#introduction",
            "text": "tf-seq2seq is a general-purpose encoder-decoder framework for Tensorflow that can be used for Machine Translation, Text Summarization, Conversational Modeling, Image Captioning, and more.",
            "title": "Introduction"
        },
        {
            "location": "/#design-goals",
            "text": "We built tf-seq2seq with the following goals in mind:    General Purpose : We initially built this framework for Machine Translation, but have since used it for a variety of other tasks, including Summarization, Conversational Modeling, and Image Captioning. As long as your problem can be phrased as encoding input data in one format and decoding it into another format, you should be able to use or extend this framework.    Usability : You can train a model with a single command. Several types of input data are supported, including standard raw text.    Reproducibility : Training pipelines and models are configured using YAML files. This allows other to run your exact same model configurations.    Extensibility : Code is structured in a modular way and that easy to build upon. For example, adding a new type of attention mechanism or encoder architecture requires only minimal code changes.    Documentation : All code is documented using standard Python docstrings, and we have written guides to help you get started with common tasks.    Good Performance : For the sake of code simplicity, we did not try to squeeze out every last bit of performance, but the implementation is fast enough to cover almost all production and research use cases. tf-seq2seq also supports distributed training to trade off computational power and training time.",
            "title": "Design Goals"
        },
        {
            "location": "/#faq",
            "text": "1. How does this framework compare to the  Google Neural Machine Translation  system? Is this the official open-source implementation?  No, this is not an official open-source implementation of the GNMT system. This framework was built from the bottom up to cover a wider range of tasks, Neural Machine Translation being one of them. We have not replicated the exact GNMT architecture in this framework, but we welcome  contributions  in that direction.",
            "title": "FAQ"
        },
        {
            "location": "/#related-frameworks",
            "text": "The following frameworks offer functionality similar to that of tf-seq2seq. We hope to collaborate with the authors of these frameworks so that we can learn from each other.   OpenNMT (Torch)  Neural Monkey (Tensorflow)  NEMATUS (Theano)",
            "title": "Related Frameworks"
        },
        {
            "location": "/getting_started/",
            "text": "Download & Setup\n\n\nTo use tf-seq2seq you need a working installation of TensorFlow 1.0 with\nPython 2.7 or Python 3.5. Follow the \nTensorFlow Getting Started\n guide for detailed setup instructions. With TensorFlow installed, you can clone this repository:\n\n\ngit clone https://github.com/google/seq2seq.git\ncd seq2seq\n\n# Install package and dependencies\npip install -e .\n\n\n\n\nTo make sure everything works as expect you can run a simple pipeline unit test:\n\n\npython -m unittest seq2seq.test.pipeline_test\n\n\n\n\nIf you see a \"OK\" message, you are all set. Note that you may need to install pyrouge, pyyaml, and matplotlib, in order for these tests to pass. If you run into other setup issues,\nplease \nfile a Github issue\n.\n\n\nCommon Installation Issues\n\n\nIncorrect matploblib backend\n\n\nIn order to generate plots using matplotlib you need to have set the correct \nbackend\n. Also see this \nStackOverflow thread\n. To use the \nAgg\n backend, simply:\n\n\necho \"backend : Agg\" >> $HOME/.config/matplotlib/matplotlibrc\n\n\n\n\nNext Steps\n\n\n\n\nLearn about \nconcepts and terminology\n\n\nRead through the \nNeural Machine Translation Tutorial\n\n\nUse \npre-processed datasets\n or train a model on your own data\n\n\nContribute!",
            "title": "Getting Started"
        },
        {
            "location": "/getting_started/#download-setup",
            "text": "To use tf-seq2seq you need a working installation of TensorFlow 1.0 with\nPython 2.7 or Python 3.5. Follow the  TensorFlow Getting Started  guide for detailed setup instructions. With TensorFlow installed, you can clone this repository:  git clone https://github.com/google/seq2seq.git\ncd seq2seq\n\n# Install package and dependencies\npip install -e .  To make sure everything works as expect you can run a simple pipeline unit test:  python -m unittest seq2seq.test.pipeline_test  If you see a \"OK\" message, you are all set. Note that you may need to install pyrouge, pyyaml, and matplotlib, in order for these tests to pass. If you run into other setup issues,\nplease  file a Github issue .",
            "title": "Download &amp; Setup"
        },
        {
            "location": "/getting_started/#common-installation-issues",
            "text": "",
            "title": "Common Installation Issues"
        },
        {
            "location": "/getting_started/#incorrect-matploblib-backend",
            "text": "In order to generate plots using matplotlib you need to have set the correct  backend . Also see this  StackOverflow thread . To use the  Agg  backend, simply:  echo \"backend : Agg\" >> $HOME/.config/matplotlib/matplotlibrc",
            "title": "Incorrect matploblib backend"
        },
        {
            "location": "/getting_started/#next-steps",
            "text": "Learn about  concepts and terminology  Read through the  Neural Machine Translation Tutorial  Use  pre-processed datasets  or train a model on your own data  Contribute!",
            "title": "Next Steps"
        },
        {
            "location": "/concepts/",
            "text": "Configuration\n\n\nMany objects, including Encoders, Decoders, Models, Input Pipelines, and Inference Tasks, are configured using key-value parameters. These parameters are typically passed as \nYAML\n through configuration files or directly on the command line. For example, you can pass a \nmodel_params\n string to the training script configure model. Configurations are often be nested, as in the following example:\n\n\nmodel_params:\n  attention.class: seq2seq.decoders.attention.AttentionLayerBahdanau\n  attention.params:\n    num_units: 512\n  embedding.dim: 1024\n  encoder.class: seq2seq.encoders.BidirectionalRNNEncoder\n  encoder.params:\n    rnn_cell:\n      cell_class: LSTMCell\n      cell_params:\n        num_units: 512\n\n\n\n\nInput Pipeline\n\n\nAn \nInputPipeline\n defines how data is read, parsed, and separated into features and labels. For example, the \nParallelTextInputPipeline\n reads data from two text files, separates tokens by a delimiter, and produces tensors corresponding to the \nsource_tokens\n, \nsource_length\n, \ntarget_tokens\n, and \ntarget_length\n for each example. If you want to read new data formats you need to implement your own input pipeline.\n\n\nEncoder\n\n\nAn encoder reads in \"source data\", e.g. a sequence of words or an image, and produces a feature representation in continuous space. For example, a Recurrent Neural Network encoder may take as input a sequence of words and produce a fixed-length vector that roughly corresponds to the meaning of the text. An encoder based on a Convolutional Neural Network may take as input an image and generate a new volume that contains higher-level features of the image. The idea is that the representation produced by the encoder can be used by the Decoder to generate new data, e.g. a sentence in another language, or the description of the image. For a list of available encoders, see the \nEncoder Reference\n.\n\n\nDecoder\n\n\nA decoder is a generative model that is conditioned on the representation created by the encoder. For example, a Recurrent Neural Network decoder may learn generate the translation for an encoded sentence in another language. For a list of available decoder, see the \nDecoder Reference\n.\n\n\nModel\n\n\nA model defines how to put together an encoder and decoder, and how to calculate and minize the loss functions. It also handles the necessary preprocessing of data read from an input pipeline. Under the hood, each model is implemented as a \nmodel_fn passed to a tf.contrib.learn Estimator\n. For a list of available models, see the \nModels Reference\n.",
            "title": "Concepts"
        },
        {
            "location": "/concepts/#configuration",
            "text": "Many objects, including Encoders, Decoders, Models, Input Pipelines, and Inference Tasks, are configured using key-value parameters. These parameters are typically passed as  YAML  through configuration files or directly on the command line. For example, you can pass a  model_params  string to the training script configure model. Configurations are often be nested, as in the following example:  model_params:\n  attention.class: seq2seq.decoders.attention.AttentionLayerBahdanau\n  attention.params:\n    num_units: 512\n  embedding.dim: 1024\n  encoder.class: seq2seq.encoders.BidirectionalRNNEncoder\n  encoder.params:\n    rnn_cell:\n      cell_class: LSTMCell\n      cell_params:\n        num_units: 512",
            "title": "Configuration"
        },
        {
            "location": "/concepts/#input-pipeline",
            "text": "An  InputPipeline  defines how data is read, parsed, and separated into features and labels. For example, the  ParallelTextInputPipeline  reads data from two text files, separates tokens by a delimiter, and produces tensors corresponding to the  source_tokens ,  source_length ,  target_tokens , and  target_length  for each example. If you want to read new data formats you need to implement your own input pipeline.",
            "title": "Input Pipeline"
        },
        {
            "location": "/concepts/#encoder",
            "text": "An encoder reads in \"source data\", e.g. a sequence of words or an image, and produces a feature representation in continuous space. For example, a Recurrent Neural Network encoder may take as input a sequence of words and produce a fixed-length vector that roughly corresponds to the meaning of the text. An encoder based on a Convolutional Neural Network may take as input an image and generate a new volume that contains higher-level features of the image. The idea is that the representation produced by the encoder can be used by the Decoder to generate new data, e.g. a sentence in another language, or the description of the image. For a list of available encoders, see the  Encoder Reference .",
            "title": "Encoder"
        },
        {
            "location": "/concepts/#decoder",
            "text": "A decoder is a generative model that is conditioned on the representation created by the encoder. For example, a Recurrent Neural Network decoder may learn generate the translation for an encoded sentence in another language. For a list of available decoder, see the  Decoder Reference .",
            "title": "Decoder"
        },
        {
            "location": "/concepts/#model",
            "text": "A model defines how to put together an encoder and decoder, and how to calculate and minize the loss functions. It also handles the necessary preprocessing of data read from an input pipeline. Under the hood, each model is implemented as a  model_fn passed to a tf.contrib.learn Estimator . For a list of available models, see the  Models Reference .",
            "title": "Model"
        },
        {
            "location": "/nmt/",
            "text": "Neural Machine Translation Background\n\n\nThis tutorial is not meant to be a general introduction to Neural Machine Translation and does not go into detail of how these models works internally. For more details on the theory of Sequence-to-Sequence and Machine Translation models, we recommend the following resources:\n\n\n\n\nNeural Machine Translation and Sequence-to-sequence Models: A Tutorial (Neubig et al.)\n\n\nNeural Machine Translation by Jointly Learning to Align and Translate (Bahdanau et al.)\n\n\nTensorflow Sequence-To-Sequence Tutorial\n\n\n\n\nData Format\n\n\nA standard format used in both statistical and neural translation is the \nparallel text format\n. It consists of a pair of plain text with files corresponding to source sentences and target translations, aligned line-by-bline. For example,\n\n\nsources.en (English):\n\n\nMadam President, I should like to draw your attention to a case in which this Parliament has consistently shown an interest.\nIt is the case of Alexander Nikitin.\n\n\n\n\ntargets.de (German):\n\n\nFrau Pr\u00e4sidentin! Ich m\u00f6chte Sie auf einen Fall aufmerksam machen, mit dem sich dieses Parlament immer wieder befa\u00dft hat.\nDas ist der Fall von Alexander Nikitin.\n\n\n\n\nEach line corresponds to a sequence of \ntokens\n, separated by spaces. In the simplest case, the tokens are the words in the sentence. Typically, we use a tokenizer to split sentences into tokens while taking into account word stems and punctuation. For example, common choices for tokenizers are the Moses \ntokenizer.perl\n script or libraries such a \nspaCy\n, \nnltk\n or \nStanford Tokenizer\n.\n\n\nHowever, learning a model based on words has a couple of drawbacks. Because NMT models output a probability distribution over words, they can became very slow with large number of possible words. If you include misspellings and derived words in your vocabulary, the number of possible words is essentially ininfite and we need to impose an artificial limit of how of the most common words we want our model to handle. This is also called the \nvocabulary size\n and typically set to something in the range of 10,000 to 100,000. Another drawback of training on word tokens is that the model does not learn about common \"stems\" of words. For example, it would consider \"loved\" and \"loving\" as completely separate classes despite their common root.\n\n\nOne way to handle an \nopen vocabulary\n issue is learn \nsubword units\n for a given text. For example, the word \"loved\" may be split up into \"lov\" and \"ed\", while \"loving\" would be split up into \"lov\" and \"ing\". This allows to model to generalize to new words, while also resulting in a smaller vocabulary size. There are several techniques for learning such subword units, including \nByte Pair Encoding (BPE)\n, which is what we used in this tutorial. To generate a BPE for a given text, you can follow the instructions in the official \nsubword-nmt\n repository:\n\n\n# Clone from Github\ngit clone https://github.com/rsennrich/subword-nmt\ncd subword-nmt\n\n# Learn a vocabulary using 10,000 merge operations\n./learn_bpe.py -s 10000 < train.tok > codes.bpe\n\n# Apply the vocabulary to the training file\n./apply_bpe.py -c codes.bpe < train.tok > train.tok.bpe\n\n\n\n\nAfter tokenizing and applying BPE to a dataset, the original sentences may look like the following. Note that the name \"Nikitin\" is a rare word that has been split up into subword units delimited by \n@@\n.\n\n\nMadam President , I should like to draw your attention to a case in which this Parliament has consistently shown an interest .\nIt is the case of Alexander Ni@@ ki@@ tin .\n\n\n\n\nFrau Pr\u00e4sidentin ! Ich m\u00f6chte Sie auf einen Fall aufmerksam machen , mit dem sich dieses Parlament immer wieder befa\u00dft hat .\nDas ist der Fall von Alexander Ni@@ ki@@ tin\n\n\n\n\nDownload Data\n\n\nTo make it easy to get started we have prepared an already pre-processed dataset based on the \nEnglish-German WMT'16 Translation Task\n. To learn more about how the data was generated, you can take a look at the \nwmt16_en_de.sh\n data generation script. The script downloads the data, tokenizes it using the \nMoses Tokenizer\n, cleans the training data, and learns a vocabulary of ~32,000 subword units.\n\n\n\n\nDownload pre-processed WMT'16 EN-DE Data (502MB)\n\n\n\n\nAfter extraction, you should see the folowing files:\n\n\n\n\n\n\n\n\nFilename\n\n\nDescription\n\n\n\n\n\n\n\n\n\n\ntrain.tok.clean.bpe.32000.en\n\n\nThe English training data, one sentence per line, processed using BPE.\n\n\n\n\n\n\ntrain.tok.clean.bpe.32000.de\n\n\nThe German training data, one sentence per line, processed using BPE.\n\n\n\n\n\n\nvocab.bpe.32000\n\n\nThe full vocabulary used in the training data, one token per line.\n\n\n\n\n\n\nnewstestXXXX.*\n\n\nDevelopment and test data sets, in the same format as the training data. We provide both pre-processed and original data files used for evaluation.\n\n\n\n\n\n\n\n\nLet's set a few data-specific environment variables so that we can easily use them later on:\n\n\n# Set this to where you extracted the downloaded file\nexport DATA_PATH=\n\nexport VOCAB_SOURCE=${DATA_PATH}/vocab.bpe.32000\nexport VOCAB_TARGET=${DATA_PATH}/vocab.bpe.32000\nexport TRAIN_SOURCES=${DATA_PATH}/train.tok.clean.bpe.32000.en\nexport TRAIN_TARGETS=${DATA_PATH}/train.tok.clean.bpe.32000.de\nexport DEV_SOURCES=${DATA_PATH}/newstest2013.tok.bpe.32000.en\nexport DEV_TARGETS=${DATA_PATH}/newstest2013.tok.bpe.32000.de\n\nexport DEV_TARGETS_REF=${DATA_PATH}/newstest2013.tok.de\nexport TRAIN_STEPS=1000000\n\n\n\n\nAlternative: Generate Toy Data\n\n\nTraining on real-world translation data can take a very long time. If you do not have access to a machine with a GPU but would like to play around with a smaller dataset, we provide a way to generate toy data. The following command will generate a dataset where the target sequences are reversed source sequences. That is, the model needs to learn the reverse the inputs.  While this task is not very useful in practice, we can train such a model quickly and use it as as sanity-check to make sure that the end-to-end pipeline is working as intended.\n\n\nDATA_TYPE=reverse ./bin/data/toy.sh\n\n\n\n\nInstead of the translation data, use the files generated by the script:\n\n\nexport VOCAB_SOURCE=${HOME}/nmt_data/toy_reverse/train/vocab.sources.txt\nexport VOCAB_TARGET=${HOME}/nmt_data/toy_reverse/train/vocab.targets.txt\nexport TRAIN_SOURCES=${HOME}/nmt_data/toy_reverse/train/sources.txt\nexport TRAIN_TARGETS=${HOME}/nmt_data/toy_reverse/train/targets.txt\nexport DEV_SOURCES=${HOME}/nmt_data/toy_reverse/dev/sources.txt\nexport DEV_TARGETS=${HOME}/nmt_data/toy_reverse/dev/targets.txt\n\nexport DEV_TARGETS_REF=${HOME}/nmt_data/toy_reverse/dev/targets.txt\nexport TRAIN_STEPS=1000\n\n\n\n\nDefining the model\n\n\nWith the data in place, it is now time to define what type of model we would like to train. The standard choice is a \nSequence-To-Sequence model with attention\n. This type of model has a large number of available hyperparameters, or knobs you can tune, all of which will affect training time and final performance. That's why we provide \nexample configurations\n for small, medium, and large models that you can use or extend. For example, the configuration for the medium-sized model look as follows:\n\n\nmodel: AttentionSeq2Seq\nmodel_params:\n  attention.class: seq2seq.decoders.attention.AttentionLayerBahdanau\n  attention.params:\n    num_units: 256\n  bridge.class: seq2seq.models.bridges.ZeroBridge\n  embedding.dim: 256\n  encoder.class: seq2seq.encoders.BidirectionalRNNEncoder\n  encoder.params:\n    rnn_cell:\n      cell_class: GRUCell\n      cell_params:\n        num_units: 256\n      dropout_input_keep_prob: 0.8\n      dropout_output_keep_prob: 1.0\n      num_layers: 1\n  decoder.class: seq2seq.decoders.AttentionDecoder\n  decoder.params:\n    rnn_cell:\n      cell_class: GRUCell\n      cell_params:\n        num_units: 256\n      dropout_input_keep_prob: 0.8\n      dropout_output_keep_prob: 1.0\n      num_layers: 2\n  optimizer.name: Adam\n  optimizer.learning_rate: 0.0001\n  source.max_seq_len: 50\n  source.reverse: false\n  target.max_seq_len: 50\n\n\n\n\nYou can write your own configuration files in YAML/JSON format, overwrite configuration values with another file, or pass them directly to the training script. We found that saving configuration files is useful for reproducibility and experiment tracking.\n\n\nTraining\n\n\nNow that we have both a model configuration and training data we can start the actual training process. On a single modern GPU (e.g. a TitanX), training to convergence can easily take a few days for the WMT'16 English-German data, even with the small model configuration. We found that the large model configuration typically trains in 2-3 days on 8 GPUs using distributed training in Tensorflow. The toy data should train in ~10 minutes on a CPU, and 1000 steps are sufficient.\n\n\nexport MODEL_DIR=${TMPDIR:-/tmp}/nmt_tutorial\nmkdir -p $MODEL_DIR\n\npython -m bin.train \\\n  --config_paths=\"\n      ./example_configs/nmt_small.yml,\n      ./example_configs/train_seq2seq.yml,\n      ./example_configs/text_metrics_bpe.yml\" \\\n  --model_params \"\n      vocab_source: $VOCAB_SOURCE\n      vocab_target: $VOCAB_TARGET\" \\\n  --input_pipeline_train \"\n    class: ParallelTextInputPipeline\n    params:\n      source_files:\n        - $TRAIN_SOURCES\n      target_files:\n        - $TRAIN_TARGETS\" \\\n  --input_pipeline_dev \"\n    class: ParallelTextInputPipeline\n    params:\n       source_files:\n        - $DEV_SOURCES\n       target_files:\n        - $DEV_TARGETS\" \\\n  --batch_size 32 \\\n  --train_steps $TRAIN_STEPS \\\n  --output_dir $MODEL_DIR\n\n\n\n\nLet's look at the training command in more detail and explore what each of the options mean.\n\n\n\n\nThe \nconfig_paths\n argument allows you to pass one or more configuration files used for training. Configuration files are merged in the order they are specified, which allows you to overwrite specific values in your own files. Here, we pass two configuration files. The file \nnmt_small.yml\n contains the model type and hyperparameters (as explained in the previous section) and \ntrain_seq2seq.yml\n contains common options about the training process, such as which metrics to track, and how often to sample responses.\n\n\nThe \nmodel_params\n argument allows us to overwrite model parameters. It is YAML/JSON string. Most of the parameters are defined in the \nnmt_small.yml\n file, but since the vocabulary depends on the data we are using, we need to overwrite it from the command line.\n\n\nThe \ninput_pipeline_train\n and \ninput_pipeline_dev\n arguments define how to read training and development data. In our case, we use a parallel text format. Note that you would typically define these arguments in the configuration file. We don't do this in this tutorial because the path of the extracted data depends on your local setup.\n\n\nThe \noutput_dir\n is the desination directory for model checkpoints and summaries.\n\n\n\n\nThroughout the traning process you will see the loss decreasing and samples generated by the model. To monitor the training process, you can start Tensorboard pointing to the output directory:\n\n\ntensorboard --logdir $MODEL_DIR\n\n\n\n\n\n\n\n\nMaking predictions\n\n\nOnce your model is trained, you can start predictions, i.e. translating new data from German to English. For example:\n\n\nexport PRED_DIR=${MODEL_DIR}/pred\nmkdir -p ${PRED_DIR}\n\npython -m bin.infer \\\n  --tasks \"\n    - class: DecodeText\" \\\n  --model_dir $MODEL_DIR \\\n  --input_pipeline \"\n    class: ParallelTextInputPipeline\n    params:\n      source_files:\n        - $DEV_SOURCES\" \\\n  >  ${PRED_DIR}/predictions.txt\n\n\n\n\nLet's take a closer look at this command again:\n\n\n\n\nThe \ntasks\n argument specifies which inference tasks to run. It is a YAML/JSON string, containing a list of class names and optional parameters. In our command, we only execute the \nDecodeText\n task, which takes the model predictions and prints them to stdout. Other possible tasks include \nDumpAttention\n or \nDumpBeams\n, which can be used to write  debugging information about what your model is doing. For more details, refer to the \nInference\n section.\n\n\nThe \nmodel_dir\n argument points to the path containing the model checkpoints. It is the same as the \noutput_dir\n passed during training.\n\n\ninput_pipeline\n defines how we read data and is of the same format as the input pipeline definition used during training.\n\n\n\n\nDecoding with Beam Search\n\n\nBeam Search is a commonly used decoding technique that improves translation performance. Instead of decoding the most probable word in a greedy fashion, beam search keeps several hypotheses, or \"beams\", in memory and chooses the best one based on a scoring function. To enable beam search decoding, you can overwrite the appropriate model parameter when running inference. Note that decoding with beam search will take significantly longer.\n\n\npython -m bin.infer \\\n  --tasks \"\n    - class: DecodeText\n    - class: DumpBeams\n      params:\n        file: ${PRED_DIR}/beams.npz\" \\\n  --model_dir $MODEL_DIR \\\n  --model_params \"\n    inference.beam_search.beam_width: 5\" \\\n  --input_pipeline \"\n    class: ParallelTextInputPipeline\n    params:\n      source_files:\n        - $DEV_SOURCES\" \\\n  > ${PRED_DIR}/predictions.txt\n\n\n\n\nThe above command also demonstrates how to pass several tasks to the inference script. In our case, we also dump beam search debugging information to a file on disk so that we can inspect it later.\n\n\nEvaluating specific checkpoint\n\n\nThe training script will save multiple model checkpoints throughout training. The exact checkpoint behavior can be controlled via \ntraining script flags\n. By default, the inference script evaluates the latest checkpoint in the model directory. To evaluate a specific checkpiint you can pass the \ncheckpoint_path\n flag.\n\n\nCalcuating BLEU scores\n\n\nBLEU\n is a commonly used metric to evaluate translation performance. Now that you have generated prediction in plain text format, you can evaluate your translations against the reference translations using BLEU scores:\n\n\n./bin/tools/multi-bleu.perl ${DEV_TARGETS_REF} < ${PRED_DIR}/predictions.txt\n\n\n\n\nThe \nmulti-bleu.perl\n script is taken from \nMoses\n and is one of the most common ways to calculcate BLEU. Note that we calculate BLEU scores on tokenized text. An alternative is to calculate BLEU on untokenized text. To do this, would first need to detokenize your model outputs.",
            "title": "Tutorial: Neural Machine Translation"
        },
        {
            "location": "/nmt/#neural-machine-translation-background",
            "text": "This tutorial is not meant to be a general introduction to Neural Machine Translation and does not go into detail of how these models works internally. For more details on the theory of Sequence-to-Sequence and Machine Translation models, we recommend the following resources:   Neural Machine Translation and Sequence-to-sequence Models: A Tutorial (Neubig et al.)  Neural Machine Translation by Jointly Learning to Align and Translate (Bahdanau et al.)  Tensorflow Sequence-To-Sequence Tutorial",
            "title": "Neural Machine Translation Background"
        },
        {
            "location": "/nmt/#data-format",
            "text": "A standard format used in both statistical and neural translation is the  parallel text format . It consists of a pair of plain text with files corresponding to source sentences and target translations, aligned line-by-bline. For example,  sources.en (English):  Madam President, I should like to draw your attention to a case in which this Parliament has consistently shown an interest.\nIt is the case of Alexander Nikitin.  targets.de (German):  Frau Pr\u00e4sidentin! Ich m\u00f6chte Sie auf einen Fall aufmerksam machen, mit dem sich dieses Parlament immer wieder befa\u00dft hat.\nDas ist der Fall von Alexander Nikitin.  Each line corresponds to a sequence of  tokens , separated by spaces. In the simplest case, the tokens are the words in the sentence. Typically, we use a tokenizer to split sentences into tokens while taking into account word stems and punctuation. For example, common choices for tokenizers are the Moses  tokenizer.perl  script or libraries such a  spaCy ,  nltk  or  Stanford Tokenizer .  However, learning a model based on words has a couple of drawbacks. Because NMT models output a probability distribution over words, they can became very slow with large number of possible words. If you include misspellings and derived words in your vocabulary, the number of possible words is essentially ininfite and we need to impose an artificial limit of how of the most common words we want our model to handle. This is also called the  vocabulary size  and typically set to something in the range of 10,000 to 100,000. Another drawback of training on word tokens is that the model does not learn about common \"stems\" of words. For example, it would consider \"loved\" and \"loving\" as completely separate classes despite their common root.  One way to handle an  open vocabulary  issue is learn  subword units  for a given text. For example, the word \"loved\" may be split up into \"lov\" and \"ed\", while \"loving\" would be split up into \"lov\" and \"ing\". This allows to model to generalize to new words, while also resulting in a smaller vocabulary size. There are several techniques for learning such subword units, including  Byte Pair Encoding (BPE) , which is what we used in this tutorial. To generate a BPE for a given text, you can follow the instructions in the official  subword-nmt  repository:  # Clone from Github\ngit clone https://github.com/rsennrich/subword-nmt\ncd subword-nmt\n\n# Learn a vocabulary using 10,000 merge operations\n./learn_bpe.py -s 10000 < train.tok > codes.bpe\n\n# Apply the vocabulary to the training file\n./apply_bpe.py -c codes.bpe < train.tok > train.tok.bpe  After tokenizing and applying BPE to a dataset, the original sentences may look like the following. Note that the name \"Nikitin\" is a rare word that has been split up into subword units delimited by  @@ .  Madam President , I should like to draw your attention to a case in which this Parliament has consistently shown an interest .\nIt is the case of Alexander Ni@@ ki@@ tin .  Frau Pr\u00e4sidentin ! Ich m\u00f6chte Sie auf einen Fall aufmerksam machen , mit dem sich dieses Parlament immer wieder befa\u00dft hat .\nDas ist der Fall von Alexander Ni@@ ki@@ tin",
            "title": "Data Format"
        },
        {
            "location": "/nmt/#download-data",
            "text": "To make it easy to get started we have prepared an already pre-processed dataset based on the  English-German WMT'16 Translation Task . To learn more about how the data was generated, you can take a look at the  wmt16_en_de.sh  data generation script. The script downloads the data, tokenizes it using the  Moses Tokenizer , cleans the training data, and learns a vocabulary of ~32,000 subword units.   Download pre-processed WMT'16 EN-DE Data (502MB)   After extraction, you should see the folowing files:     Filename  Description      train.tok.clean.bpe.32000.en  The English training data, one sentence per line, processed using BPE.    train.tok.clean.bpe.32000.de  The German training data, one sentence per line, processed using BPE.    vocab.bpe.32000  The full vocabulary used in the training data, one token per line.    newstestXXXX.*  Development and test data sets, in the same format as the training data. We provide both pre-processed and original data files used for evaluation.     Let's set a few data-specific environment variables so that we can easily use them later on:  # Set this to where you extracted the downloaded file\nexport DATA_PATH=\n\nexport VOCAB_SOURCE=${DATA_PATH}/vocab.bpe.32000\nexport VOCAB_TARGET=${DATA_PATH}/vocab.bpe.32000\nexport TRAIN_SOURCES=${DATA_PATH}/train.tok.clean.bpe.32000.en\nexport TRAIN_TARGETS=${DATA_PATH}/train.tok.clean.bpe.32000.de\nexport DEV_SOURCES=${DATA_PATH}/newstest2013.tok.bpe.32000.en\nexport DEV_TARGETS=${DATA_PATH}/newstest2013.tok.bpe.32000.de\n\nexport DEV_TARGETS_REF=${DATA_PATH}/newstest2013.tok.de\nexport TRAIN_STEPS=1000000",
            "title": "Download Data"
        },
        {
            "location": "/nmt/#alternative-generate-toy-data",
            "text": "Training on real-world translation data can take a very long time. If you do not have access to a machine with a GPU but would like to play around with a smaller dataset, we provide a way to generate toy data. The following command will generate a dataset where the target sequences are reversed source sequences. That is, the model needs to learn the reverse the inputs.  While this task is not very useful in practice, we can train such a model quickly and use it as as sanity-check to make sure that the end-to-end pipeline is working as intended.  DATA_TYPE=reverse ./bin/data/toy.sh  Instead of the translation data, use the files generated by the script:  export VOCAB_SOURCE=${HOME}/nmt_data/toy_reverse/train/vocab.sources.txt\nexport VOCAB_TARGET=${HOME}/nmt_data/toy_reverse/train/vocab.targets.txt\nexport TRAIN_SOURCES=${HOME}/nmt_data/toy_reverse/train/sources.txt\nexport TRAIN_TARGETS=${HOME}/nmt_data/toy_reverse/train/targets.txt\nexport DEV_SOURCES=${HOME}/nmt_data/toy_reverse/dev/sources.txt\nexport DEV_TARGETS=${HOME}/nmt_data/toy_reverse/dev/targets.txt\n\nexport DEV_TARGETS_REF=${HOME}/nmt_data/toy_reverse/dev/targets.txt\nexport TRAIN_STEPS=1000",
            "title": "Alternative: Generate Toy Data"
        },
        {
            "location": "/nmt/#defining-the-model",
            "text": "With the data in place, it is now time to define what type of model we would like to train. The standard choice is a  Sequence-To-Sequence model with attention . This type of model has a large number of available hyperparameters, or knobs you can tune, all of which will affect training time and final performance. That's why we provide  example configurations  for small, medium, and large models that you can use or extend. For example, the configuration for the medium-sized model look as follows:  model: AttentionSeq2Seq\nmodel_params:\n  attention.class: seq2seq.decoders.attention.AttentionLayerBahdanau\n  attention.params:\n    num_units: 256\n  bridge.class: seq2seq.models.bridges.ZeroBridge\n  embedding.dim: 256\n  encoder.class: seq2seq.encoders.BidirectionalRNNEncoder\n  encoder.params:\n    rnn_cell:\n      cell_class: GRUCell\n      cell_params:\n        num_units: 256\n      dropout_input_keep_prob: 0.8\n      dropout_output_keep_prob: 1.0\n      num_layers: 1\n  decoder.class: seq2seq.decoders.AttentionDecoder\n  decoder.params:\n    rnn_cell:\n      cell_class: GRUCell\n      cell_params:\n        num_units: 256\n      dropout_input_keep_prob: 0.8\n      dropout_output_keep_prob: 1.0\n      num_layers: 2\n  optimizer.name: Adam\n  optimizer.learning_rate: 0.0001\n  source.max_seq_len: 50\n  source.reverse: false\n  target.max_seq_len: 50  You can write your own configuration files in YAML/JSON format, overwrite configuration values with another file, or pass them directly to the training script. We found that saving configuration files is useful for reproducibility and experiment tracking.",
            "title": "Defining the model"
        },
        {
            "location": "/nmt/#training",
            "text": "Now that we have both a model configuration and training data we can start the actual training process. On a single modern GPU (e.g. a TitanX), training to convergence can easily take a few days for the WMT'16 English-German data, even with the small model configuration. We found that the large model configuration typically trains in 2-3 days on 8 GPUs using distributed training in Tensorflow. The toy data should train in ~10 minutes on a CPU, and 1000 steps are sufficient.  export MODEL_DIR=${TMPDIR:-/tmp}/nmt_tutorial\nmkdir -p $MODEL_DIR\n\npython -m bin.train \\\n  --config_paths=\"\n      ./example_configs/nmt_small.yml,\n      ./example_configs/train_seq2seq.yml,\n      ./example_configs/text_metrics_bpe.yml\" \\\n  --model_params \"\n      vocab_source: $VOCAB_SOURCE\n      vocab_target: $VOCAB_TARGET\" \\\n  --input_pipeline_train \"\n    class: ParallelTextInputPipeline\n    params:\n      source_files:\n        - $TRAIN_SOURCES\n      target_files:\n        - $TRAIN_TARGETS\" \\\n  --input_pipeline_dev \"\n    class: ParallelTextInputPipeline\n    params:\n       source_files:\n        - $DEV_SOURCES\n       target_files:\n        - $DEV_TARGETS\" \\\n  --batch_size 32 \\\n  --train_steps $TRAIN_STEPS \\\n  --output_dir $MODEL_DIR  Let's look at the training command in more detail and explore what each of the options mean.   The  config_paths  argument allows you to pass one or more configuration files used for training. Configuration files are merged in the order they are specified, which allows you to overwrite specific values in your own files. Here, we pass two configuration files. The file  nmt_small.yml  contains the model type and hyperparameters (as explained in the previous section) and  train_seq2seq.yml  contains common options about the training process, such as which metrics to track, and how often to sample responses.  The  model_params  argument allows us to overwrite model parameters. It is YAML/JSON string. Most of the parameters are defined in the  nmt_small.yml  file, but since the vocabulary depends on the data we are using, we need to overwrite it from the command line.  The  input_pipeline_train  and  input_pipeline_dev  arguments define how to read training and development data. In our case, we use a parallel text format. Note that you would typically define these arguments in the configuration file. We don't do this in this tutorial because the path of the extracted data depends on your local setup.  The  output_dir  is the desination directory for model checkpoints and summaries.   Throughout the traning process you will see the loss decreasing and samples generated by the model. To monitor the training process, you can start Tensorboard pointing to the output directory:  tensorboard --logdir $MODEL_DIR",
            "title": "Training"
        },
        {
            "location": "/nmt/#making-predictions",
            "text": "Once your model is trained, you can start predictions, i.e. translating new data from German to English. For example:  export PRED_DIR=${MODEL_DIR}/pred\nmkdir -p ${PRED_DIR}\n\npython -m bin.infer \\\n  --tasks \"\n    - class: DecodeText\" \\\n  --model_dir $MODEL_DIR \\\n  --input_pipeline \"\n    class: ParallelTextInputPipeline\n    params:\n      source_files:\n        - $DEV_SOURCES\" \\\n  >  ${PRED_DIR}/predictions.txt  Let's take a closer look at this command again:   The  tasks  argument specifies which inference tasks to run. It is a YAML/JSON string, containing a list of class names and optional parameters. In our command, we only execute the  DecodeText  task, which takes the model predictions and prints them to stdout. Other possible tasks include  DumpAttention  or  DumpBeams , which can be used to write  debugging information about what your model is doing. For more details, refer to the  Inference  section.  The  model_dir  argument points to the path containing the model checkpoints. It is the same as the  output_dir  passed during training.  input_pipeline  defines how we read data and is of the same format as the input pipeline definition used during training.",
            "title": "Making predictions"
        },
        {
            "location": "/nmt/#decoding-with-beam-search",
            "text": "Beam Search is a commonly used decoding technique that improves translation performance. Instead of decoding the most probable word in a greedy fashion, beam search keeps several hypotheses, or \"beams\", in memory and chooses the best one based on a scoring function. To enable beam search decoding, you can overwrite the appropriate model parameter when running inference. Note that decoding with beam search will take significantly longer.  python -m bin.infer \\\n  --tasks \"\n    - class: DecodeText\n    - class: DumpBeams\n      params:\n        file: ${PRED_DIR}/beams.npz\" \\\n  --model_dir $MODEL_DIR \\\n  --model_params \"\n    inference.beam_search.beam_width: 5\" \\\n  --input_pipeline \"\n    class: ParallelTextInputPipeline\n    params:\n      source_files:\n        - $DEV_SOURCES\" \\\n  > ${PRED_DIR}/predictions.txt  The above command also demonstrates how to pass several tasks to the inference script. In our case, we also dump beam search debugging information to a file on disk so that we can inspect it later.",
            "title": "Decoding with Beam Search"
        },
        {
            "location": "/nmt/#evaluating-specific-checkpoint",
            "text": "The training script will save multiple model checkpoints throughout training. The exact checkpoint behavior can be controlled via  training script flags . By default, the inference script evaluates the latest checkpoint in the model directory. To evaluate a specific checkpiint you can pass the  checkpoint_path  flag.",
            "title": "Evaluating specific checkpoint"
        },
        {
            "location": "/nmt/#calcuating-bleu-scores",
            "text": "BLEU  is a commonly used metric to evaluate translation performance. Now that you have generated prediction in plain text format, you can evaluate your translations against the reference translations using BLEU scores:  ./bin/tools/multi-bleu.perl ${DEV_TARGETS_REF} < ${PRED_DIR}/predictions.txt  The  multi-bleu.perl  script is taken from  Moses  and is one of the most common ways to calculcate BLEU. Note that we calculate BLEU scores on tokenized text. An alternative is to calculate BLEU on untokenized text. To do this, would first need to detokenize your model outputs.",
            "title": "Calcuating BLEU scores"
        },
        {
            "location": "/summarization/",
            "text": "Coming Soon\n\n\nTraining a summarization model is very similar to \ntraining a Neural Machine Translation\n. Please refer to NMT tutorial for the time being while we are working on a summarization-specific tutorial.",
            "title": "Tutorial: Summarization"
        },
        {
            "location": "/summarization/#coming-soon",
            "text": "Training a summarization model is very similar to  training a Neural Machine Translation . Please refer to NMT tutorial for the time being while we are working on a summarization-specific tutorial.",
            "title": "Coming Soon"
        },
        {
            "location": "/image_captioning/",
            "text": "Coming Soon\n\n\nThis tutorial is coming soon. It is easy to swap out the RNN encoder with a Convolutional Neural Network to perform image captioning.",
            "title": "Tutorial: Image Captioning"
        },
        {
            "location": "/image_captioning/#coming-soon",
            "text": "This tutorial is coming soon. It is easy to swap out the RNN encoder with a Convolutional Neural Network to perform image captioning.",
            "title": "Coming Soon"
        },
        {
            "location": "/data/",
            "text": "Available Datasets\n\n\nWe provide data generation scripts to generate standard datasets.\n\n\n\n\n\n\n\n\nDataset\n\n\nDescription\n\n\nTraining/Dev/Test Size\n\n\nVocabulary\n\n\nDownload\n\n\n\n\n\n\n\n\n\n\nWMT'16 EN-DE\n\n\nData for the \nWMT'16 Translation Task\n English to German. Training data is combined from Europarl v7, Common Crawl, and News Commentary v11. Development data sets include \nnewstest[2010-2015]\n. \nnewstest2016\n should serve as test data. All SGM files were converted to plain text.\n\n\n4.56M/3K/2.6K\n\n\n32k BPE\n\n\nGenerate\n \n \nDownload\n\n\n\n\n\n\nWMT'17 All Pairs\n\n\nData for the \nWMT'17 Translation Task\n.\n\n\nComing soon.\n\n\nComing soon.\n\n\nComing soon\n\n\n\n\n\n\nToy Copy\n\n\nA toy dataset where the target sequence is equal to the source sequence. The model must learn to copy the source sequence.\n\n\n10k/1k/1k\n\n\n20\n\n\nGenerate\n\n\n\n\n\n\nToy Reverse\n\n\nA toy dataset where the target sequence is equal to the reversed source sequence. The model must learn to reverse the source sequence.\n\n\n10k/1k/1k\n\n\n20\n\n\nGenerate\n\n\n\n\n\n\n\n\nCreating your own data\n\n\nTo create your own data, we recommend taking a look at the data generation scripts above. A typical data preprocessing pipeline looks as follows:\n\n\n\n\nGenerate data in parallel text format\n\n\nTokenize your data\n\n\nCreate fixed vocabularies for your source and target data\n\n\nLearn and apply subword units to handle rare and unknown words",
            "title": "Data"
        },
        {
            "location": "/data/#available-datasets",
            "text": "We provide data generation scripts to generate standard datasets.     Dataset  Description  Training/Dev/Test Size  Vocabulary  Download      WMT'16 EN-DE  Data for the  WMT'16 Translation Task  English to German. Training data is combined from Europarl v7, Common Crawl, and News Commentary v11. Development data sets include  newstest[2010-2015] .  newstest2016  should serve as test data. All SGM files were converted to plain text.  4.56M/3K/2.6K  32k BPE  Generate     Download    WMT'17 All Pairs  Data for the  WMT'17 Translation Task .  Coming soon.  Coming soon.  Coming soon    Toy Copy  A toy dataset where the target sequence is equal to the source sequence. The model must learn to copy the source sequence.  10k/1k/1k  20  Generate    Toy Reverse  A toy dataset where the target sequence is equal to the reversed source sequence. The model must learn to reverse the source sequence.  10k/1k/1k  20  Generate",
            "title": "Available Datasets"
        },
        {
            "location": "/data/#creating-your-own-data",
            "text": "To create your own data, we recommend taking a look at the data generation scripts above. A typical data preprocessing pipeline looks as follows:   Generate data in parallel text format  Tokenize your data  Create fixed vocabularies for your source and target data  Learn and apply subword units to handle rare and unknown words",
            "title": "Creating your own data"
        },
        {
            "location": "/training/",
            "text": "For a concrete of how to run the training script, refer to the \nNeural Machine Translation Tutorial\n.\n\n\nConfiguring Training\n\n\nAlso see \nConfiguration\n. The configuration for input data, models, and training parameters is done via \nYAML\n. You can pass YAML strings directly to the training script, or create configuration files and pass their paths to the script. These two approaches are technically equivalent. However, large YAML strings can become difficult to manage so we recommend the latter one. For example, the following two are equivalent:\n\n\n1. Pass FLAGS directly:\n\n\npython -m bin.train \\\n  --model AttentionSeq2Seq \\\n  --model_params \"\n    embedding.dim: 256\n    encoder.class: seq2seq.encoders.BidirectionalRNNEncoder\n    encoder.params:\n      rnn_cell:\n        cell_class: GRUCell\"\n\n\n\n\n2. Define \nconfig.yml\n\n\nmodel: AttentionSeq2Seq\nmodel_params:\n  embedding.dim: 256\n  encoder.class: seq2seq.encoders.BidirectionalRNNEncoder\n  encoder.params:\n    rnn_cell:\n      cell_class: GRUCell\n\n\n\n\n... and pass FLAGS via config:\n\n\npython -m bin.train --config_paths config.yml\n\n\n\n\nMultiple configuration files are merged recursively, in the order they are passed. This means you can have separate configuration files for model hyperparameters, input data, and training options, and mix and match as needed.\n\n\nFor a concrete examples of configuration files, refer to the \nexample configurations\n and \nNeural Machine Translation Tutorial\n.\n\n\nMonitoring Training\n\n\nIn addition to looking at the output of the training script, Tensorflow write summaries and training logs into the specified \noutput_dir\n. Use \nTensorboard\n to visualize training progress.\n\n\ntensorboard --logdir=/path/to/model/dir\n\n\n\n\nDistributed Training\n\n\nDistributed Training is supported out of the box using \ntf.learn\n. Cluster Configurations can be specified using the \nTF_CONFIG\n environment variable, which is parsed by the \nRunConfig\n. Refer to the \nDistributed Tensorflow\n Guide for more information.\n\n\nTraining script Reference\n\n\nThe \ntrain.py\n script has many more options.\n\n\n\n\n\n\n\n\nArgument\n\n\nDefault\n\n\nDescription\n\n\n\n\n\n\n\n\n\n\nconfig_paths\n\n\n\"\"\n\n\nPath to a YAML configuration file defining FLAG values. Multiple files can be separated by commas. Files are merged recursively. Setting a key in these files is equivalent to setting the FLAG value with the same name.\n\n\n\n\n\n\nhooks\n\n\n\"[]\"\n\n\nYAML configuration string for the training hooks to use.\n\n\n\n\n\n\nmetrics\n\n\n\"[]\"\n\n\nYAML configuration string for the training metrics to use.\n\n\n\n\n\n\nmodel\n\n\n\"\"\n\n\nName of the model class. Can be either a fully-qualified name, or the name of a class defined in \nseq2seq.models\n.\n\n\n\n\n\n\nmodel_params\n\n\n\"{}\"\n\n\nYAML configuration string for the model parameters.\n\n\n\n\n\n\ninput_pipeline_train\n\n\n\"{}\"\n\n\nYAML configuration string for the training data input pipeline.\n\n\n\n\n\n\ninput_pipeline_dev\n\n\n\"{}\"\n\n\nYAML configuration string for the development data input pipeline.\n\n\n\n\n\n\nbuckets\n\n\nNone\n\n\nBuckets input sequences according to these length. A comma-separated list of sequence length buckets, e.g. \n\"10,20,30\"\n would result in 4 buckets: \n<10, 10-20, 20-30, >30\n. \nNone\n disables bucketing.\n\n\n\n\n\n\nbatch_size\n\n\n16\n\n\nBatch size used for training and evaluation.\n\n\n\n\n\n\noutput_dir\n\n\nNone\n\n\nThe directory to write model checkpoints and summaries to. If None, a local temporary directory is created.\n\n\n\n\n\n\ntrain_steps\n\n\nNone\n\n\nMaximum number of training steps to run. If None, train forever.\n\n\n\n\n\n\neval_every_n_steps\n\n\n1000\n\n\nRun evaluation on validation data every N steps.\n\n\n\n\n\n\ntf_random_seed\n\n\nNone\n\n\nRandom seed for TensorFlow initializers. Setting this value allows consistency between reruns.\n\n\n\n\n\n\nsave_checkpoints_secs\n\n\n600\n\n\nSave checkpoints every N seconds. Can not be specified with \nsave_checkpoints_steps\n.\n\n\n\n\n\n\nsave_checkpoints_steps\n\n\nNone\n\n\nSave checkpoints every N steps. Can not be specified with \nsave_checkpoints_secs\n.\n\n\n\n\n\n\nkeep_checkpoint_max\n\n\n5\n\n\nMaximum number of recent checkpoint files to keep. As new files are created, older files are deleted. If None or 0, all checkpoint files are kept.\n\n\n\n\n\n\nkeep_checkpoint_every_n_hours\n\n\n4\n\n\nIn addition to keeping the most recent checkpoint files, keep one checkpoint file for every N hours of training.",
            "title": "Training"
        },
        {
            "location": "/training/#configuring-training",
            "text": "Also see  Configuration . The configuration for input data, models, and training parameters is done via  YAML . You can pass YAML strings directly to the training script, or create configuration files and pass their paths to the script. These two approaches are technically equivalent. However, large YAML strings can become difficult to manage so we recommend the latter one. For example, the following two are equivalent:  1. Pass FLAGS directly:  python -m bin.train \\\n  --model AttentionSeq2Seq \\\n  --model_params \"\n    embedding.dim: 256\n    encoder.class: seq2seq.encoders.BidirectionalRNNEncoder\n    encoder.params:\n      rnn_cell:\n        cell_class: GRUCell\"  2. Define  config.yml  model: AttentionSeq2Seq\nmodel_params:\n  embedding.dim: 256\n  encoder.class: seq2seq.encoders.BidirectionalRNNEncoder\n  encoder.params:\n    rnn_cell:\n      cell_class: GRUCell  ... and pass FLAGS via config:  python -m bin.train --config_paths config.yml  Multiple configuration files are merged recursively, in the order they are passed. This means you can have separate configuration files for model hyperparameters, input data, and training options, and mix and match as needed.  For a concrete examples of configuration files, refer to the  example configurations  and  Neural Machine Translation Tutorial .",
            "title": "Configuring Training"
        },
        {
            "location": "/training/#monitoring-training",
            "text": "In addition to looking at the output of the training script, Tensorflow write summaries and training logs into the specified  output_dir . Use  Tensorboard  to visualize training progress.  tensorboard --logdir=/path/to/model/dir",
            "title": "Monitoring Training"
        },
        {
            "location": "/training/#distributed-training",
            "text": "Distributed Training is supported out of the box using  tf.learn . Cluster Configurations can be specified using the  TF_CONFIG  environment variable, which is parsed by the  RunConfig . Refer to the  Distributed Tensorflow  Guide for more information.",
            "title": "Distributed Training"
        },
        {
            "location": "/training/#training-script-reference",
            "text": "The  train.py  script has many more options.     Argument  Default  Description      config_paths  \"\"  Path to a YAML configuration file defining FLAG values. Multiple files can be separated by commas. Files are merged recursively. Setting a key in these files is equivalent to setting the FLAG value with the same name.    hooks  \"[]\"  YAML configuration string for the training hooks to use.    metrics  \"[]\"  YAML configuration string for the training metrics to use.    model  \"\"  Name of the model class. Can be either a fully-qualified name, or the name of a class defined in  seq2seq.models .    model_params  \"{}\"  YAML configuration string for the model parameters.    input_pipeline_train  \"{}\"  YAML configuration string for the training data input pipeline.    input_pipeline_dev  \"{}\"  YAML configuration string for the development data input pipeline.    buckets  None  Buckets input sequences according to these length. A comma-separated list of sequence length buckets, e.g.  \"10,20,30\"  would result in 4 buckets:  <10, 10-20, 20-30, >30 .  None  disables bucketing.    batch_size  16  Batch size used for training and evaluation.    output_dir  None  The directory to write model checkpoints and summaries to. If None, a local temporary directory is created.    train_steps  None  Maximum number of training steps to run. If None, train forever.    eval_every_n_steps  1000  Run evaluation on validation data every N steps.    tf_random_seed  None  Random seed for TensorFlow initializers. Setting this value allows consistency between reruns.    save_checkpoints_secs  600  Save checkpoints every N seconds. Can not be specified with  save_checkpoints_steps .    save_checkpoints_steps  None  Save checkpoints every N steps. Can not be specified with  save_checkpoints_secs .    keep_checkpoint_max  5  Maximum number of recent checkpoint files to keep. As new files are created, older files are deleted. If None or 0, all checkpoint files are kept.    keep_checkpoint_every_n_hours  4  In addition to keeping the most recent checkpoint files, keep one checkpoint file for every N hours of training.",
            "title": "Training script Reference"
        },
        {
            "location": "/inference/",
            "text": "Inference Tasks\n\n\nWhen calling the inference script \nbin/infer.py\n, you must provide a list of tasks to run. The most basic task, \nDecodeText\n, simply prints out the model predictions. By additing more tasks you can perform additional features, such as storing debugging infromation or visualization attention scores. Under the hood, each \nInferenceTask\n is implemented as a Tensorflow \nSessionRunHook\n that requests outputs from the model and knows how to process them.\n\n\nDecodeText\n\n\nThe \nDecodeText\n task reads the model predictions and prints the predictions to standard output. It has the following parameters:\n\n\n\n\ndelimiter\n: String to join the tokens predicted by the model on. Defaults to space.\n\n\nunk_replace\n: If set to \nTrue\n, perform unknown token replacement based on attention scores. Default is \nFalse\n. See below for more details.\n\n\nunk_mapping\n: If set to the path of a dictionary file, use the provided mapping to perform unknown token replacement. See below for more details.\n\n\n\n\nUNK token replacement using a Copy Mechanism\n\n\nRare words (such as place and people names) are often absent from the target vocabulary and result in \nUNK\n tokens in the output predictions. An easy strategy to target sequences is to replace each \nUNK\n token with the word in the source sequence it is best aligned with. Alignments are typically calculated using an attention mechanism which produces alignment scores for each target token. If you trained a model that generates such attention scores (e.g. \nAttentionSeq2Seq\n), you can use them to perform UNK token replacement by activating the \nunk_replace\n parameter.\n\n\nmkdir -p ${DATA_PATH}/pred\npython -m bin.infer \\\n  --tasks \"\n    - class: DecodeText\n      params:\n        unk_replace: True\"\n\n\n\n\nUNK token replacement using a mapping\n\n\nA more sophisticated approach to UNK token replacement is to use a mapping instead of copying words from the source. For example, the English word \"Munich\" is usually translated as \"M\u00fcnchen\" in German. Simply copying \"Munich\" from the source you would never result in the right translation even if the words were perfectly aligned using attention scores. One strategy is to use \nfast_align\n to generate a mapping based on the conditional probabilities of target given source.\n\n\n# Download and build fast_align\ngit clone https://github.com/clab/fast_align.git\nmkdir fast_align/build && cd fast_align/build\ncmake ../ && make\n\n# Convert your data into a format that fast_align understands:\n# <source> ||| <target>\npaste \\\n  $HOME/nmt_data/toy_reverse/train/sources.txt \\\n  $HOME/nmt_data/toy_reverse/train/targets.txt \\\n  | sed \"s/$(printf '\\t')/ ||| /g\" > $HOME/nmt_data/toy_reverse/train/source_targets.fastalign\n\n# Learn alignments\n./fast_align \\\n  -i $HOME/nmt_data/toy_reverse/train/source_targets.fastalign \\\n  -v -p $HOME/nmt_data/toy_reverse/train/source_targets.cond \\\n  > $HOME/nmt_data/toy_reverse/train/source_targets.align\n\n# Find the most probable traslation for each word and write them to a file\nsort -k1,1 -k3,3gr $HOME/nmt_data/toy_reverse/train/source_targets.cond \\\n  | sort -k1,1 -u \\\n  > $HOME/nmt_data/toy_reverse/train/source_targets.cond.dict\n\n\n\n\n\nThe output file specified by the \n-p\n argument will contain conditional probabilities for \np(target | source)\n in the form of \n<source>\\t<target>\\t<prob>\n. These can be used to do smarter UNK token replacement by passing the \nunk_mapping\n flag.\n\n\nmkdir -p ${DATA_PATH}/pred\npython -m bin.infer \\\n  --tasks \"\n    - class: DecodeText\n      params:\n        unk_replace: True\"\n        unk_mapping: $HOME/nmt_data/toy_reverse/train/source_targets.cond.dict\"\n  ...\n\n\n\n\nVisualizing Attention\n\n\nIf you trained a model using the  \nAttentionDecoder\n, you can dump the raw attention scores and generate alignment visualizations during inference using the \nDumpAttention\n task.\n\n\npython -m bin.infer \\\n  --tasks \"\n    - class: DecodeText\n    - class: DumpAttention\n      params:\n        output_dir: $HOME/attention\" \\\n  ...\n\n\n\n\nBy default, this script generates an \nattention_score.npy\n array file and one attention plot per example. The array file can be \nloaded used numpy\n and will contain a list of arrays with shape \n[target_length, source_length]\n. If you only want the raw attention score data without the plots you can enable the \ndump_atention_no_plot\n parameter.\n\n\nDumping Beams\n\n\nIf you are using beam search during decoding, you can use the \nDumpBeams\n task to write beam search debugging information to disk. You can later inspect the data using numpy, or use the \nprovided script\n to generate visualizations.\n\n\npython -m bin.infer \\\n  --tasks \"\n    - class: DecodeText\n    - class: DumpBeams\n      params:\n        file: ${TMPDIR:-/tmp}/wmt_16_en_de/newstest2014.pred.beams.npz\" \\\n  --model_params \"\n    inference.beam_search.beam_width: 5\" \\\n  ...",
            "title": "Inference"
        },
        {
            "location": "/inference/#inference-tasks",
            "text": "When calling the inference script  bin/infer.py , you must provide a list of tasks to run. The most basic task,  DecodeText , simply prints out the model predictions. By additing more tasks you can perform additional features, such as storing debugging infromation or visualization attention scores. Under the hood, each  InferenceTask  is implemented as a Tensorflow  SessionRunHook  that requests outputs from the model and knows how to process them.",
            "title": "Inference Tasks"
        },
        {
            "location": "/inference/#decodetext",
            "text": "The  DecodeText  task reads the model predictions and prints the predictions to standard output. It has the following parameters:   delimiter : String to join the tokens predicted by the model on. Defaults to space.  unk_replace : If set to  True , perform unknown token replacement based on attention scores. Default is  False . See below for more details.  unk_mapping : If set to the path of a dictionary file, use the provided mapping to perform unknown token replacement. See below for more details.",
            "title": "DecodeText"
        },
        {
            "location": "/inference/#unk-token-replacement-using-a-copy-mechanism",
            "text": "Rare words (such as place and people names) are often absent from the target vocabulary and result in  UNK  tokens in the output predictions. An easy strategy to target sequences is to replace each  UNK  token with the word in the source sequence it is best aligned with. Alignments are typically calculated using an attention mechanism which produces alignment scores for each target token. If you trained a model that generates such attention scores (e.g.  AttentionSeq2Seq ), you can use them to perform UNK token replacement by activating the  unk_replace  parameter.  mkdir -p ${DATA_PATH}/pred\npython -m bin.infer \\\n  --tasks \"\n    - class: DecodeText\n      params:\n        unk_replace: True\"",
            "title": "UNK token replacement using a Copy Mechanism"
        },
        {
            "location": "/inference/#unk-token-replacement-using-a-mapping",
            "text": "A more sophisticated approach to UNK token replacement is to use a mapping instead of copying words from the source. For example, the English word \"Munich\" is usually translated as \"M\u00fcnchen\" in German. Simply copying \"Munich\" from the source you would never result in the right translation even if the words were perfectly aligned using attention scores. One strategy is to use  fast_align  to generate a mapping based on the conditional probabilities of target given source.  # Download and build fast_align\ngit clone https://github.com/clab/fast_align.git\nmkdir fast_align/build && cd fast_align/build\ncmake ../ && make\n\n# Convert your data into a format that fast_align understands:\n# <source> ||| <target>\npaste \\\n  $HOME/nmt_data/toy_reverse/train/sources.txt \\\n  $HOME/nmt_data/toy_reverse/train/targets.txt \\\n  | sed \"s/$(printf '\\t')/ ||| /g\" > $HOME/nmt_data/toy_reverse/train/source_targets.fastalign\n\n# Learn alignments\n./fast_align \\\n  -i $HOME/nmt_data/toy_reverse/train/source_targets.fastalign \\\n  -v -p $HOME/nmt_data/toy_reverse/train/source_targets.cond \\\n  > $HOME/nmt_data/toy_reverse/train/source_targets.align\n\n# Find the most probable traslation for each word and write them to a file\nsort -k1,1 -k3,3gr $HOME/nmt_data/toy_reverse/train/source_targets.cond \\\n  | sort -k1,1 -u \\\n  > $HOME/nmt_data/toy_reverse/train/source_targets.cond.dict  The output file specified by the  -p  argument will contain conditional probabilities for  p(target | source)  in the form of  <source>\\t<target>\\t<prob> . These can be used to do smarter UNK token replacement by passing the  unk_mapping  flag.  mkdir -p ${DATA_PATH}/pred\npython -m bin.infer \\\n  --tasks \"\n    - class: DecodeText\n      params:\n        unk_replace: True\"\n        unk_mapping: $HOME/nmt_data/toy_reverse/train/source_targets.cond.dict\"\n  ...",
            "title": "UNK token replacement using a mapping"
        },
        {
            "location": "/inference/#visualizing-attention",
            "text": "If you trained a model using the   AttentionDecoder , you can dump the raw attention scores and generate alignment visualizations during inference using the  DumpAttention  task.  python -m bin.infer \\\n  --tasks \"\n    - class: DecodeText\n    - class: DumpAttention\n      params:\n        output_dir: $HOME/attention\" \\\n  ...  By default, this script generates an  attention_score.npy  array file and one attention plot per example. The array file can be  loaded used numpy  and will contain a list of arrays with shape  [target_length, source_length] . If you only want the raw attention score data without the plots you can enable the  dump_atention_no_plot  parameter.",
            "title": "Visualizing Attention"
        },
        {
            "location": "/inference/#dumping-beams",
            "text": "If you are using beam search during decoding, you can use the  DumpBeams  task to write beam search debugging information to disk. You can later inspect the data using numpy, or use the  provided script  to generate visualizations.  python -m bin.infer \\\n  --tasks \"\n    - class: DecodeText\n    - class: DumpBeams\n      params:\n        file: ${TMPDIR:-/tmp}/wmt_16_en_de/newstest2014.pred.beams.npz\" \\\n  --model_params \"\n    inference.beam_search.beam_width: 5\" \\\n  ...",
            "title": "Dumping Beams"
        },
        {
            "location": "/tools/",
            "text": "Generating Vocabulary\n\n\nA vocabulary file is a raw text file that contains one word per line, followed by a tab separator and the word count. The total number of lines is equal to the size of the vocabulary and each token is mapped to its line number. We provide a helper script \nbin/tools/generate_vocab.py\n that takes in a raw text file of space-delimited tokens and generates a vocabulary file:\n\n\n./bin/tools/generate_vocab.py < data.txt > vocab\n\n\n\n\nGenerating Character Vocabulary\n\n\nSometimes you want to run training on characters instead of words or subword units. The \nbin/tools/generate_char_vocab.py\n can generate a vocabulary file that contains the unique set of characters found in the text:\n\n\n./bin/tools/generate_char_vocab.py < data.txt > vocab\n\n\n\n\nTo run training on characters you must pass the \n--delimiter=\"\"\n flag to the training script to avoid splitting words on spaces. See the \nTraining documentation\n for more details.\n\n\nVisualizing Beam Search\n\n\nIf you use the \nDumpBeams\n inference task (see \nInference\n for more details) you can inspect the beam search data by loading the array using numpy, or generate beam search visualizations using the \ngenerate_beam_viz.py\n script. This required the \nnetworkx\n module to be installed.\n\n\npython -m bin.tools.generate_beam_viz  \\\n  -o ${TMPDIR:-/tmp}/beam_visualizations \\\n  -d ${TMPDIR:-/tmp}/beams.npz \\\n  -v $HOME/nmt_data/toy_reverse//train/vocab.targets.txt",
            "title": "Tools"
        },
        {
            "location": "/tools/#generating-vocabulary",
            "text": "A vocabulary file is a raw text file that contains one word per line, followed by a tab separator and the word count. The total number of lines is equal to the size of the vocabulary and each token is mapped to its line number. We provide a helper script  bin/tools/generate_vocab.py  that takes in a raw text file of space-delimited tokens and generates a vocabulary file:  ./bin/tools/generate_vocab.py < data.txt > vocab",
            "title": "Generating Vocabulary"
        },
        {
            "location": "/tools/#generating-character-vocabulary",
            "text": "Sometimes you want to run training on characters instead of words or subword units. The  bin/tools/generate_char_vocab.py  can generate a vocabulary file that contains the unique set of characters found in the text:  ./bin/tools/generate_char_vocab.py < data.txt > vocab  To run training on characters you must pass the  --delimiter=\"\"  flag to the training script to avoid splitting words on spaces. See the  Training documentation  for more details.",
            "title": "Generating Character Vocabulary"
        },
        {
            "location": "/tools/#visualizing-beam-search",
            "text": "If you use the  DumpBeams  inference task (see  Inference  for more details) you can inspect the beam search data by loading the array using numpy, or generate beam search visualizations using the  generate_beam_viz.py  script. This required the  networkx  module to be installed.  python -m bin.tools.generate_beam_viz  \\\n  -o ${TMPDIR:-/tmp}/beam_visualizations \\\n  -d ${TMPDIR:-/tmp}/beams.npz \\\n  -v $HOME/nmt_data/toy_reverse//train/vocab.targets.txt",
            "title": "Visualizing Beam Search"
        },
        {
            "location": "/results/",
            "text": "Machine Translation: WMT'15 English-German\n\n\nSingle models only, no ensembles. Results are listed in chronological order.\n\n\n\n\n\n\n\n\nModel Name & Reference\n\n\nSettings / Notes\n\n\nTraining Time\n\n\nTest Set BLEU\n\n\n\n\n\n\n\n\n\n\ntf-seq2seq\n\n\nConfiguration\n\n\n~4 days on 8 NVidia K80 GPUs\n\n\nnewstest2014: \n22.19\n \n newstest2015: \n25.23\n\n\n\n\n\n\nGehring, et al. (2016-11)\n \n Deep Convolutional 15/5\n\n\n\n\n---\n\n\nnewstest2014: - \n newstest2015: \n24.3\n\n\n\n\n\n\nWu et al. (2016-09)\n \n GNMT\n\n\n8 encoder/decoder layers, 1024 LSTM units, 32k shared wordpieces (similar to BPE); residual between layers connections; lots of other tricks; newstest2012 and newstest2013 as validation sets.\n\n\n---\n\n\nnewstest2014:\u00a0\n24.61\n \nnewstest2015: -\n\n\n\n\n\n\nZhou et al. (2016-06)\n \n Deep-Att\n\n\n\n\n---\n\n\nnewstest2014: \n20.6\n \n newstest2015: -\n\n\n\n\n\n\nChung, et al. (2016-03)\n \n BPE-Char\n\n\nCharacter-level decoder with BPE encoder.\n Based on Bahdanau attention model; Bidirectional encoder with 512 GRU units; 2-layer GRU decoder with 1024 units; Adam; batch size 128; gradient clipping at norm 1; Moses Tokenizer; limit sequences to 50 symbols in source and 100 symbols and 500 characters in target.\n\n\n---\n\n\nnewstest2014: \n21.5\n \n newstest2015: \n23.9\n\n\n\n\n\n\nSennrich et al. (2015-8)\n \n BPE\n\n\nAuthors propose BPE for subword unit nsegmentation as a pre/post-processing step to handle open vocabulary\n;  Base model is based on \nBahndanau's paper\n. Bidirectional encoder; GRU; 1000 hidden units; 1000 attention units; 620-dimensional word embeddings; single-layer; beam search width 12; Adadelta with batch size 80; Using \nGroundhog\n;\n\n\n\n\nnewstest2014: - \nnewstest2015: \n20.5\n\n\n\n\n\n\nLuong et al. (2015-08)\n\n\nNovel local/global attention mechanism;\n 50k vocabulary; 4 layers in encoder and decoder; unidirectional encoder; gradient clipping at norm 5;  1028 LSTM units, 1028-dimensional embeddings; (somewhat complicated) SGD decay schedule; dropout 0.2; UNK replace;\n\n\n---\n\n\nnewstest2014: \n20.9\n \n newstest2015: -\n\n\n\n\n\n\nJean et al. (2014-12)\n \n RNNsearch-LV\n\n\nAuthors propose a new sampling-based approach to incorporate a larger vocabulary\n; Base model is based on \nBahndanau's paper\n. Bidirectional encoder; GRU; 1000 hidden units; 1000 attention units; 620-dimensional word embeddings; single-layer; beam search width 12;\n\n\n---\n\n\nnewstest2014: \n19.4\n \n newstest2015: -\n\n\n\n\n\n\n\n\nMachine Translation: WMT'17\n\n\nComing soon.\n\n\nText Summarization: Gigaword\n\n\nComing soon.\n\n\nImage Captioning: MSCOCO\n\n\nComing soon.\n\n\nConversational Modeling\n\n\nComing soon.",
            "title": "Results"
        },
        {
            "location": "/results/#machine-translation-wmt15-english-german",
            "text": "Single models only, no ensembles. Results are listed in chronological order.     Model Name & Reference  Settings / Notes  Training Time  Test Set BLEU      tf-seq2seq  Configuration  ~4 days on 8 NVidia K80 GPUs  newstest2014:  22.19    newstest2015:  25.23    Gehring, et al. (2016-11)    Deep Convolutional 15/5   ---  newstest2014: -   newstest2015:  24.3    Wu et al. (2016-09)    GNMT  8 encoder/decoder layers, 1024 LSTM units, 32k shared wordpieces (similar to BPE); residual between layers connections; lots of other tricks; newstest2012 and newstest2013 as validation sets.  ---  newstest2014:\u00a0 24.61   newstest2015: -    Zhou et al. (2016-06)    Deep-Att   ---  newstest2014:  20.6    newstest2015: -    Chung, et al. (2016-03)    BPE-Char  Character-level decoder with BPE encoder.  Based on Bahdanau attention model; Bidirectional encoder with 512 GRU units; 2-layer GRU decoder with 1024 units; Adam; batch size 128; gradient clipping at norm 1; Moses Tokenizer; limit sequences to 50 symbols in source and 100 symbols and 500 characters in target.  ---  newstest2014:  21.5    newstest2015:  23.9    Sennrich et al. (2015-8)    BPE  Authors propose BPE for subword unit nsegmentation as a pre/post-processing step to handle open vocabulary ;  Base model is based on  Bahndanau's paper . Bidirectional encoder; GRU; 1000 hidden units; 1000 attention units; 620-dimensional word embeddings; single-layer; beam search width 12; Adadelta with batch size 80; Using  Groundhog ;   newstest2014: -  newstest2015:  20.5    Luong et al. (2015-08)  Novel local/global attention mechanism;  50k vocabulary; 4 layers in encoder and decoder; unidirectional encoder; gradient clipping at norm 5;  1028 LSTM units, 1028-dimensional embeddings; (somewhat complicated) SGD decay schedule; dropout 0.2; UNK replace;  ---  newstest2014:  20.9    newstest2015: -    Jean et al. (2014-12)    RNNsearch-LV  Authors propose a new sampling-based approach to incorporate a larger vocabulary ; Base model is based on  Bahndanau's paper . Bidirectional encoder; GRU; 1000 hidden units; 1000 attention units; 620-dimensional word embeddings; single-layer; beam search width 12;  ---  newstest2014:  19.4    newstest2015: -",
            "title": "Machine Translation: WMT'15 English-German"
        },
        {
            "location": "/results/#machine-translation-wmt17",
            "text": "Coming soon.",
            "title": "Machine Translation: WMT'17"
        },
        {
            "location": "/results/#text-summarization-gigaword",
            "text": "Coming soon.",
            "title": "Text Summarization: Gigaword"
        },
        {
            "location": "/results/#image-captioning-mscoco",
            "text": "Coming soon.",
            "title": "Image Captioning: MSCOCO"
        },
        {
            "location": "/results/#conversational-modeling",
            "text": "Coming soon.",
            "title": "Conversational Modeling"
        },
        {
            "location": "/help/",
            "text": "Getting Help\n\n\nIf you run into problems or find bugs in the code, please file a \nGithub Issue\n.",
            "title": "Getting Help"
        },
        {
            "location": "/help/#getting-help",
            "text": "If you run into problems or find bugs in the code, please file a  Github Issue .",
            "title": "Getting Help"
        },
        {
            "location": "/contributing/",
            "text": "What to work on\n\n\nWe are always looking for contributors. If you are interested in contributing but are not sure to what work on, take a look at the open \nGithub Issues\n that are unassigned. Those with the \nhelp wanted\n label are especially good candidates. If you are working on a larger task and unsure how to approach it, just leave a comment to get feedback on design decisions. We are also always looking for the following:\n\n\n\n\nFix issues with the documentation (typos, outdated docs, ...)\n\n\nImprove code quality through refactoring, more tests, better docstrings, etc.\n\n\nImplement standard benchmark model found in the literature\n\n\nRunning benchmarks on standard datasets\n\n\n\n\nDevelopment Setup\n\n\nWe recommend using Python 3. If you're on a Mac the easiest way to do this is probably using \nHomebrew\n. Then,\n\n\n# Clone this repository.\ngit clone https://github.com/google/seq2seq.git\ncd seq2seq\n\n# Create a new virtual environment and activate it.\npython3 -m venv ~/tf-venv\nsource ~/tf-venv/bin/activate\n\n# Install package dependencies and utilities.\npip install -e .\npip install nose pylint tox yapf mkdocs\n\n# Make sure the tests are passing.\nnosetests\n\n# Code :)\n\n# Make sure the tests are passing\nnosetests\n\n# Before submitting a pull request,\n# run the full test suite for Python 3 and Python 2.7\ntox\n\n\n\n\nPython Style\n\n\nWe use \npylint\n to enforce coding style. Before submitting a pull request, make\nsure you run:\n\n\npylint seq2seq\n\n\n\n\nCircleCI integration tests will fail if pylint reports any critica errors, preventing use from merging your changes. If you are unsure about code formatting, you can use \nyapf\n for automated code formatting:\n\n\nyapf -ir ./seq2seq/some/file/you/changed\n\n\n\n\nRecommended Tensorflow Style\n\n\nGraphModule\n\n\nAll classes that modify the Graph should inherit from \nseq2seq.graph_module.GraphModule\n, which is a wrapper around TensorFlow's \ntf.make_template\n function that enables easy variable sharing, allowing you to do something like this:\n\n\nencode_fn = SomeEncoderModule(...)\n\n# New variables are created in this call.\noutput1 = encode_fn(input1)\n\n# No new variables are created here. The variables from the above call are re-used.\n# Note how this is different from normal TensorFlow where you would need to use variable scopes.\noutput2 = encode_fn(input2)\n\n# Because this is a new instance a second set of variables is created.\nencode_fn2 = SomeEncoderModule(...)\noutput3 = encode_fn2(input3)\n\n\n\n\nFunctions vs. Classes\n\n\n\n\nOperations that \ncreate new variables\n must be implemented as classes and must inherit from \nGraphModule\n.\n\n\nOperations that \ndo not create new variables\n can be implemented as standard python functions, or as classes that inherit from \nGraphModule\n if they have a lot of logic.",
            "title": "Contributing"
        },
        {
            "location": "/contributing/#what-to-work-on",
            "text": "We are always looking for contributors. If you are interested in contributing but are not sure to what work on, take a look at the open  Github Issues  that are unassigned. Those with the  help wanted  label are especially good candidates. If you are working on a larger task and unsure how to approach it, just leave a comment to get feedback on design decisions. We are also always looking for the following:   Fix issues with the documentation (typos, outdated docs, ...)  Improve code quality through refactoring, more tests, better docstrings, etc.  Implement standard benchmark model found in the literature  Running benchmarks on standard datasets",
            "title": "What to work on"
        },
        {
            "location": "/contributing/#development-setup",
            "text": "We recommend using Python 3. If you're on a Mac the easiest way to do this is probably using  Homebrew . Then,  # Clone this repository.\ngit clone https://github.com/google/seq2seq.git\ncd seq2seq\n\n# Create a new virtual environment and activate it.\npython3 -m venv ~/tf-venv\nsource ~/tf-venv/bin/activate\n\n# Install package dependencies and utilities.\npip install -e .\npip install nose pylint tox yapf mkdocs\n\n# Make sure the tests are passing.\nnosetests\n\n# Code :)\n\n# Make sure the tests are passing\nnosetests\n\n# Before submitting a pull request,\n# run the full test suite for Python 3 and Python 2.7\ntox",
            "title": "Development Setup"
        },
        {
            "location": "/contributing/#python-style",
            "text": "We use  pylint  to enforce coding style. Before submitting a pull request, make\nsure you run:  pylint seq2seq  CircleCI integration tests will fail if pylint reports any critica errors, preventing use from merging your changes. If you are unsure about code formatting, you can use  yapf  for automated code formatting:  yapf -ir ./seq2seq/some/file/you/changed",
            "title": "Python Style"
        },
        {
            "location": "/contributing/#recommended-tensorflow-style",
            "text": "",
            "title": "Recommended Tensorflow Style"
        },
        {
            "location": "/contributing/#graphmodule",
            "text": "All classes that modify the Graph should inherit from  seq2seq.graph_module.GraphModule , which is a wrapper around TensorFlow's  tf.make_template  function that enables easy variable sharing, allowing you to do something like this:  encode_fn = SomeEncoderModule(...)\n\n# New variables are created in this call.\noutput1 = encode_fn(input1)\n\n# No new variables are created here. The variables from the above call are re-used.\n# Note how this is different from normal TensorFlow where you would need to use variable scopes.\noutput2 = encode_fn(input2)\n\n# Because this is a new instance a second set of variables is created.\nencode_fn2 = SomeEncoderModule(...)\noutput3 = encode_fn2(input3)",
            "title": "GraphModule"
        },
        {
            "location": "/contributing/#functions-vs-classes",
            "text": "Operations that  create new variables  must be implemented as classes and must inherit from  GraphModule .  Operations that  do not create new variables  can be implemented as standard python functions, or as classes that inherit from  GraphModule  if they have a lot of logic.",
            "title": "Functions vs. Classes"
        },
        {
            "location": "/license/",
            "text": "License\n\n\nThe code is available under the \nApache License\n.\n\n\nCitation\n\n\nIf you use this code for academic purposes, plase cite it as:\n\n\n@ARTICLE{Britz:2017,\n  author          = {{Britz}, D. and {Goldie}, A. and {Luong}, T. and {Le}, Q.},\n  title           = \"{Massive Exploration of Neural Machine Translation Architectures}\",\n  journal         = {ArXiv e-prints},\n  archivePrefix   = \"arXiv\",\n  eprinttype      = {arxiv},\n  eprint          = {1703.03906},\n  primaryClass    = \"cs.CL\",\n  keywords        = {Computer Science - Computation and Language},\n  year            = 2017,\n  month           = mar,\n}",
            "title": "License"
        },
        {
            "location": "/license/#license",
            "text": "The code is available under the  Apache License .",
            "title": "License"
        },
        {
            "location": "/license/#citation",
            "text": "If you use this code for academic purposes, plase cite it as:  @ARTICLE{Britz:2017,\n  author          = {{Britz}, D. and {Goldie}, A. and {Luong}, T. and {Le}, Q.},\n  title           = \"{Massive Exploration of Neural Machine Translation Architectures}\",\n  journal         = {ArXiv e-prints},\n  archivePrefix   = \"arXiv\",\n  eprinttype      = {arxiv},\n  eprint          = {1703.03906},\n  primaryClass    = \"cs.CL\",\n  keywords        = {Computer Science - Computation and Language},\n  year            = 2017,\n  month           = mar,\n}",
            "title": "Citation"
        },
        {
            "location": "/models/",
            "text": "When calling the training script, you can specify a model class using the \n--model\n flag and model-specific hyperparameters using the \n--model_params\n flag. This page lists all supported models and hyperparameters.\n\n\nModelBase\n\n\n\n\nThis is an abstract class that cannot be used as a model during training. Other model classes inherit from this. The following parameters are shared by all models, unless explicitly stated otherwise in the model section.\n\n\n\n\n\n\n\n\nName\n\n\nDefault\n\n\nDescription\n\n\n\n\n\n\n\n\n\n\noptimizer.name\n\n\nAdam\n\n\nType of Optimizer to use, e.g. \nAdam\n, \nSGD\n or \nMomentum\n. The name is fed to TensorFlow's \noptimize_loss\n function. See TensorFlow documentation for more details and all available options.\n\n\n\n\n\n\noptimizer.learning_rate\n\n\n1e-4\n\n\nInitial learning rate for the optimizer. This is fed to TensorFlow's \noptimize_loss\n function.\n\n\n\n\n\n\noptimizer.lr_decay_type\n\n\n\n\nThe name of one of TensorFlow's \nlearning rate decay functions\n defined in \ntf.train\n, e.g. \nexponential_decay\n. If this is an empty string (default) then no learning rate decay is used.\n\n\n\n\n\n\noptimizer.lr_decay_steps\n\n\n100\n\n\nHow often to apply decay. This is fed as the \ndecay_steps\n argument to the decay function defined above. See Tensoflow documentation for more details.\n\n\n\n\n\n\noptimizer.lr_decay_rate\n\n\n0.99\n\n\nThe decay rate. This is fed as the \ndecay_rate\n argument to the decay function defined above. See TensorFlow documentation for more details.\n\n\n\n\n\n\noptimizer.lr_start_decay_at\n\n\n0\n\n\nStart learning rate decay at this step.\n\n\n\n\n\n\noptimizer.lr_stop_decay_at\n\n\n1e9\n\n\nStop learning rate decay at this step.\n\n\n\n\n\n\noptimizer.lr_min_learning_rate\n\n\n1e-12\n\n\nNever decay below this learning rate.\n\n\n\n\n\n\noptimizer.lr_staircase\n\n\nFalse\n\n\nIf \nTrue\n, decay the learning rate at discrete intervals. This is fed as the \nstaircase\n argument to the decay function defined above. See TensorFlow documentation for more details.\n\n\n\n\n\n\noptimizer.clip_gradients\n\n\n5.0\n\n\nClip gradients by their global norm.\n\n\n\n\n\n\n\n\nSeq2SeqModel\n\n\n\n\nThis is an abstract class that cannot be used as a model during training. Other model classes inherit from this. The following hyperparameters are shared by all models that inherit from \nSeq2SeqModel\n, unless explicitly stated otherwise.\n\n\n\n\n\n\n\n\nName\n\n\nDefault\n\n\nDescription\n\n\n\n\n\n\n\n\n\n\nsource.max_seq_len\n\n\n50\n\n\nMaximum length of source sequences. An example is sliced to this length before being fed to the encoder.\n\n\n\n\n\n\nsource.reverse\n\n\nTrue\n\n\nIf set to true, reverse the source sequence before feeding it into the encoder.\n\n\n\n\n\n\ntarget.max_seq_len\n\n\n50\n\n\nMaximum length of target sequences. An example is sliced to this length before being fed to the decoder.\n\n\n\n\n\n\nembedding.dim\n\n\n100\n\n\nDimensionality of the embedding layer.\n\n\n\n\n\n\nembedding.share\n\n\nFalse\n\n\nIf set to true, share embedding parameters for source and target sequences.\n\n\n\n\n\n\ninference.beam_search.beam_width\n\n\n0\n\n\nBeam Search beam width used during inference. A value of less or equal than \n1\n disables beam search.\n\n\n\n\n\n\ninference.max_decode_length\n\n\n100\n\n\nDuring inference mode, decode up to this length or until a \nSEQUENCE_END\n token is encountered, whichever happens first.\n\n\n\n\n\n\ninference.beam_search.length_penalty_weight\n\n\n0.0\n\n\nLength penalty factor applied to beam search hypotheses, as described in \nhttps://arxiv.org/abs/1609.08144\n.\n\n\n\n\n\n\nvocab_source\n\n\n\"\"\n\n\nPath to the source vocabulary to use. This is used to map input tokens to integer IDs.\n\n\n\n\n\n\nvocab_target\n\n\n\"\"\n\n\nPath to the target vocabulary to use. This is used to map input tokens to integer IDs.\n\n\n\n\n\n\n\n\nBasicSeq2Seq\n\n\n\n\nIncludes all parameters from \nSeq2SeqModel\n. The \nBasicSeq2Seq\n model uses an encoder and decoder with no attention mechanism. The last encoder state is passed through a fully connected layer and used to initialize the decoder (this behavior can be changed using the \nbridge.*\n hyperparameter). This is the \"vanilla\" implementation of the standard seq2seq architecture.\n\n\n\n\n\n\n\n\nName\n\n\nDefault\n\n\nDescription\n\n\n\n\n\n\n\n\n\n\nbridge.class\n\n\nseq2seq.models.bridges.InitialStateBridge\n\n\nType of bridge to use. The bridge defines how state is passed between the encoder and decoder. Refer to the \nseq2seq.models.bridges\n module for more details.\n\n\n\n\n\n\nbridge.params\n\n\n{}\n\n\nParameters passed to the bridge during construction.\n\n\n\n\n\n\nencoder.class\n\n\nseq2seq.encoders.UnidirectionalRNNEncoder\n\n\nType of encoder to use. See the \nEncoder Reference\n for more details and available encoders.\n\n\n\n\n\n\nencoder.params\n\n\n{}\n\n\nParameters passed to the encoder during construction. See the \nEncoder Reference\n for more details.\n\n\n\n\n\n\ndecoder.class\n\n\nseq2seq.decoders.BasicDecoder\n\n\nType of decoder to use. See the \nDecoder Reference\n for more details and available encoders.\n\n\n\n\n\n\ndecoder.params\n\n\n{}\n\n\nParameters passed to the decoder during construction. See the \nDecoder Reference\n for more details.\n\n\n\n\n\n\n\n\nAttentionSeq2Seq\n\n\n\n\nIncludes all parameters from \nSeq2SeqModel\n and \nBasicSeq2Seq\n. This model is similar to \nBasicSeq2Seq\n, except that it uses an attention mechanism during decoding. By default, the last encoder state is not fed to the decoder.  The implementation is comparable to the model in \nNeural Machine Translation by Jointly Learning to Align and Translate\n.\n\n\n\n\n\n\n\n\nName\n\n\nDefault\n\n\nDescription\n\n\n\n\n\n\n\n\n\n\nattention.class\n\n\nAttentionLayerBahdanau\n\n\nClass name of the attention layer. Can be a fully-qualified name or is assumed to be defined in \nseq2seq.decoders.attention\n. Currently available layers are \nAttentionLayerBahdanau\n and \nAttentionLayerDot\n.\n\n\n\n\n\n\nattention.params\n\n\n{\"num_units\": 128}\n\n\nA dictionary of  parameters passed to the attention class constructor.\n\n\n\n\n\n\nbridge.class\n\n\nseq2seq.models.bridges.ZeroBridge\n\n\nType of bridge to use. The bridge defines how state is passed between the encoder and decoder. Refer to the \nseq2seq.models.bridges\n module for more details.\n\n\n\n\n\n\nencoder.class\n\n\nseq2seq.encoders.BidirectionalRNNEncoder\n\n\nType of encoder to use. See the \nEncoder Reference\n for more details and available encoders.\n\n\n\n\n\n\ndecoder.class\n\n\nseq2seq.decoders.AttentionDecoder\n\n\nType of decoder to use. See the \nDecoder Reference\n for more details and available encoders.\n\n\n\n\n\n\n\n\nImage2Seq\n\n\n\n\nThis model is currently experimental.\n This model uses the same parameters as \nAttentionSeq2Seq\n with the following differences:\n\n\n\n\nThe default encoder is \nseq2seq.encoders.InceptionV3Encoder\n\n\nThere are not \nsource.max_seq_len\n and \nsource.reverse\n, and \nvocab_source\n parameters.",
            "title": "Reference: Models"
        },
        {
            "location": "/models/#modelbase",
            "text": "This is an abstract class that cannot be used as a model during training. Other model classes inherit from this. The following parameters are shared by all models, unless explicitly stated otherwise in the model section.     Name  Default  Description      optimizer.name  Adam  Type of Optimizer to use, e.g.  Adam ,  SGD  or  Momentum . The name is fed to TensorFlow's  optimize_loss  function. See TensorFlow documentation for more details and all available options.    optimizer.learning_rate  1e-4  Initial learning rate for the optimizer. This is fed to TensorFlow's  optimize_loss  function.    optimizer.lr_decay_type   The name of one of TensorFlow's  learning rate decay functions  defined in  tf.train , e.g.  exponential_decay . If this is an empty string (default) then no learning rate decay is used.    optimizer.lr_decay_steps  100  How often to apply decay. This is fed as the  decay_steps  argument to the decay function defined above. See Tensoflow documentation for more details.    optimizer.lr_decay_rate  0.99  The decay rate. This is fed as the  decay_rate  argument to the decay function defined above. See TensorFlow documentation for more details.    optimizer.lr_start_decay_at  0  Start learning rate decay at this step.    optimizer.lr_stop_decay_at  1e9  Stop learning rate decay at this step.    optimizer.lr_min_learning_rate  1e-12  Never decay below this learning rate.    optimizer.lr_staircase  False  If  True , decay the learning rate at discrete intervals. This is fed as the  staircase  argument to the decay function defined above. See TensorFlow documentation for more details.    optimizer.clip_gradients  5.0  Clip gradients by their global norm.",
            "title": "ModelBase"
        },
        {
            "location": "/models/#seq2seqmodel",
            "text": "This is an abstract class that cannot be used as a model during training. Other model classes inherit from this. The following hyperparameters are shared by all models that inherit from  Seq2SeqModel , unless explicitly stated otherwise.     Name  Default  Description      source.max_seq_len  50  Maximum length of source sequences. An example is sliced to this length before being fed to the encoder.    source.reverse  True  If set to true, reverse the source sequence before feeding it into the encoder.    target.max_seq_len  50  Maximum length of target sequences. An example is sliced to this length before being fed to the decoder.    embedding.dim  100  Dimensionality of the embedding layer.    embedding.share  False  If set to true, share embedding parameters for source and target sequences.    inference.beam_search.beam_width  0  Beam Search beam width used during inference. A value of less or equal than  1  disables beam search.    inference.max_decode_length  100  During inference mode, decode up to this length or until a  SEQUENCE_END  token is encountered, whichever happens first.    inference.beam_search.length_penalty_weight  0.0  Length penalty factor applied to beam search hypotheses, as described in  https://arxiv.org/abs/1609.08144 .    vocab_source  \"\"  Path to the source vocabulary to use. This is used to map input tokens to integer IDs.    vocab_target  \"\"  Path to the target vocabulary to use. This is used to map input tokens to integer IDs.",
            "title": "Seq2SeqModel"
        },
        {
            "location": "/models/#basicseq2seq",
            "text": "Includes all parameters from  Seq2SeqModel . The  BasicSeq2Seq  model uses an encoder and decoder with no attention mechanism. The last encoder state is passed through a fully connected layer and used to initialize the decoder (this behavior can be changed using the  bridge.*  hyperparameter). This is the \"vanilla\" implementation of the standard seq2seq architecture.     Name  Default  Description      bridge.class  seq2seq.models.bridges.InitialStateBridge  Type of bridge to use. The bridge defines how state is passed between the encoder and decoder. Refer to the  seq2seq.models.bridges  module for more details.    bridge.params  {}  Parameters passed to the bridge during construction.    encoder.class  seq2seq.encoders.UnidirectionalRNNEncoder  Type of encoder to use. See the  Encoder Reference  for more details and available encoders.    encoder.params  {}  Parameters passed to the encoder during construction. See the  Encoder Reference  for more details.    decoder.class  seq2seq.decoders.BasicDecoder  Type of decoder to use. See the  Decoder Reference  for more details and available encoders.    decoder.params  {}  Parameters passed to the decoder during construction. See the  Decoder Reference  for more details.",
            "title": "BasicSeq2Seq"
        },
        {
            "location": "/models/#attentionseq2seq",
            "text": "Includes all parameters from  Seq2SeqModel  and  BasicSeq2Seq . This model is similar to  BasicSeq2Seq , except that it uses an attention mechanism during decoding. By default, the last encoder state is not fed to the decoder.  The implementation is comparable to the model in  Neural Machine Translation by Jointly Learning to Align and Translate .     Name  Default  Description      attention.class  AttentionLayerBahdanau  Class name of the attention layer. Can be a fully-qualified name or is assumed to be defined in  seq2seq.decoders.attention . Currently available layers are  AttentionLayerBahdanau  and  AttentionLayerDot .    attention.params  {\"num_units\": 128}  A dictionary of  parameters passed to the attention class constructor.    bridge.class  seq2seq.models.bridges.ZeroBridge  Type of bridge to use. The bridge defines how state is passed between the encoder and decoder. Refer to the  seq2seq.models.bridges  module for more details.    encoder.class  seq2seq.encoders.BidirectionalRNNEncoder  Type of encoder to use. See the  Encoder Reference  for more details and available encoders.    decoder.class  seq2seq.decoders.AttentionDecoder  Type of decoder to use. See the  Decoder Reference  for more details and available encoders.",
            "title": "AttentionSeq2Seq"
        },
        {
            "location": "/models/#image2seq",
            "text": "This model is currently experimental.  This model uses the same parameters as  AttentionSeq2Seq  with the following differences:   The default encoder is  seq2seq.encoders.InceptionV3Encoder  There are not  source.max_seq_len  and  source.reverse , and  vocab_source  parameters.",
            "title": "Image2Seq"
        },
        {
            "location": "/encoders/",
            "text": "Encoder Reference\n\n\nAll encoders inherit from the abstract \nEncoder\n defined in \nseq2seq.encoders.encoder\n and receive \nparams\n, \nmode\n arguments at instantiation time. Available hyperparameters vary by encoder class.\n\n\nUnidirectionalRNNEncoder\n\n\n\n\n\n\n\n\n\n\nName\n\n\nDefault\n\n\nDescription\n\n\n\n\n\n\n\n\n\n\nrnn_cell.cell_class\n\n\nBasicLSTMCell\n\n\nThe class of the rnn cell. Cell classes can be fully defined (e.g. \ntensorflow.contrib.rnn.BasicRNNCell\n) or must be in \ntf.contrib.rnn\n or \nseq2seq.contrib.rnn_cell\n.\n\n\n\n\n\n\nrnn_cell.cell_params\n\n\n{\"num_units\": 128}\n\n\nA dictionary of parameters to pass to the cell class constructor.\n\n\n\n\n\n\nrnn_cell.dropout_input_keep_prob\n\n\n1.0\n\n\nApply dropout to the (non-recurrent) inputs of each RNN layer using this keep probability. A value of \n1.0\n disables dropout.\n\n\n\n\n\n\nrnn_cell.dropout_output_keep_prob\n\n\n1.0\n\n\nApply dropout to the (non-recurrent) outputs of each RNN layer using this keep probability. A value of \n1.0\n disables dropout.\n\n\n\n\n\n\nrnn_cell.num_layers\n\n\n1\n\n\nNumber of RNN layers.\n\n\n\n\n\n\nrnn_cell.residual_connections\n\n\nFalse\n\n\nIf true, add residual connections between RNN layers in the encoder.\n\n\n\n\n\n\n\n\nBidirectionalRNNEncoder\n\n\n\n\nSame as the \nUnidirectionalRNNEncoder\n. The same cell is used for forward and backward RNNs.\n\n\nStackBidirectionalRNNEncoder\n\n\n\n\nSame as the \nUnidirectionalRNNEncoder\n. The same cell is used for forward and backward RNNs.\n\n\nPoolingEncoder\n\n\n\n\nAn encoder that pools over embeddings, as described in \nhttps://arxiv.org/abs/1611.02344\n. The encoder supports optional positions embeddings and a configurable pooling window.\n\n\n\n\n\n\n\n\nName\n\n\nDefault\n\n\nDescription\n\n\n\n\n\n\n\n\n\n\npooling_fn\n\n\ntensorflow.layers.average_pooling1d\n\n\nThe 1-d pooling function to use, e.g. \ntensorflow.layers.average_pooling1d\n.\n\n\n\n\n\n\npool_size\n\n\n5\n\n\nThe pooling window, passed as \npool_size\n to the pooling function.\n\n\n\n\n\n\nstrides\n\n\n1\n\n\nThe stride during pooling, passed as \nstrides\n the pooling function.\n\n\n\n\n\n\nposition_embeddings.enable\n\n\nTrue\n\n\nIf true, add position embeddings to the inputs before pooling.\n\n\n\n\n\n\nposition_embeddings.combiner_fn\n\n\ntensorflow.add\n\n\nFunction used to combine the position embeddings with the inputs. For example, \ntensorflow.add\n.\n\n\n\n\n\n\nposition_embeddings.num_positions\n\n\n100\n\n\nSize of the position embedding matrix. This should be set to the maximum sequence length of the inputs.\n\n\n\n\n\n\n\n\nInceptionV3Encoder\n\n\n\n\nThis encoder is experimental\n. This encoder puts the image through an InceptionV3 network and uses the last\nhidden layer before the logits as the feature representation.\n\n\n\n\n\n\n\n\nName\n\n\nDefault\n\n\nDescription\n\n\n\n\n\n\n\n\n\n\nresize_height\n\n\n299\n\n\nResize the image to this height before feeding it into the convolutional network.\n\n\n\n\n\n\nresize_width\n\n\n299\n\n\nResize the image to this width before feeding it into the convolutional network.",
            "title": "Reference: Encoders"
        },
        {
            "location": "/encoders/#encoder-reference",
            "text": "All encoders inherit from the abstract  Encoder  defined in  seq2seq.encoders.encoder  and receive  params ,  mode  arguments at instantiation time. Available hyperparameters vary by encoder class.",
            "title": "Encoder Reference"
        },
        {
            "location": "/encoders/#unidirectionalrnnencoder",
            "text": "Name  Default  Description      rnn_cell.cell_class  BasicLSTMCell  The class of the rnn cell. Cell classes can be fully defined (e.g.  tensorflow.contrib.rnn.BasicRNNCell ) or must be in  tf.contrib.rnn  or  seq2seq.contrib.rnn_cell .    rnn_cell.cell_params  {\"num_units\": 128}  A dictionary of parameters to pass to the cell class constructor.    rnn_cell.dropout_input_keep_prob  1.0  Apply dropout to the (non-recurrent) inputs of each RNN layer using this keep probability. A value of  1.0  disables dropout.    rnn_cell.dropout_output_keep_prob  1.0  Apply dropout to the (non-recurrent) outputs of each RNN layer using this keep probability. A value of  1.0  disables dropout.    rnn_cell.num_layers  1  Number of RNN layers.    rnn_cell.residual_connections  False  If true, add residual connections between RNN layers in the encoder.",
            "title": "UnidirectionalRNNEncoder"
        },
        {
            "location": "/encoders/#bidirectionalrnnencoder",
            "text": "Same as the  UnidirectionalRNNEncoder . The same cell is used for forward and backward RNNs.",
            "title": "BidirectionalRNNEncoder"
        },
        {
            "location": "/encoders/#stackbidirectionalrnnencoder",
            "text": "Same as the  UnidirectionalRNNEncoder . The same cell is used for forward and backward RNNs.",
            "title": "StackBidirectionalRNNEncoder"
        },
        {
            "location": "/encoders/#poolingencoder",
            "text": "An encoder that pools over embeddings, as described in  https://arxiv.org/abs/1611.02344 . The encoder supports optional positions embeddings and a configurable pooling window.     Name  Default  Description      pooling_fn  tensorflow.layers.average_pooling1d  The 1-d pooling function to use, e.g.  tensorflow.layers.average_pooling1d .    pool_size  5  The pooling window, passed as  pool_size  to the pooling function.    strides  1  The stride during pooling, passed as  strides  the pooling function.    position_embeddings.enable  True  If true, add position embeddings to the inputs before pooling.    position_embeddings.combiner_fn  tensorflow.add  Function used to combine the position embeddings with the inputs. For example,  tensorflow.add .    position_embeddings.num_positions  100  Size of the position embedding matrix. This should be set to the maximum sequence length of the inputs.",
            "title": "PoolingEncoder"
        },
        {
            "location": "/encoders/#inceptionv3encoder",
            "text": "This encoder is experimental . This encoder puts the image through an InceptionV3 network and uses the last\nhidden layer before the logits as the feature representation.     Name  Default  Description      resize_height  299  Resize the image to this height before feeding it into the convolutional network.    resize_width  299  Resize the image to this width before feeding it into the convolutional network.",
            "title": "InceptionV3Encoder"
        },
        {
            "location": "/decoders/",
            "text": "Decoder Reference\n\n\nThe following tables list available decoder classes and parameters.\n\n\nBasicDecoder\n\n\n\n\nA Recurrent Neural Network decoder that produces a sequence of output tokens.\n\n\n\n\n\n\n\n\nName\n\n\nDefault\n\n\nDescription\n\n\n\n\n\n\n\n\n\n\nmax_decode_length\n\n\n100\n\n\nStop decoding early if a sequence reaches this length threshold.\n\n\n\n\n\n\nrnn_cell.cell_class\n\n\nBasicLSTMCell\n\n\nThe class of the rnn cell. Cell classes can be fully defined (e.g. \ntensorflow.contrib.rnn.BasicRNNCell\n) or must be in \ntf.contrib.rnn\n or \nseq2seq.contrib.rnn_cell\n.\n\n\n\n\n\n\nrnn_cell.cell_params\n\n\n{\"num_units\": 128}\n\n\nA dictionary of parameters to pass to the cell class constructor.\n\n\n\n\n\n\nrnn_cell.dropout_input_keep_prob\n\n\n1.0\n\n\nApply dropout to the (non-recurrent) inputs of each RNN layer using this keep probability. A value of \n1.0\n disables dropout.\n\n\n\n\n\n\nrnn_cell.dropout_output_keep_prob\n\n\n1.0\n\n\nApply dropout to the (non-recurrent) outputs of each RNN layer using this keep probability. A value of \n1.0\n disables dropout.\n\n\n\n\n\n\nrnn_cell.num_layers\n\n\n1\n\n\nNumber of RNN layers.\n\n\n\n\n\n\nrnn_cell.residual_connections\n\n\nFalse\n\n\nIf true, add residual connections between all RNN layers in the encoder.\n\n\n\n\n\n\n\n\nAttentionDecoder\n\n\n\n\nA Recurrent Neural Network decoder that produces a sequence of output tokens using an attention mechanisms over its inputs. Parameters are the same as for \nBasicDecoder\n.",
            "title": "Reference: Decoders"
        },
        {
            "location": "/decoders/#decoder-reference",
            "text": "The following tables list available decoder classes and parameters.",
            "title": "Decoder Reference"
        },
        {
            "location": "/decoders/#basicdecoder",
            "text": "A Recurrent Neural Network decoder that produces a sequence of output tokens.     Name  Default  Description      max_decode_length  100  Stop decoding early if a sequence reaches this length threshold.    rnn_cell.cell_class  BasicLSTMCell  The class of the rnn cell. Cell classes can be fully defined (e.g.  tensorflow.contrib.rnn.BasicRNNCell ) or must be in  tf.contrib.rnn  or  seq2seq.contrib.rnn_cell .    rnn_cell.cell_params  {\"num_units\": 128}  A dictionary of parameters to pass to the cell class constructor.    rnn_cell.dropout_input_keep_prob  1.0  Apply dropout to the (non-recurrent) inputs of each RNN layer using this keep probability. A value of  1.0  disables dropout.    rnn_cell.dropout_output_keep_prob  1.0  Apply dropout to the (non-recurrent) outputs of each RNN layer using this keep probability. A value of  1.0  disables dropout.    rnn_cell.num_layers  1  Number of RNN layers.    rnn_cell.residual_connections  False  If true, add residual connections between all RNN layers in the encoder.",
            "title": "BasicDecoder"
        },
        {
            "location": "/decoders/#attentiondecoder",
            "text": "A Recurrent Neural Network decoder that produces a sequence of output tokens using an attention mechanisms over its inputs. Parameters are the same as for  BasicDecoder .",
            "title": "AttentionDecoder"
        }
    ]
}