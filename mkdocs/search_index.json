{
    "docs": [
        {
            "location": "/", 
            "text": "Design Goals\n\n\nWe build this framework with the following goals in mind.\n\n\n\n\n\n\nEase of use.\n You should be able train a model with single command. The input data are raw text files instead of esoteric file formats. Similarly, using a pre-trained model to make predictions should be straightforward.\n\n\n\n\n\n\nEase to extend\n. Code is structured in such a way that it is easy to build upon. For example, adding a new type of attention mechanism or a new encoder architecture requires only minimal code changes.\n\n\n\n\n\n\nWell-documented\n. In addition to \ngenerated API documentation\n we have written up multiple guides to help users familiarize themselves with the framework.\n\n\n\n\n\n\nGood performance\n. For the sake of code simplicity we have not tried to squeeze out every last bit of performance, but our implementation is fast enough to cover almost all production use cases. It also supports distributed training to trade off computational power and training time.\n\n\n\n\n\n\nStandard Benchmarks\n. We provide \npre-trained models and benchmark results\n for several standard datasets. We hope these can serve as a baseline for further research.", 
            "title": "Overview"
        }, 
        {
            "location": "/#design-goals", 
            "text": "We build this framework with the following goals in mind.    Ease of use.  You should be able train a model with single command. The input data are raw text files instead of esoteric file formats. Similarly, using a pre-trained model to make predictions should be straightforward.    Ease to extend . Code is structured in such a way that it is easy to build upon. For example, adding a new type of attention mechanism or a new encoder architecture requires only minimal code changes.    Well-documented . In addition to  generated API documentation  we have written up multiple guides to help users familiarize themselves with the framework.    Good performance . For the sake of code simplicity we have not tried to squeeze out every last bit of performance, but our implementation is fast enough to cover almost all production use cases. It also supports distributed training to trade off computational power and training time.    Standard Benchmarks . We provide  pre-trained models and benchmark results  for several standard datasets. We hope these can serve as a baseline for further research.", 
            "title": "Design Goals"
        }, 
        {
            "location": "/getting_started/", 
            "text": "Download \n Setup\n\n\nTODO\n\n\nUsing a pre-trained model\n\n\nTODO\n\n\nTraining your own model\n\n\nTODO\n\n\nDeveloping custom models\n\n\nTODO", 
            "title": "Getting Started"
        }, 
        {
            "location": "/getting_started/#download-setup", 
            "text": "TODO", 
            "title": "Download &amp; Setup"
        }, 
        {
            "location": "/getting_started/#using-a-pre-trained-model", 
            "text": "TODO", 
            "title": "Using a pre-trained model"
        }, 
        {
            "location": "/getting_started/#training-your-own-model", 
            "text": "TODO", 
            "title": "Training your own model"
        }, 
        {
            "location": "/getting_started/#developing-custom-models", 
            "text": "TODO", 
            "title": "Developing custom models"
        }, 
        {
            "location": "/data/", 
            "text": "Pre-processed datasets\n\n\nSee the data generation notebooks\n for details on how this data was generated.\n\n\n\n\n\n\n\n\nDataset\n\n\nDescription\n\n\nTraining/Dev/Test Examples\n\n\nVocab Size\n\n\nURL\n\n\n\n\n\n\n\n\n\n\nWMT'16 EN-DE\n\n\nData for the \nWMT'16 Translation Task\n English to German. Training data is combined from Europarl v7, Common Crawl, and News v11. Development data is newstest2013. Test data is newstest2015.\n\n\n4.56M/3K/2.6K\n\n\n50k\n\n\nDownload\n\n\n\n\n\n\nToy Copy\n\n\nA toy dataset where the target sequence is equal to the source sequence. Thus, the network must learn to \"copy\" the source sequence.\n\n\n10k/1k/1k\n\n\n20\n\n\nDownload\n\n\n\n\n\n\nToy Reverse\n\n\nA toy dataset where the target sequence is equal to the reversed source sequence.\n\n\n10k/1k/1k\n\n\n20\n\n\nDownload\n\n\n\n\n\n\n\n\nTraining/Dev data in Parallel Text Format\n\n\nThe input pipeline expect parallel tokenized data in raw text format. That is, you need \nsources.txt\n and a \ntargets.txt\n file that contain corresponding sentences, aligned line-by-line. Each line corresponds to one input/output example. These words/tokens in these files must be separated by spaces.\n\n\nGenerating Vocabulary\n\n\nA vocabulary file is a raw text file that contains one token per line. The total number of lines is the size of the vocabulary, and each token is mapped to its line number. The special words \nUNK\n, \nSEQUENCE_START\n and \nSEQUENCE_END\n are not part of the vocabulary file, and correspond to \nvocab_size + 1\n, \nvocab_size + 2\n, and \nvocab_size + 3\n respectively\n\n\nGiven a raw text file of tokens separated by spaces you can generate a vocabulary file using the \ngenerate_vocab.py\n script:\n\n\n./seq2seq/scripts/generate_vocab.py \\\n  --input_file /data/source.txt \\\n  --output_file /data/source_vocab \\\n  --min_frequency 1 \\\n  --max_vocab_size 50000\n\n\n\n\nThe resulting vocabulary file contains one word per line.\n\n\nSubword Unit Preprocessing\n\n\nIn order to deal with an open vocabulary, rare words can be split into\nsubword units as proposed in [1]. This improves the model's translation\nperformance particularly on rare words. The authors propose to use\nByte Pair Encoding (BPE), a simple compression algorithm, for splitting\nwords into subwords. Starting from characters, BPE iteratively replaces the\nmost frequent pair of symbols with a new symbol. The final symbol vocabulary\nis equal to the size of the initial vocabulary plus the number of merge\noperations, which is the only hyperparameter of the method.\nTo apply BPE as a pre-processing step to your raw text input,\nfollow the below steps:\n\n\n\n\nDownload the open-source implementation of the paper\n   from \nhere\n.\n\n\nLearn the BPE encoding: \ncd subword-nmt\n.\n   \n./learn_bpe.py -s {num_operations} -i {train_file} -o {codes_file}\n.\n   \nnum_operations\n is the number of merge operations. Default is \n10000\n.\n\n\nApply the BPE encoding to the training and test files:\n  \n./apply_bpe.py -c {codes_file} -i {input_file} -o {output_file}\n.\n\n\n\n\nThe resulting BPE-processed files can be used as-is in place of the raw text\nfiles for training the NMT model.\n\n\nReferences:\n\n\n[1] Sennrich, R., Haddow, B., \n Birch, A. (2016). Neural Machine Translation of\nRare Words with Subword Units. In Proceedings of the 54th Annual Meeting of\nthe Association for Computational Linguistics (ACL 2016).\nRetrieved from http://arxiv.org/abs/1508.07909\n\n\nTraining/Dev data in TFRecords format (Old)\n\n\nThe input pipeline expects data in \nTFRecord\n format consisting of \ntf.Example\n protocol buffers. Each \nExample\n record contains the following fields:\n\n\n\n\npair_id (string)\n (optional) is a dataset-unique id for this example.\n\n\nsource_len (int64)\n is the length of the source sequence.\n\n\ntarget_len (int64)\n is the length of the target sequence.\n\n\nsource_tokens (string)\n is a list of source tokens.\n\n\ntarget_tokens (string)\n is a list of targets tokens.\n\n\n\n\nGiven a parallel corpus, i.e. source and target files aligned by line such as \nthose from WMT\n, we provide a \nscript\n to generate a corresponding TFRecords file:\n\n\n./seq2seq/scripts/generate_examples.py \\\n  --source_file /path/to/data/train.de.txt \\\n  --target_file /path/to/data/train.en.txt \\\n  --output_file /path/to/data/train.tfrecords", 
            "title": "Data"
        }, 
        {
            "location": "/data/#pre-processed-datasets", 
            "text": "See the data generation notebooks  for details on how this data was generated.     Dataset  Description  Training/Dev/Test Examples  Vocab Size  URL      WMT'16 EN-DE  Data for the  WMT'16 Translation Task  English to German. Training data is combined from Europarl v7, Common Crawl, and News v11. Development data is newstest2013. Test data is newstest2015.  4.56M/3K/2.6K  50k  Download    Toy Copy  A toy dataset where the target sequence is equal to the source sequence. Thus, the network must learn to \"copy\" the source sequence.  10k/1k/1k  20  Download    Toy Reverse  A toy dataset where the target sequence is equal to the reversed source sequence.  10k/1k/1k  20  Download", 
            "title": "Pre-processed datasets"
        }, 
        {
            "location": "/data/#trainingdev-data-in-parallel-text-format", 
            "text": "The input pipeline expect parallel tokenized data in raw text format. That is, you need  sources.txt  and a  targets.txt  file that contain corresponding sentences, aligned line-by-line. Each line corresponds to one input/output example. These words/tokens in these files must be separated by spaces.", 
            "title": "Training/Dev data in Parallel Text Format"
        }, 
        {
            "location": "/data/#generating-vocabulary", 
            "text": "A vocabulary file is a raw text file that contains one token per line. The total number of lines is the size of the vocabulary, and each token is mapped to its line number. The special words  UNK ,  SEQUENCE_START  and  SEQUENCE_END  are not part of the vocabulary file, and correspond to  vocab_size + 1 ,  vocab_size + 2 , and  vocab_size + 3  respectively  Given a raw text file of tokens separated by spaces you can generate a vocabulary file using the  generate_vocab.py  script:  ./seq2seq/scripts/generate_vocab.py \\\n  --input_file /data/source.txt \\\n  --output_file /data/source_vocab \\\n  --min_frequency 1 \\\n  --max_vocab_size 50000  The resulting vocabulary file contains one word per line.", 
            "title": "Generating Vocabulary"
        }, 
        {
            "location": "/data/#subword-unit-preprocessing", 
            "text": "In order to deal with an open vocabulary, rare words can be split into\nsubword units as proposed in [1]. This improves the model's translation\nperformance particularly on rare words. The authors propose to use\nByte Pair Encoding (BPE), a simple compression algorithm, for splitting\nwords into subwords. Starting from characters, BPE iteratively replaces the\nmost frequent pair of symbols with a new symbol. The final symbol vocabulary\nis equal to the size of the initial vocabulary plus the number of merge\noperations, which is the only hyperparameter of the method.\nTo apply BPE as a pre-processing step to your raw text input,\nfollow the below steps:   Download the open-source implementation of the paper\n   from  here .  Learn the BPE encoding:  cd subword-nmt .\n    ./learn_bpe.py -s {num_operations} -i {train_file} -o {codes_file} .\n    num_operations  is the number of merge operations. Default is  10000 .  Apply the BPE encoding to the training and test files:\n   ./apply_bpe.py -c {codes_file} -i {input_file} -o {output_file} .   The resulting BPE-processed files can be used as-is in place of the raw text\nfiles for training the NMT model.  References:  [1] Sennrich, R., Haddow, B.,   Birch, A. (2016). Neural Machine Translation of\nRare Words with Subword Units. In Proceedings of the 54th Annual Meeting of\nthe Association for Computational Linguistics (ACL 2016).\nRetrieved from http://arxiv.org/abs/1508.07909", 
            "title": "Subword Unit Preprocessing"
        }, 
        {
            "location": "/data/#trainingdev-data-in-tfrecords-format-old", 
            "text": "The input pipeline expects data in  TFRecord  format consisting of  tf.Example  protocol buffers. Each  Example  record contains the following fields:   pair_id (string)  (optional) is a dataset-unique id for this example.  source_len (int64)  is the length of the source sequence.  target_len (int64)  is the length of the target sequence.  source_tokens (string)  is a list of source tokens.  target_tokens (string)  is a list of targets tokens.   Given a parallel corpus, i.e. source and target files aligned by line such as  those from WMT , we provide a  script  to generate a corresponding TFRecords file:  ./seq2seq/scripts/generate_examples.py \\\n  --source_file /path/to/data/train.de.txt \\\n  --target_file /path/to/data/train.en.txt \\\n  --output_file /path/to/data/train.tfrecords", 
            "title": "Training/Dev data in TFRecords format (Old)"
        }, 
        {
            "location": "/models/", 
            "text": "TODO\n\n\nModel Library and Hyperparameter documentation", 
            "title": "Models"
        }, 
        {
            "location": "/models/#todo", 
            "text": "Model Library and Hyperparameter documentation", 
            "title": "TODO"
        }, 
        {
            "location": "/training/", 
            "text": "Input Files\n\n\nTo train a model you need to follow files. See \nData\n for more details on how to generate or download each of them.\n\n\n\n\nTraining data: Two parallel (line by line aligned) text files that are tokenized, i.e. have words separated by spaces.\n\n\nDevelopment data: Same format as the training data, but used for validation.\n\n\nSource vocabulary file: A file with one word per line that defines the source vocabulary.\n\n\nTarget vocabulary file: Same format as the source vocabulary, but for the target language.\n\n\n\n\nRunning Training\n\n\nFrom the root directory, run:\n\n\npython -m seq2seq.training.train \\\n--train_source=data/train.sources.txt \\\n--train_target=data/train.targets.txt \\\n--dev_source=data/dev.sources.txt \\\n--dev_target=data/dev.targets.txt \\\n--vocab_source=data/vocab_source \\\n--vocab_target=data/vocab_target \\\n--model AttentionSeq2Seq \\\n--batch_size=16 \\\n--hparams=\nrnn_cell.num_layers=2,\n  rnn_cell.type=GRUCell,\n  rnn_cell.num_units=128,\n  source.max_seq_len=40,\n  target.max_seq_len=40\n \\\n--output_dir=/path/to/model/dir \\\n--buckets=10,15,20,25,30,35\n\n\n\n\nHere, \ntrain_source\n, \ntrain_target\n \ndev_source\n, \ndev_source\n, \nvocab_source\n and \nvocab_target\n are the input files described above. \n\n\nmodel\n is the name of some class defined in \nseq2seq.models\n. Currently, the available models are:\n\n\n\n\nBasicSeq2Seq\n - Uses a unidirectional RNN encoder, passes state from encoder to decoder, and uses no attention mechanism.\n\n\nAttentionSeq2Seq\n - Uses a bidirectional RNN encoder and an attention mechanism.\n\n\n\n\nRefer to the source code comments for more details of these details.\n\n\nhparams\n are model-specific Hyperparameters. Refer to the \ndefault_params\n of model classes for a list of all available hyperparameters.\n\n\nbuckets\n is an optional argument that you can use to speed up training by bucketing training examples into batches of roughly equal length. \n10,15,20,25,30,35\n would result in 8 buckets: Sequences of length \n10\n, \n10..15\n, ..., and \n35\n.\n\n\nMonitoring Training\n\n\nIn addition to looking at the training output, you can use Tensorboard to monitor progress:\n\n\ntensorboard --logdir=/path/to/model/dir", 
            "title": "Training"
        }, 
        {
            "location": "/training/#input-files", 
            "text": "To train a model you need to follow files. See  Data  for more details on how to generate or download each of them.   Training data: Two parallel (line by line aligned) text files that are tokenized, i.e. have words separated by spaces.  Development data: Same format as the training data, but used for validation.  Source vocabulary file: A file with one word per line that defines the source vocabulary.  Target vocabulary file: Same format as the source vocabulary, but for the target language.", 
            "title": "Input Files"
        }, 
        {
            "location": "/training/#running-training", 
            "text": "From the root directory, run:  python -m seq2seq.training.train \\\n--train_source=data/train.sources.txt \\\n--train_target=data/train.targets.txt \\\n--dev_source=data/dev.sources.txt \\\n--dev_target=data/dev.targets.txt \\\n--vocab_source=data/vocab_source \\\n--vocab_target=data/vocab_target \\\n--model AttentionSeq2Seq \\\n--batch_size=16 \\\n--hparams= rnn_cell.num_layers=2,\n  rnn_cell.type=GRUCell,\n  rnn_cell.num_units=128,\n  source.max_seq_len=40,\n  target.max_seq_len=40  \\\n--output_dir=/path/to/model/dir \\\n--buckets=10,15,20,25,30,35  Here,  train_source ,  train_target   dev_source ,  dev_source ,  vocab_source  and  vocab_target  are the input files described above.   model  is the name of some class defined in  seq2seq.models . Currently, the available models are:   BasicSeq2Seq  - Uses a unidirectional RNN encoder, passes state from encoder to decoder, and uses no attention mechanism.  AttentionSeq2Seq  - Uses a bidirectional RNN encoder and an attention mechanism.   Refer to the source code comments for more details of these details.  hparams  are model-specific Hyperparameters. Refer to the  default_params  of model classes for a list of all available hyperparameters.  buckets  is an optional argument that you can use to speed up training by bucketing training examples into batches of roughly equal length.  10,15,20,25,30,35  would result in 8 buckets: Sequences of length  10 ,  10..15 , ..., and  35 .", 
            "title": "Running Training"
        }, 
        {
            "location": "/training/#monitoring-training", 
            "text": "In addition to looking at the training output, you can use Tensorboard to monitor progress:  tensorboard --logdir=/path/to/model/dir", 
            "title": "Monitoring Training"
        }, 
        {
            "location": "/inference/", 
            "text": "To make predictions with a trained model you need the following:\n\n\n\n\nA trained model directory, \nmodel_dir\n: The directory should contain the model checkpoint and a \nhparams.txt\n file with the hyperparameters used during training. This is the \noutput_dir\n you specify in the training script.\n\n\nA \nsource_text\n file in the same format as the \ntraining input data\n, one tokenized input example per line.\n\n\nVocabulary files for both source and targets, \nvocab_source\n and \nvocab_target\n. Same as used during training.\n\n\n\n\nGiven the above, you use \nscripts/infer.py\n as follows:\n\n\n./seq2seq/scripts/infer.py \\\n  --source sources.txt \\\n  --model_dir /tmp/model \\\n  --vocab_source vocab_source.txt \\\n  --vocab_target vocab_target.txt \n out.txt", 
            "title": "Inference"
        }, 
        {
            "location": "/benchmarks/", 
            "text": "Machine Translation\n\n\n\n\n\n\n\n\nCorpus\n\n\nSetttings\n\n\nTraining Time\n\n\nEvaluation\n\n\nDownloads\n\n\n\n\n\n\n\n\n\n\nWMT'16 English-German\u00a0(4.5M Sentence Pairs)\n\n\nHyperparameters\n\n\nN/A\n\n\n0.00 PPL \n 0.00 BLEU\n\n\nModel\n \n \nData\n\n\n\n\n\n\nWMT'16 English-French (?? Sentence Pairs)\n\n\n---\n\n\n---\n\n\n---\n\n\n---\n\n\n\n\n\n\nIWSLT 2015 English-Vietnamese (200k Sentence Pairs)\n\n\n---\n\n\n---\n\n\n---\n\n\n---", 
            "title": "Benchmarks"
        }, 
        {
            "location": "/benchmarks/#machine-translation", 
            "text": "Corpus  Setttings  Training Time  Evaluation  Downloads      WMT'16 English-German\u00a0(4.5M Sentence Pairs)  Hyperparameters  N/A  0.00 PPL   0.00 BLEU  Model     Data    WMT'16 English-French (?? Sentence Pairs)  ---  ---  ---  ---    IWSLT 2015 English-Vietnamese (200k Sentence Pairs)  ---  ---  ---  ---", 
            "title": "Machine Translation"
        }, 
        {
            "location": "/profiling/", 
            "text": "There are various ways to profile your model, from printing out the number of parameters to viewing detailed timing and device placement information for all graph operations.\n\n\nMetadataCaptureHook\n\n\nThe easiest way to generate profiling information is to use the \nMetadataCaptureHook\n, which saves a full graph trace and timeline information for a single configurable training step. By default these files are saved a \nmetadata\n subdirectory of your model checkpoint directory.\n\n\nTimelines\n\n\nIf you used the \nMetadataCaptureHook\n you can view the generated \ntimeline.json\n file in your web browser:\n\n\n\n\nGo to \nchrome://tracing\n\n\nLoad the \ntimeline.json\n file that was saved by the \nMetadataCaptureHook\n.\n\n\n\n\nFor complicated graphs timeline files can become quite large and analyzing them using Chrome may be slow and inconvenient. \n\n\nProfiling Script\n\n\nAn easy way to get basic information about your model is to run the \nprofile.py\n script. It uses \nTFProf\n to read metadata saved by the \nMetadataCaptureHook\n and generates several analyses.\n\n\n/seq2seq/scripts/profile.py --model_dir=/path/to/model/dir\n\n\n\n\nThis command will generate 4 files:\n\n\n/path/to/model/dir/params.txt\n contains an analysis of model parameter, including the number of parameters and their shapes and sizes:\n\n\natt_seq2seq/attention_decoder/RNN/rnn_step/logits/weights (384x103, 39.55k/39.55k params, 158.21KB/158.21KB)\n\n\n\n\n/path/to/model/dir/flops.txt\n contains information about floating point operations:\n\n\natt_seq2seq/OptimizeLoss/gradients/att_seq2seq/attention_decoder/RNN/while/rnn_step/attention/inputs_att/MatMul_grad/MatMul (18.87m/18.87m flops, 1.64ms/1.64ms, /job:localhost/replica:0/task:0/cpu:0)\n\n\n\n\n/path/to/model/dir/micro.txt\n contains microsecond timing information for operations that take longer than 1 milliseconds, organized by graph structure:\n\n\natt_seq2seq/attention_decoder/RNN/while/rnn_step/attention/mul_1 (1.89ms/13.72ms, /job:localhost/replica:0/task:0/cpu:0)\n  att_seq2seq/attention_decoder/RNN/while/rnn_step/attention/inputs_att/MatMul (1.21ms/11.83ms, /job:localhost/replica:0/task:0/cpu:0)\n  ....\n\n\n\n\n/path/to/model/dir/device.txt\n contains detailed device placement information for all operations.", 
            "title": "Profiling"
        }, 
        {
            "location": "/profiling/#metadatacapturehook", 
            "text": "The easiest way to generate profiling information is to use the  MetadataCaptureHook , which saves a full graph trace and timeline information for a single configurable training step. By default these files are saved a  metadata  subdirectory of your model checkpoint directory.", 
            "title": "MetadataCaptureHook"
        }, 
        {
            "location": "/profiling/#timelines", 
            "text": "If you used the  MetadataCaptureHook  you can view the generated  timeline.json  file in your web browser:   Go to  chrome://tracing  Load the  timeline.json  file that was saved by the  MetadataCaptureHook .   For complicated graphs timeline files can become quite large and analyzing them using Chrome may be slow and inconvenient.", 
            "title": "Timelines"
        }, 
        {
            "location": "/profiling/#profiling-script", 
            "text": "An easy way to get basic information about your model is to run the  profile.py  script. It uses  TFProf  to read metadata saved by the  MetadataCaptureHook  and generates several analyses.  /seq2seq/scripts/profile.py --model_dir=/path/to/model/dir  This command will generate 4 files:  /path/to/model/dir/params.txt  contains an analysis of model parameter, including the number of parameters and their shapes and sizes:  att_seq2seq/attention_decoder/RNN/rnn_step/logits/weights (384x103, 39.55k/39.55k params, 158.21KB/158.21KB)  /path/to/model/dir/flops.txt  contains information about floating point operations:  att_seq2seq/OptimizeLoss/gradients/att_seq2seq/attention_decoder/RNN/while/rnn_step/attention/inputs_att/MatMul_grad/MatMul (18.87m/18.87m flops, 1.64ms/1.64ms, /job:localhost/replica:0/task:0/cpu:0)  /path/to/model/dir/micro.txt  contains microsecond timing information for operations that take longer than 1 milliseconds, organized by graph structure:  att_seq2seq/attention_decoder/RNN/while/rnn_step/attention/mul_1 (1.89ms/13.72ms, /job:localhost/replica:0/task:0/cpu:0)\n  att_seq2seq/attention_decoder/RNN/while/rnn_step/attention/inputs_att/MatMul (1.21ms/11.83ms, /job:localhost/replica:0/task:0/cpu:0)\n  ....  /path/to/model/dir/device.txt  contains detailed device placement information for all operations.", 
            "title": "Profiling Script"
        }, 
        {
            "location": "/developer_guide/", 
            "text": "Development Setup\n\n\n1. Install Python3. If you're on a Mac the easiest way to do this is probably using \nHomebrew\n.\n\n\nbrew install python3\n\n\n\n\n2. Clone this repository.\n\n\ngit clone https://github.com/dennybritz/seq2seq.git\ncd seq2seq\n\n\n\n\n3. Create a new \nvirtual environment\n and activate it:\n\n\npython3 -m venv ~/tf-venv\nsource ~/tf-venv/bin/activate\n\n\n\n\n4. Install package dependencies and utilities. \n\n\npip install -e .\npip install nose pylint tox yapf mkdocs\n\n\n\n\n5. Make sure tests are passing.\n\n\nnosetests\n\n\n\n\n6. Code :)\n\n\nGithub Workflow\n\n\nPushing directly to the master branch is disabled and you must create feature branches and submit them via pull request. To make things easier you can also use the \nGithub Desktop app\n. A typical workflow looks as follows:\n\n\n# Make sure you are in the seq2seq root directory\n# Start from the master branch\ngit checkout master\n\n# Pull latest changes from github\ngit pull\n\n# Create a new feature branch\ngit checkout -b feature/my-new-feature\n\n# Make changes and commits\necho \nblabla\n \n test\ngit commit -am \nTest commit\n\n\n# Push the branch upstream\ngit push -u origin/my-new-feature\n\n# Submit a pull request on Github\n\n\n\n\nAfter you submit a Pull Request one person must review the change and\nCircleCI integration tests must be passing before you can merge into the\nmaster branch.\n\n\nPython Style\n\n\nWe use \npylint\n and \nyapf\n\nfor automated code formatting. Before submitting a pull request, make sure you\nrun them:\n\n\npylint ./seq2seq\nyapf -ir ./seq2seq\n\n\n\n\nNote that CirlceCI integrations test will fail if pylint reports any critical\nerrors, preventing you from merging your changes.\n\n\nTensorflow Style\n\n\nGraphModule\n\n\nAll classes that modify the Graph should inherit from \nseq2seq.graph_module.GraphModule\n, which is a wrapper around Tensorflow's \ntf.make_template\n function that allows for easy variable sharing. Basically, it allows you to do something like this:\n\n\nencode_fn = SomeEncoderModule(...)\n\n# New variables are created in this call.\noutput1 = encode_fn(input1)\n\n# No new variables are created here. The variables from the above call are re-used.\n# Note how this is different from normal Tensorflow where you would need to use variable scopes.\noutput2 = encode_fn(input2)\n\n# Because this is a new instance a second set of variables is created\nencode_fn2 = SomeEncoderModule(...)\noutput3 = encode_fn2(input3)\n\n\n\n\nFunctions vs. Classes\n\n\n\n\nOperations that \ncreate new variables\n must be implemented as classes and must inherit from \nGraphModule\n.\n\n\nOperations that \ndo not create new variables\n can be implemented as standard python functions, or as classes that inherit from \nGraphModule\n if they have a lot of logic.", 
            "title": "Developer Guide"
        }, 
        {
            "location": "/developer_guide/#development-setup", 
            "text": "1. Install Python3. If you're on a Mac the easiest way to do this is probably using  Homebrew .  brew install python3  2. Clone this repository.  git clone https://github.com/dennybritz/seq2seq.git\ncd seq2seq  3. Create a new  virtual environment  and activate it:  python3 -m venv ~/tf-venv\nsource ~/tf-venv/bin/activate  4. Install package dependencies and utilities.   pip install -e .\npip install nose pylint tox yapf mkdocs  5. Make sure tests are passing.  nosetests  6. Code :)", 
            "title": "Development Setup"
        }, 
        {
            "location": "/developer_guide/#github-workflow", 
            "text": "Pushing directly to the master branch is disabled and you must create feature branches and submit them via pull request. To make things easier you can also use the  Github Desktop app . A typical workflow looks as follows:  # Make sure you are in the seq2seq root directory\n# Start from the master branch\ngit checkout master\n\n# Pull latest changes from github\ngit pull\n\n# Create a new feature branch\ngit checkout -b feature/my-new-feature\n\n# Make changes and commits\necho  blabla    test\ngit commit -am  Test commit \n\n# Push the branch upstream\ngit push -u origin/my-new-feature\n\n# Submit a pull request on Github  After you submit a Pull Request one person must review the change and\nCircleCI integration tests must be passing before you can merge into the\nmaster branch.", 
            "title": "Github Workflow"
        }, 
        {
            "location": "/developer_guide/#python-style", 
            "text": "We use  pylint  and  yapf \nfor automated code formatting. Before submitting a pull request, make sure you\nrun them:  pylint ./seq2seq\nyapf -ir ./seq2seq  Note that CirlceCI integrations test will fail if pylint reports any critical\nerrors, preventing you from merging your changes.", 
            "title": "Python Style"
        }, 
        {
            "location": "/developer_guide/#tensorflow-style", 
            "text": "", 
            "title": "Tensorflow Style"
        }, 
        {
            "location": "/developer_guide/#graphmodule", 
            "text": "All classes that modify the Graph should inherit from  seq2seq.graph_module.GraphModule , which is a wrapper around Tensorflow's  tf.make_template  function that allows for easy variable sharing. Basically, it allows you to do something like this:  encode_fn = SomeEncoderModule(...)\n\n# New variables are created in this call.\noutput1 = encode_fn(input1)\n\n# No new variables are created here. The variables from the above call are re-used.\n# Note how this is different from normal Tensorflow where you would need to use variable scopes.\noutput2 = encode_fn(input2)\n\n# Because this is a new instance a second set of variables is created\nencode_fn2 = SomeEncoderModule(...)\noutput3 = encode_fn2(input3)", 
            "title": "GraphModule"
        }, 
        {
            "location": "/developer_guide/#functions-vs-classes", 
            "text": "Operations that  create new variables  must be implemented as classes and must inherit from  GraphModule .  Operations that  do not create new variables  can be implemented as standard python functions, or as classes that inherit from  GraphModule  if they have a lot of logic.", 
            "title": "Functions vs. Classes"
        }
    ]
}