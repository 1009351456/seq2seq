<!DOCTYPE html>
<!--[if IE 8]><html class="no-js lt-ie9" lang="en" > <![endif]-->
<!--[if gt IE 8]><!--> <html class="no-js" lang="en" > <!--<![endif]-->
<head>
  <meta charset="utf-8">
  <meta http-equiv="X-UA-Compatible" content="IE=edge">
  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  
  
  <link rel="shortcut icon" href="../img/favicon.ico">
  <title>Benchmarks - seq2seq</title>
  <link href='https://fonts.googleapis.com/css?family=Lato:400,700|Roboto+Slab:400,700|Inconsolata:400,700' rel='stylesheet' type='text/css'>

  <link rel="stylesheet" href="../css/theme.css" type="text/css" />
  <link rel="stylesheet" href="../css/theme_extra.css" type="text/css" />
  <link rel="stylesheet" href="../css/highlight.css">
  <link href="../extra.css" rel="stylesheet">
  
  <script>
    // Current page data
    var mkdocs_page_name = "Benchmarks";
    var mkdocs_page_input_path = "benchmarks.md";
    var mkdocs_page_url = "/benchmarks/";
  </script>
  
  <script src="../js/jquery-2.1.1.min.js"></script>
  <script src="../js/modernizr-2.8.3.min.js"></script>
  <script type="text/javascript" src="../js/highlight.pack.js"></script> 
  
</head>

<body class="wy-body-for-nav" role="document">

  <div class="wy-grid-for-nav">

    
    <nav data-toggle="wy-nav-shift" class="wy-nav-side stickynav">
      <div class="wy-side-nav-search">
        <a href=".." class="icon icon-home"> seq2seq</a>
        <div role="search">
  <form id ="rtd-search-form" class="wy-form" action="../search.html" method="get">
    <input type="text" name="q" placeholder="Search docs" />
  </form>
</div>
      </div>

      <div class="wy-menu wy-menu-vertical" data-spy="affix" role="navigation" aria-label="main navigation">
	<ul class="current">
          
            <li>
    <li class="toctree-l1 ">
        <a class="" href="..">Overview</a>
        
    </li>
<li>
          
            <li>
    <li class="toctree-l1 ">
        <a class="" href="../getting_started/">Getting Started</a>
        
    </li>
<li>
          
            <li>
    <li class="toctree-l1 ">
        <a class="" href="../data/">Data</a>
        
    </li>
<li>
          
            <li>
    <li class="toctree-l1 ">
        <a class="" href="../models/">Models</a>
        
    </li>
<li>
          
            <li>
    <li class="toctree-l1 ">
        <a class="" href="../training/">Training</a>
        
    </li>
<li>
          
            <li>
    <li class="toctree-l1 ">
        <a class="" href="../inference/">Inference</a>
        
    </li>
<li>
          
            <li>
    <li class="toctree-l1 ">
        <a class="" href="../tools/">Tools</a>
        
    </li>
<li>
          
            <li>
    <li class="toctree-l1 current">
        <a class="current" href="./">Benchmarks</a>
        
            <ul>
            
                <li class="toctree-l3"><a href="#machine-translation-wmt16-english-german">Machine Translation: WMT'16 English-German</a></li>
                
            
                <li class="toctree-l3"><a href="#machine-translation-wmt16-german-english">Machine Translation: WMT'16 German-English</a></li>
                
            
                <li class="toctree-l3"><a href="#machine-translation-wmt16-english-french">Machine Translation: WMT'16 English-French</a></li>
                
            
                <li class="toctree-l3"><a href="#machine-translation-wmt16-french-english">Machine Translation: WMT'16 French-English</a></li>
                
            
                <li class="toctree-l3"><a href="#text-summarization">Text Summarization</a></li>
                
            
                <li class="toctree-l3"><a href="#conversational-modeling">Conversational Modeling</a></li>
                
            
            </ul>
        
    </li>
<li>
          
            <li>
    <li class="toctree-l1 ">
        <a class="" href="../contributing/">Contributing</a>
        
    </li>
<li>
          
        </ul>
      </div>
      &nbsp;
    </nav>

    <section data-toggle="wy-nav-shift" class="wy-nav-content-wrap">

      
      <nav class="wy-nav-top" role="navigation" aria-label="top navigation">
        <i data-toggle="wy-nav-top" class="fa fa-bars"></i>
        <a href="..">seq2seq</a>
      </nav>

      
      <div class="wy-nav-content">
        <div class="rst-content">
          <div role="navigation" aria-label="breadcrumbs navigation">
  <ul class="wy-breadcrumbs">
    <li><a href="..">Docs</a> &raquo;</li>
    
      
    
    <li>Benchmarks</li>
    <li class="wy-breadcrumbs-aside">
      
    </li>
  </ul>
  <hr/>
</div>
          <div role="main">
            <div class="section">
              
                <h2 id="machine-translation-wmt16-english-german">Machine Translation: WMT'16 English-German</h2>
<p>Single models only, no ensembles. Results are listed in chronological order.</p>
<table>
<thead>
<tr>
<th>Model Name &amp; Reference</th>
<th>Settings / Notes</th>
<th>Training Time</th>
<th>Test Set BLEU</th>
<th>Downloads</th>
</tr>
</thead>
<tbody>
<tr>
<td>2-Layer LSTM Attention Model + BPE</td>
<td><a href="">Hyperparameters</a></td>
<td>---</td>
<td>newstest2014: - </br> newstest2015: -</td>
<td><a href="">Model</a> <br/> <a href="">Data</a></td>
</tr>
<tr>
<td><a href="https://arxiv.org/abs/1611.02344">Gehring, et al. (2016-11)</a> <br/> Deep Convolutional 15/5</td>
<td></td>
<td>---</td>
<td>newstest2014: - <br/> newstest2015: <strong>24.3</strong></td>
<td>---</td>
</tr>
<tr>
<td><a href="https://arxiv.org/abs/1609.08144">Wu et al. (2016-09)</a> <br/> GNMT</td>
<td>8 encoder/decoder layers, 1024 LSTM units, 32k shared wordpieces (similar to BPE); residual between layers connections; lots of other tricks; newstest2012 and newstest2013 as validation sets. Paper mentions 5M training examples, how is this possible if WMT only has ~4.5M?</td>
<td>---</td>
<td>newstest2014:&nbsp;<strong>24.61</strong> <br/>newstest2015: -</td>
<td>---</td>
</tr>
<tr>
<td><a href="https://arxiv.org/abs/1606.04199">Zhou et al. (2016-06)</a> <br/> Deep-Att</td>
<td></td>
<td>---</td>
<td>newstest2014: <strong>20.6</strong> <br/> newstest2015: -</td>
<td>---</td>
</tr>
<tr>
<td><a href="https://arxiv.org/abs/1603.06147v4">Chung, et al. (2016-03)</a> <br/> BPE-Char</td>
<td><strong>Character-level decoder with BPE encoder.</strong> Based on Bahdanau attention model; Bidirectional encoder with 512 GRU units; 2-layer GRU decoder with 1024 units; Adam; batch size 128; gradient clipping at norm 1; Moses Tokenizer; limit sequences to 50 symbols in source and 100 symbols and 500 characters in target.</td>
<td>---</td>
<td>newstest2014: <strong>21.5</strong> </br> newstest2015: <strong>23.9</strong></td>
<td>---</td>
</tr>
<tr>
<td><a href="https://arxiv.org/abs/1508.07909">Sennrich et al. (2015-8)</a> <br/> BPE</td>
<td><strong>Authors propose BPE for subword unit nsegmentation as a pre/post-processing step to handle open vocabulary</strong>;  Base model is based on <a href="https://arxiv.org/abs/1409.0473">Bahndanau's paper</a>. Bidirectional encoder; GRU; 1000 hidden units; 1000 attention units; 620-dimensional word embeddings; single-layer; beam search width 12; Adadelta with batch size 80; Using <a href="https://github.com/sebastien-j/LV_groundhog">Groundhog</a>;</td>
<td></td>
<td>newstest2014: - <br/>newstest2015: <strong>20.5</strong></td>
<td>---</td>
</tr>
<tr>
<td><a href="https://arxiv.org/abs/1508.04025">Luong et al. (2015-08)</a></td>
<td><strong>Novel local/global attention mechanism;</strong> 50k vocabulary; 4 layers in encoder and decoder; unidirectional encoder; gradient clipping at norm 5;  1028 LSTM units, 1028-dimensional embeddings; (somewhat complicated) SGD decay schedule; dropout 0.2; UNK replace;</td>
<td>---</td>
<td>newstest2014: <strong>20.9</strong> <br/> newstest2015: -</td>
<td>---</td>
</tr>
<tr>
<td><a href="https://arxiv.org/abs/1412.2007">Jean et al. (2014-12)</a> <br/> RNNsearch-LV</td>
<td><strong>Authors propose a new sampling-based approach to incorporate a larger vocabulary</strong>; Base model is based on <a href="https://arxiv.org/abs/1409.0473">Bahndanau's paper</a>. Bidirectional encoder; GRU; 1000 hidden units; 1000 attention units; 620-dimensional word embeddings; single-layer; beam search width 12;</td>
<td>---</td>
<td>newstest2014: <strong>19.4</strong> <br/> newstest2015: -</td>
<td>---</td>
</tr>
</tbody>
</table>
<h2 id="machine-translation-wmt16-german-english">Machine Translation: WMT'16 German-English</h2>
<p>Coming soon.</p>
<h2 id="machine-translation-wmt16-english-french">Machine Translation: WMT'16 English-French</h2>
<p>Coming soon.</p>
<h2 id="machine-translation-wmt16-french-english">Machine Translation: WMT'16 French-English</h2>
<p>Coming soon.</p>
<h2 id="text-summarization">Text Summarization</h2>
<p>Coming soon.</p>
<h2 id="conversational-modeling">Conversational Modeling</h2>
<p>Coming soon.</p>
<p>4 layers, 1000-dimensional embeddings, LSTM cell, unidirectional encoder, gradient norm clipping at 5; SGD with decay schedule; dropout 0.2.</p>
              
            </div>
          </div>
          <footer>
  
    <div class="rst-footer-buttons" role="navigation" aria-label="footer navigation">
      
        <a href="../contributing/" class="btn btn-neutral float-right" title="Contributing">Next <span class="icon icon-circle-arrow-right"></span></a>
      
      
        <a href="../tools/" class="btn btn-neutral" title="Tools"><span class="icon icon-circle-arrow-left"></span> Previous</a>
      
    </div>
  

  <hr/>

  <div role="contentinfo">
    <!-- Copyright etc -->
    
  </div>

  Built with <a href="http://www.mkdocs.org">MkDocs</a> using a <a href="https://github.com/snide/sphinx_rtd_theme">theme</a> provided by <a href="https://readthedocs.org">Read the Docs</a>.
</footer>
	  
        </div>
      </div>

    </section>
    
  </div>

  <div class="rst-versions" role="note" style="cursor: pointer">
    <span class="rst-current-version" data-toggle="rst-current-version">
      
      
        <span><a href="../tools/" style="color: #fcfcfc;">&laquo; Previous</a></span>
      
      
        <span style="margin-left: 15px"><a href="../contributing/" style="color: #fcfcfc">Next &raquo;</a></span>
      
    </span>
</div>
    <script src="../js/theme.js"></script>

</body>
</html>
